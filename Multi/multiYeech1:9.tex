%{{{ Preamble
\documentclass [12 pt, twoside] {book}
\usepackage[margin=1in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{color}
\usepackage{setspace}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{txfonts}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{wrapfig}

\usetikzlibrary{automata,topaths}

\newcommand\la{\textlangle}
\newcommand\ra{\textrangle}
\newcommand\+{\text{ }}
\newcommand\suprem{\text{sup }}
\newcommand\infim{\text{inf }}
\newcommand{\pconv}{\overset{p}{\to}}
\newcommand{\uconv}{\overset{u}{\to}}
\newcommand{\gradient}{\vec{\nabla}}
\newcommand{\vfofpv}{\vec{f}(\vec{p})}
\newcommand{\fofpv}{f(\vec{p})}
\newcommand{\exists}{\exists\+}
\newcommand{\det}{\text{det }}

\setlist{nosep}
\setlength{\parindent}{0pt}

\begin{document}

\frontmatter

\title{Multivariable Calculus}
\author{Yicheng Wang}
\date{2015-2016}

\maketitle
\newpage
\setcounter{tocdepth}{3}
\tableofcontents
\newpage
%}}}
%{{{ Intro to Class
\chapter{Introduction}
\section*{Intro to Class}
\subsection*{Class Information}
\begin{itemize}
    \item Mr. Joseph Stern
    \item jstern6@schools.nyc.gov (official uses only)
    \item mr.stern.mathematics@gmail.com (often checks, use for mundane matters)
    \item Office: 351 (is free during pd. 3, 8, and 9)
    \item Grading policies are the same as the ones listed for the math dept.
    \item Not a lot of tests and not a lot of homeworks, more focused on learning
\end{itemize}

\subsection*{Fundamental Theorem of Calculus}
This is the culminating point of single variable calculus, it has two parts:\\
\textbf{Part I}:
$$\int_a^bf'(x)dx = f(x)\bigg|_a^b = f(b) - f(a)$$
\textbf{Part II}:
$$\frac{d}{dx}\int_a^x f(u)du = f(x)$$

\subsection*{Multivariable Calculus}
This branch of calculus deals with how functions that takes in multiple
independent variables behave. Like the function $f(x, y, z) = (x\sin{y})^{e^z}$.
However, we often observe multiple-dimension functions with a bound on the
variables.
%}}}
%{{{ Big Q's
\section*{Big Question of Multivariable Calculus}
\subsection*{The Big Questions}
\begin{itemize}
    \item What, if anything, is the higher dimensional analogue of the
        Fundamental Theorem of Calculus.
    \item Assuming there are higher dimensional analogues, do their form depend
        on the particular dimensions involved.
\end{itemize}

\subsection*{Strict Definition of the FTC}
When we integrate from $a$ to $b$ of a function $f(x)$, we are integrating over
the interval $(a,b)$. This process does not concern the boundry
points. However, the FTC establishes a relationship between the integral over
the interval and the integral over the boundries. It states a equivalency of the
integral of the derivative over the interval and the integral of the function at
the boundries.

In the one variable form, we can write the FTC as:
$$\int_a^bf'(x)dx = \int_{\partial[a,b]} f = +f(b)-f(a)$$

The negative in front of $f(a)$ signifies a unintented orientation of the curve,
that as $x$ go towards $b$, their value grow larger.

Note that $\partial$ is the symbol we use to denote the boundary of something.

\subsection*{Basics of Calculus with 2D Domain}

Below is a curve $C$ in 2 dimensions, and the region inside it (gray area) can
be denoted as $R$.

\begin{figure}[ht]
\centering
\begin{tikzpicture}
    \begin{axis} [
        scale only axis,
        grid=major,
        axis lines=middle,
        inner axis line style={->},
        ymin=-2.5,
        xmin=-2.5,
        ymax=2.5,
        xmax=2.5,
    ]
    \addplot[color=red, fill=gray, fill opacity=0.5, smooth, thick] coordinates
    {(0,-2) (-1,1) (1,2) (1,0) (2,-1) (0,-2)};
    \end{axis}
\end{tikzpicture}
\end{figure}

This curve is known as a \textbf{simple}, \textbf{closed} curve.

\textbf{Simple} means that each point is crossed by the curve at most once.

\textbf{Closed} means that the curve does not have a unqiue starting point and
end point.

When integrating in 2 dimensions, we also need to pick an ``interval.'' In this
case, $R$, the bounded region, would be an interval (analogous to $(a,b)$ in
single variable calculus) and $C$ would be the boundary (analogous to $a, b$).

If we were to integrate a function $f(x,y)$ over $R$, it is denoted by:
$$\iint_R f(x,y) dA_{xy}$$

The $dA_{xy}$ is what is known as the \textbf{area element}. It is an
infinitesimally small piece of area (this is analogous to $dx$, the length
element in single variable calculus).

Note that in single variable calculus, there is an implied orientation, going
left to right is the positive ``direction,'' In multivariable calculus, it is
accepted that the positive direction for the curve to go in is the
\textit{counterclockwise} direction.

\subsection*{Green's Theorem}
This is one of the FTC's generalization to higher dimensions. The Green's
Theorem works with functions that take in 2 variables.

Suppose there exists $f(x, y)$ and $g(x, y)$, and a region $R$ bounded by a
positively oriented, simple closed curve $C$. Then Green's theorem states that:

$$\iint_R (\frac{\partial g}{\partial x} - \frac{\partial f}{\partial y}) dA_{xy} =
\int_C fdx + gdy$$

The RHS of the equation is evaluated parametrically.

\subsection*{Generalized FTC}
The goal of this course in multivariable calculus is to reach the following
conclusion:

For some function $\omega$ evaluated over the region $M$:
$$\int_M d\omega = \int_{\partial M} \omega$$
%}}}
%{{{ R
\mainmatter
\chapter{Foundations of Calculus}
\section{Foundations - $\varmathbb{R}$}

\subsection{Definitions of Different Number Structures}
\subsubsection{$\varmathbb{N}$}
We can define the natural number system by sets, like the following:

$$0 = \emptyset$$

\noindent And from there we introduce a succession operation:

$$n + 1 = n \cup \{n\}$$

\noindent So for example, $1 = \{0\} = \{\emptyset\}$, $1 = \{0, 1\} = \{\emptyset,
\{\emptyset\}\}$, etc.

\noindent Set theory is the most basic concept in mathematics, and nothing goes deeper.

\subsubsection{$\varmathbb{Z}$}
Positive integers are defined as an ordered pair of natural numbers, for
example, $2 = (2, 0)$, and $-2 = (0, 2)$

\subsubsection{$\varmathbb{Q}$}
Rationals are defined as an infinite set of ordered pairs of integers.
A rational number $q = \frac{n}{m}$, then $q = \{(n, m), (2n, 2m), (3n, 3m)
\dots (-2n, -2m), (-3, -3m) \dots\}$

\subsubsection{$\varmathbb{R}$}

There are several definitions of the real number system
\begin{itemize}
    \item Each real number can be thouhgt of as an infinite sequence in the
        following format:

        $$(s, N, d_1, d_2, d_3 \dots)$$

        Where $s = \pm 1$, $N \in \varmathbb{N}$, and $d_n \in \{0, 1, 2, 3,
        \dots 8, 9\}$. It is also not the case that $d_n = d_{n+1} = \dots = 0$.
        If there is a terminal decimal, we express it as 9 repeated.
    \item Each real number forms a subdivision of $\varmathbb{Q}$ into two
        disjoint sets that cover the entirety of $\varmathbb{Q}$, one of which
        lies entirely to the left of the other.
\end{itemize}

\subsection{Basic Structure of $\varmathbb{R}$}
$\varmathbb{R}$ is an instance of many kinds of mathematical structures, such as:
\begin{itemize}
    \item Field under addition and multiplication.
    \item Ordered Set
        \begin{itemize}
            \item the set has an ordering that reflect the operations in the
                field
            \item this structure is what allows for comparisons, like the $<$
                function
        \end{itemize}
    \item Metric Space
        \begin{itemize}
            \item there exists a standard distance operation between numbers
            \item in $\varmathbb{R}$, $dist(x, y) := |x - y|$ ($:=$ means ``is
                defined as'')
            \item the distance operation obeys certain laws such as the triangle
                inequality
        \end{itemize}
    \item Vector Space
        \begin{itemize}
            \item elements can be thought of as vectors
        \end{itemize}
    \item Geometric Space
        \begin{itemize}
            \item this structure means that you ca measure both length and angle
            \item in $\varmathbb{R}$, the angle measure can be either $0$ or $\pi$
            \item in higher dimensions there are more angles
        \end{itemize}
\end{itemize}

\subsection{Properties of $\varmathbb{R}^n$}
In higher dimensions, several of the properties of $\varmathbb{R}$ are no longer
valid. $\forall n > 1$, $\varmathbb{R}^n$ is \textbf{NOT} a field, and \textbf{NOT} an ordered set.


\subsection{Basic Axioms for $\varmathbb{R}$}
$\varmathbb{R}$ is a field under addition and multiplication ($x, y \in \varmathbb{R}$)
\begin{enumerate}
    \item Additive closure: $x + y \in \varmathbb{R}$
    \item Associative Property of Addition: $x + (y + z) = (x + y) + z$
    \item Communicative Property of Addition: $x + y = y + x$
    \item 0 is the identity element of addition: $x + 0 = x$
    \item Every element has an additive inverse: $x + (-x) = 0$
    \item Multiplicative closure: $xy \in \varmathbb{R}$
    \item Associative Property of Multiplication: $x(yz) = (xy)z$
    \item Communicative Property of Multiplication: $xy = yx$
    \item 1 is the identity element of multiplication: $x(1) = x$
    \item Every element (except 0) has a multiplicative inverse: $x \cdot \frac{1}{x} = 1$
        \begin{itemize}
            \item Theorem: $\forall x \in \varmathbb{R}$, $x \cdot 0 = 0$
            \item Proof: $x \cdot 0 = x \cdot (0 + 0)$, then we apply the distributive law,
                and get $x \cdot 0 = x \cdot 0 + x \cdot 0$. Now we add $-x
                \cdot 0$ to both side,
                and we get: $0 = x \cdot0$
        \end{itemize}
    \item Distributive Law: $x(y + z) = xy + xz$
\end{enumerate}

$\varmathbb{R}$ is an ordered field and has a proper subset (aka not the entire
set) $\varmathbb{R}^+$ (the \textbf{positives}) such that:
\begin{enumerate}
    \item $\varmathbb{R}^+$ is closed under addition and multiplication.
    \item $1 \in \varmathbb{R}^+, 0 \notin \varmathbb{R}^+$
    \item \textbf{Trichotomy Property}: for any $x \in \varmathbb{R}$, $x$ is
        either $0$, $\in \varmathbb{R}^+$ or $\notin \varmathbb{R}^+$
\end{enumerate}

Definition of $<$ and $>$:
\begin{itemize}
    \item $x < y$ means $y - x \in \varmathbb{R}^+$
    \item $x > y$ means $y < x$
\end{itemize}

\subsection{Separation Axiom}

If $\mathcal{A} \subseteq \varmathbb{R}$ and $\mathcal{B} \subseteq
\varmathbb{R}$. Satisfying:
\begin{enumerate}
    \item $\mathcal{A} \cap \mathcal{B} = \emptyset$
    \item $\mathcal{A} \neq \emptyset$, $\mathcal{B} \neq \emptyset$
    \item $\mathcal{A} < \mathcal{B}$ ``$\mathcal{A}$ is to the left of
        $\mathcal{B}$''
    \begin{itemize}
        \item $\forall a \in \mathcal{A}, \forall b \in \mathcal{B}, a < b$
    \end{itemize}
\end{enumerate}

$\exists$ at least 1 $c \in \varmathbb{R}$ such that $\mathcal{A} \leq c \leq
\mathcal{B}$

Note that $\mathcal{A}$ and $\mathcal{B}$ do not have cover the entire real
number line.

\subsubsection{Existence of Irrationals}

The main difference between the $\varmathbb{Q}$ and the $\varmathbb{R}$ is the
separation axiom. The rationals does not have that property. So it is possible
to have two non-empty, non-overlapping subsets, one entirely to the left of the
other, but has no rational number to form a boundary. For example:

$$\mathcal{A} = \varmathbb{Q}^- \cup \{0\} \cup \{q \in \varmathbb{Q}^+ | q^2 < 2\}$$
$$\mathcal{B} = \{q \in \varmathbb{Q}^+ | q^2 \geq 2\}$$

We know that $\mathcal{A} \neq \emptyset$ and $\mathcal{B} \neq \emptyset$
because 0 is in $\mathcal{A}$ and 2 is in $\mathcal{B}$. We also know that
$\mathcal{A} \cup \mathcal{B} = \varmathbb{Q}$ and $\mathcal{A} \cap \mathcal{B}
= \emptyset$. As well as the fact that $\mathcal{A} < \mathcal{B}$.

Now, if we want to find the boundary element, $q_0$, which separates
$\mathcal{A}$ and $\mathcal{B}$. We know that $\mathcal{A} \leq q_0 \leq
\mathcal{B}$. So that must mean $q_0 = \sqrt{2}$. However, $\sqrt{2} \notin
\varmathbb{Q}$. Therefore, we know that the set of rational numbers do not
follow the separation axiom.

This basically means that the rational number system have holes in it, holes
that are filled within the real number system.

%}}}
%{{{ Sequences and Series in R
\section{Sequences in $\varmathbb{R}$}

\subsection{The Least Upper Bound Theorem (LUB Theorem)}

\subsubsection{Theorem}
If $\mathcal{A} \subseteq \varmathbb{R}$
is non-empty, and is \textbf{bounded above} (So $\exists b_1 \in \varmathbb{R}$
such that $\mathcal{A} < b_1$), then $\mathcal{A}$ has a
\textbf{least upper bound}, i.e. a number $b_0 \in \varmathbb{R}$ such that
$\mathcal{A} \leq b_0$ and for any $b$ with $\mathcal{A} \leq b$, $b_0 \leq b$

$$\mathcal{A}\subseteq\varmathbb{R}, \mathcal{A} \neq \emptyset, (\exists\text{
} b_1 \in \varmathbb{R}: \mathcal{A} \leq b_1)\to[\exists\text{ } b_0 \in
\varmathbb{R}: \mathcal{A} < b_0, \forall b \in \varmathbb{R} (\mathcal{A} \leq
b \to b_0 \leq b)]$$

$b_0$ is known as the least possible upper bound, or the \textit{supremum} of
$\mathcal{A}$, we write $b_0 = \textrm{sup } \mathcal{A}$.


Similarly, for any non-empty set $\mathcal{A}$ bounded below, it has a
\textbf{greatest lower bound}, $\textrm{inf } \mathcal{A}$, called the \textit{infimum}
of $\mathcal{A}$

Some other notations on this, $\sup_{x\in D} f(x)$ means the supremum of all the
values of $f(x)$ over $D$.

\subsubsection{Proof}
Define $\mathcal{B}$ to be the set of all upper bounds of $\mathcal{A}$. Let
$\mathcal{C} = \varmathbb{R} \setminus \mathcal{B}$. Clearly $\mathcal{B}$ is
nonempty; also $\mathcal{C}$ is non-empty because it contains $x_0 - 1$, where
$x_0 \in \mathcal{A}$. By the way in which we defined $\mathcal{C}$,
$\mathcal{B} \cap \mathcal{C} = \emptyset$. Pick any $c \in \mathcal{C}$ and $b
\in \mathcal{B}$. By thedefinition of $\mathcal{C}$, $\exists$ $x_1 \in
\mathcal{A}: c < x_1$. But $x_1 \leq b$ by the definition of $\mathcal{B}$.
Therefore $\mathcal{C} < \mathcal{B}$. By
the separation postulate, $\exists$ $b_0 \in \varmathbb{R}: \mathcal{C} \leq
b_0 \leq \mathcal{B}$. Note that $\mathcal{A}\setminus\{b_0\} \subseteq
\mathcal{C}$. Thus, $b_0$ is an upper bound for $\mathcal{A}$. Morever, it is
the least upper bound because $b_0 \leq \mathcal{B}$.

\subsection{Bounded Monotone Sequence Theorem}

\subsubsection{Theorem}
For any sequence $\{a_n\}$
\begin{enumerate}
    \item If $a_n \leq a_{n+1}$ for all $n \geq 1$, and $\exists$ $b \in
        \varmathbb{R}$ such that $a_n \leq b$ for all $n \geq 1$,  then
        $\lim_{n\to\infty} a_n$ exists and is less than or equal to $b$
    \item If $a_n \geq a_{n+1}$ for all $n \geq 1$, and $\exists$ $b \in
        \varmathbb{R}$ such that $a_n \geq b$ for all $n \geq 1$, then
        $\lim_{n\to\infty} a_n$ exists and is greater than or equal to $b$
\end{enumerate}

\subsubsection{Proof}

We first convert the sequence $\{a_n\}$, which is bounded by $b$ into the
set $\mathcal{A} = \{a_n | n \geq 1\}$. We know that $\mathcal{A} \neq
\emptyset$ because the sequence has some terms. We also know that $\mathcal{A}$
is bounded above by $b$; $\mathcal{A} < b$

By the Least Upper Bound Theorem, $\exists$ $b_0 = sup \mathcal{A}$. We now show
that $b_0 = \lim_{n\to\infty}a_n$. By the definition of limits, to say $b_0 =
\lim_{n\to\infty} a_n$ means to say $\forall \varepsilon > 0, \exists$ $N > 0,
\forall n \geq N$, $|a_n - b_0| < \varepsilon$

If we look at the number $b_0 - \varepsilon$, it is not an upper bound on
$\mathcal{A}$ because $b_0$ is the least upper bound and $\varepsilon > 0$.
Therefore, $\exists$ $a_N > b_0 - \varepsilon$. Since $\{a_n\}$ is increasing,
$\forall n > N$, $a_n > b_0 - \varepsilon$. If we rearrange the terms, we get $b_0
- a_n < \varepsilon$. Therefore, $b_0$ (which exists by the least upper bound
theorem) is the limit of $a_n$ as $n \to \infty$.

\subsection{Archimedean Property}
\subsubsection{Property}
For any positive numbers $x$ and $y$, it is possible to find some $n \in
\varmathbb{N}$ such that $nx > y$.
$$\forall x, y > 0, \exists\text{ } n \in \varmathbb{N}: nx > y$$

\subsubsection{Proof}
Assume that $\neg\+\exists\text{ } n:nx > y$, this is logically equivalent to
$\forall n: nx \leq y$. Let $\mathcal{C} = \{nx\+|\+n \in \varmathbb{N}\}$. Then
$\mathcal{C} \leq y$, let $c = \text{sup } \mathcal{C}$.
We claim that $\exists\+N: c-\frac{1}{2}x < Nx \leq c$. This is true because if
such $N$ does not exist, then $c - \frac{1}{2}x$ would be an upper bound, but
$c$ is the least upper bound, so such $N$ must exist.
Now we've established the existence of $N$, let us consider $(N + 1)x$. $(N +
1)x = Nx + x > (c - \frac{1}{2}x) + x = c + \frac{1}{2}x > c$. But $(N + 1)x \in
\mathcal{C}$, so it should be $< c$. We have a contradiction. This shows that
the original assumption is false, so $\forall x, y > 0, \exists\+n \in
\varmathbb{N}: nx > y$

\subsubsection{Consequences}
This property can be used to show that $\lim_{n\to\infty} \frac{1}{n} = 0$.


If we consider the definition of limits, the statement is equivalent to saying
that $\forall \varepsilon \in \varmathbb{R} > 0, \exists\+ N\in\varmathbb{N},
\forall n > N, n \in \varmathbb{R}, \frac{1}{n} < \varepsilon$.

If we rearrange the term, we get that we need to show $1 < \varepsilon N$ for any
$\varepsilon$. This is true because of the Archimedean Property. $\forall n > N$,
since $\varepsilon > 0$, $1 < \varepsilon N < \varepsilon n$. Therefore we know the limit
is truely 0.

\subsection{Well Ordering Principle}
\subsubsection{Principle}
For any set $\mathcal{A} \in \varmathbb{N}, \mathcal{A} \neq \emptyset$,
$\text{min } \mathcal{A}$ exists.
\subsubsection{Proof}
Consider $\mathcal{A} \neq \emptyset \in \varmathbb{N}$. Let $\mathcal{J} :=
\varmathbb{N} \setminus \mathcal{A}$. Note that $0 \notin \mathcal{A}$ because $0 =
\min{\varmathbb{N}} = \min{\mathcal{A}}$, which can not happen. Therefore $0 \in
\mathcal{J}$. Now by induction we can prove that if $0, 1, \dots, J \in
\mathcal{J}$, then $J + 1 \in \mathcal{J}$. Suppose $J + 1 \in \mathcal{A}$,
since $0, 1, \dots, J \notin \mathcal{A}$, $J + 1 = \min{\mathcal{A}}$, which
cannot happen. Therefore $J + 1 \in \mathcal{J}$. By this argument we prove that
$\mathcal{J} = \varmathbb{N}$, therefore as long as $\mathcal{A} \neq
\emptyset$, it has a minimum.
\subsection{Sunrise Lemma}
\subsubsection{Lemma}
For any sequence $(a_n)_{n=1}^\infty$ in $\varmathbb{R}$, there exists at least
one subsequence $(a_{n_k})_{k=1}^\infty$ [here, $(n_k)_{k = 1}^\infty$ is strictly
increasing sequence of $\varmathbb{N}$, note that $n_k \geq k, \forall k \in
\varmathbb{N}$] that is monotone.
$$\forall (a_n)_{n=1}^\infty \in \varmathbb{R}, \exists\+ (a_{n_k})_{k=1}^\infty:
a_{n_{k+1}} \geq a_{n_k}$$

\subsubsection{Proof}
This proof uses a concept known as \textbf{vista}. $N$ is a vista in the
sequence $(a_n)$ if $a_N > a_k \forall k > N$. Note that if two points, $M$ and
$N$ are both vistas and $M > N$, then $a_N > a_M$, because otherwise $a_N$ would
not be a vista.

There are two cases that covers all possible sequences, a sequence $(a_n)$ has
either a finite amount of vistas or an infinite amount of vistas.

\begin{figure}[ht]
\centering
\begin{tikzpicture}
    \begin{axis} [
        scale only axis,
        grid=major,
        axis lines=middle,
        inner axis line style={->},
        ymin=0,
        xmin=0,
        ymax=15,
        xmax=10,
        xlabel = $n$,
        ylabel = $a_n$,
    ]
    \addplot[color=black, fill opacity=0.5, thick, mark = *] coordinates
    {(1, 1) (2, 14) (3, 5) (4, 7) (5, 3) (6, 12) (7, 10) (8, 8) (9, 4) (10, 9)};
    \end{axis}
\end{tikzpicture}
\caption{$a_2, a_6, a_7$ and $a_10$ are vistas}
\end{figure}

Let's first look at the case in which a sequence $(a_n)$ has an infinite amount
of vistas. Let the set $\mathcal{V}$ be the set of vistas. Now we can create a
constaly decrasing sequence recursively:

$$n_1 = \min{\mathcal{V}}$$
$$n_k = \min{(\mathcal{V} \cap (n_{k-1}, \infty))}$$

We know $n_1$ exists because of the Well Ordering Principle.
And we know $n_k$ exists because $\mathcal{V}$ is an infinite set.

Thus we've created a sequence $(a_{n_k})$ that is strictly decreasing.

Now let us consider the case in which the set of vistas is finite. Still, let
$\mathcal{V}$ be the set of vistas. Then let's define $n_k$ in the following
way:

\[
        n_1 =
        \begin{dcases}
            1, & \text{if } \mathcal{V} = \emptyset\\
            1 + \max{\mathcal{V}}, & \text{if } \mathcal{V} \neq \emptyset
        \end{dcases}
\]
$$n_k = \min{\{n > n_{k-1} | a_n > a_{n_{k-1}}\}}$$

Note that $\{n > n_{k-1} | a_n > a_{n_{k-1}}\}$ cannot be empty, because the
vista set is finite by assumption.

Then $(a_{n_k})$ is increasing.

\subsection{Bolzano-Weierstrass Theorem}
\subsubsection{Theorem}
Every bounded sequence $\in \varmathbb{R}$ has at least 1 convergent
subsequence.

\subsubsection{Proof}

Let $(a_n)_{n=1}^\infty$ be a bounded sequence. Take any monotone subsequence
$(a_{n_k})_{k=1}^\infty$. We know it exists because of the Sunrise Lemma. Then
this sequence $(a_{n_k})_{k=1}^\infty$ is both monotone and bounded. Therefore
it converges by the Bounded Monotone Sequence Theorem.

\subsection{Extreme Value Theorem in $\varmathbb{R}$ (EVT - 1)}

\subsubsection{Theorem}
If $f: [a, b] \to \varmathbb{R}$ is continuous [for any point $x_0$,
$f(x) = \lim_{x\to x_0} f(x)$ for any $x_0\in(a,b)$, $f(a) = \lim_{x\to a^+}
f(x)$, $f(b) = \lim_{x\to b^-} f(x)$]. Then $\exists\+ c, d \in [a, b]$ such
that $f(c)\leq f(x) \leq f(d)$, $\forall x \in [a, b]$. In other words, the
function takes on a minimum and a maximum at some point in its domain.

\subsubsection{Proof}
First we prove that $f(x)$ is a bounded above: $\exists\+M \geq 0 : f(x) \leq
M$, $\forall x \in [a,b]$

We prove this by contradiction, assume that $f(x)$ is not bounded above. Thus,
for any $n \in \varmathbb{N}$, $\exists\+x_n \in [a, b]$ such that $f(x_n) > n$.
The sequence $(x_n)_{n = 1}^\infty$ is bounded between $[a, b]$, so by the
Bolzano-Weierstrass Theorem, it has a convergent subsequence
$(x_{n_k})_{k=1}^\infty$ converging to some point $\hat{x} = \lim_{k\to\infty}
x_{n_k}$.

\textbf{Claim}: $\hat{x} \in [a, b]$.

If not, this means that $\hat{x} < a$
or $\hat{x} > b$. For illustration, say $\hat{x} > b$. Then $\exists\+\varepsilon >
0$ such that $[a, b] \cap (\hat{x} - \varepsilon, \hat{x} + \varepsilon) = \emptyset$.
But $\exists\+ M$ such that $x_M \in (\hat{x} - \varepsilon, \hat{x} + \varepsilon)$
because $\lim_{n\to\infty} x_n = \hat{x}$. But $x_M$ is also within the closed
interval $[a, b]$, because all $x_n$ are chosen from that set. But this is a
contradiction because we found an element $x_M$ that is supposedly in two
disjoint sets. Therefore, $\hat{x} \in [a, b]$.

But now, $\lim_{k\to\infty} f(x_{n_k}) = f(\hat{x})$. Because of the assumed
continuity on $f(x)$. So $\exists\+ K$ such that $\forall k \geq K$, $f(x_{n_k})
< f(\hat{x}) + 1 \in \varmathbb{R}$. Because $f(x_{n_k})$ is approaching
$f(\hat{x})$. On the other hand, we know that $f(x_{n_k}) > n_k > f(\hat{x}) +
1$ when $k$ is sufficiently large. But now we have a contradiction, $f(x_{n_k})
> f(\hat{x}) + 1$ AND $f(x_{n_k}) < f(\hat{x}) + 1$. Therefore, $f(x)$ must be
bounded above.

A similar prove can show that the function is bounded below and is left as an
exercise to the reader.

Now we can prove that $\text{sup }f(x)$ is taken on by $f(x)$. We now know
$\mathcal{R} = f([a, b]) = \{f(x)\+|\+ x\in[a,b]\}$. We know that this set is
bounded both above and below. Thus, by the LUB Theorem, we know that $\text{sup
}\mathcal{R}$ and $\text{inf }\mathcal{R}$ exists. Let $S:=\text{sup
}\mathcal{R}$, $I:=\text{inf }\mathcal{R}$. $\exists\+(y_n)_{n=1}^\infty$ such
that $y_n \in \mathcal{R}$ for all $n$, and $\lim_{n\to\infty}y_n = S$. Since
$y_n\in\mathcal{R}$, $\exists\+x_n \in [a, b]$ such that $f(x_n) = y_n$. Now the
sequence $(x_n)_{n=1}^\infty$ is bounded between $[a, b]$, so it has a
convergent subsequence $(x_{n_k})_{k=1}^\infty$ that converges to $\hat{x} \in
[a, b]$. By the same argument as before, the limit must be within the same
interval. Also, by continuity, of the function $f(x)$,
$\lim_{k\to\infty}f(x_{n_k}) = f(\hat{x})$. But this limit is $y_{n_k}$. And 
$\lim_{k\to\infty} = y_{n_k} = S$. Because if an entire sequence approaches a
number, any subsequence converges to that number as well. But that means
$f(\hat{x}) = S$. This shows that the value $S$ is taken on by the function at
some point in $[a, b]$. And the same is for the infimum. $\blacksquare$

\subsection{Intermediate Value Theorem in $\varmathbb{R}$}
\subsubsection{Theorem}
$f: [a, b] \to \varmathbb{R}$ is continuous and $f(a) \neq f(b)$. $\forall y$
between $f(a)$ and $f(b)$ ($f(a) < y < f(b)$ or $f(a) > y > f(b)$). Then
$\exists c \in (a, b): f(c) = y$.

\subsubsection{Proof}

WLOG say $f(a) < y < f(b)$. Take the set $S = \{x \in [a, b] \+|\+ f(x) \leq
y\}$. Since $a \in S$, $S \neq \emptyset$, we also know that $S \leq b$. Now
take $c = \sup S$, $a \leq c \leq b$ ($c \in [a, b]$). There are three possible
cases, $f(c)$ is either $> y, < y$, or $= y$.

Consider the case in which $f(c) < y$. If this is the case, then we can chose an
arbitrarily small $\delta > 0$ such that $f(c + delta) < y$. However, then $c +
\delta \in S$. But this causes a contradiction because $c = \sup S$ and there
should not be any element of $S$ that's larger than $c$. Therefore, this case is
impossible.

Now consider the case in which $f(c) > y$. Take some arbitrarily small $u \in
[0, \delta], \delta > 0$ such that $f(c - u) > y$. However, then $c - \delta$ is
therefore an upper bound of the set $S$, we once again reach the same
contradiction.

Therefore, since $c$ exists, $f(c) = y$.


%}}}
%{{{ Basic Calc in R^n
\chapter{Basics of Calculus}
\section{Basic Calculus Concepts in $\varmathbb{R}^d$}
\subsection{Limits in Higher Dimensions}
\begin{figure}
\centering
\begin{tikzpicture}[x=0.4cm, y=0.3cm]
\draw[<->] (-4,0)--(4,0); % l'axe des abscisses
\draw[<->] (0,-5)--(0,5); % l'axe des ordonnées

\draw (1,3) node {.};
\draw (1.5,3.5) node[anchor=south] {$(x, y)$};
\draw (-2,-2) node {.};
\draw (-2,-3.7) node[anchor=south] {$(a, b)$};

\draw[->] (1,3)--(-2,-2);
\draw[->] (1,3)--(-2, 3)--(-2,-2);
\draw[->] (1,3)--(2, 1)--(-2,-2);
\end{tikzpicture}
\end{figure}

\subsubsection{Definition of ``Approaching''}
Note that the concept of ``approaches'' is loosely defined on the plane.

Note that there are infinitely many ways in which the point $(x, y)$ can move to
$(a, b)$. It can go in a straight line, a curve, or even a spiral. Therefore
it's not a good idea to define ``approaching'' by drawing lines.


Instead, we say that $(x, y)\to(a, b)$ iff $dist(x, y\+\+;\+ a,
b)\to0$.

\subsubsection{Definition of ``Limit''}
In single variable calculus, we sometimes describe limits on the border points
with:

$$\lim_{x\to a^+} f(x) = L$$

What we mean by this is that:

$$\forall \varepsilon > 0, \exists\+\delta(\varepsilon): \forall x\in D\cap (a,
\infty), |x - a| < \delta(\varepsilon) \to |f(x) - L| < \varepsilon$$

When we take limit in higher dimensions, the path of approach can vary as long
as $(x, y) \in D$.

Let $f: D\to\varmathbb{R}$ and $D\subseteq\varmathbb{R}^2, D \neq \emptyset$,
Let $(a, b) \in D \cup \partial D$.

Then we say:
$$L = \lim_{\substack{(x, y) \to (a, b)\\(x, y)\in D}} f(x, y) \text{ if and only if:}$$
$$\forall \varepsilon > 0,
\exists\+\delta > 0, \forall (x, y) \in D: dist(x, y\+\+;\+a,b) < \delta \to |f(x,
y) - L| < \varepsilon$$

Note that this limits the approach when $(a, b) \in \partial D$, it limits the
path to inside the domain.

\subsubsection{Finding the Limit}

In higher dimensions there is no easy ways of finding the limit like the
l'h\^{o}pital's rule. The way to find the limit is to use the definition. Pick
two arbitrary paths of approach, and calculate the limit for each. If they do
not equal, then the limit doesn't exist. But if they are the same, then you pick
more lines and that value is probably the limit, then we prove it is the limit
using its definition.

\subsubsection{Examples}

Example -- Polar Substitution 1

$$\lim_{(x, y)\to(0, 0)} \frac{2xy}{x^2 + y^2}$$

$$D = \text{dom } f = \varmathbb{R}^2\setminus\{(0,0)\}, (0, 0)\in\partial D$$

We use polar coordinates, $x = r\cos(\theta)$, $y = r\sin(\theta)$, then the
expression becomes $\frac{2r^2\cos(\theta)\sin(\theta)}{r^2}$. However, this
depends on $\theta$. But that is determined by the direction of approach. For
example, if $\theta = 0$, we are approching the origin from above in the $x$
direction, and $y$ stays the same, but if $\theta = \frac{\pi}{4}$, we are
approaching the origin from above in both the $x$ and $y$ direction. But if we
plug in 0 as $\theta$, we get that the limit is $0$, but if we plug in
$\theta = \frac{\pi}{4}$, we don't get 0.

$$\therefore \lim_{(x, y)\to(0, 0)} \frac{2xy}{x^2 + y^2} = DNE$$
Example -- Polar Substitution 2

$$\lim_{(x, y)\to(0, 0)}\frac{2xy^2}{x^2 + y^2}$$

We again do our polar substitution, we get that the expression is $\frac{2r^3
\cos(\theta)\sin^2(\theta)}{r^2} = 2r\cos(\theta)\sin^2(\theta)$.

However, we see that the approach of $(x, y)\to(0, 0)$ is equivalent to $r \to
0$. Therefore, our limit becomes

$$\lim_{r\to0^+} 2r\cos(\theta)\sin^2(\theta)$$

However, we don't know what $\theta$ is, but it doesn't matter, since the trig
functions are bounded, we know that as $r$ goes to 0, so does the limit.

$$\therefore \lim_{(x, y)\to(0, 0)}\frac{2xy^2}{x^2 + y^2} = \boxed{0}$$

\subsection{Continuity}

\subsubsection{Definition of Continuity}
Let $f: D\to\varmathbb{R}$, $D\subseteq\varmathbb{R}^2$, $D\neq\emptyset$. Let
$(a, b)\in D$. We say that $f$ is \textbf{continuous} at $(a, b)$ if:

$$f(a, b) = \lim_{\substack{(x, y)\to(a,b)\\(x, y)\in D}} f(x, y)$$

Or in other terms:

$$\forall \varepsilon > 0, \exists\+ \delta>0, \forall (x, y)\in D: dist(x,
y\+;\+a,b) < \delta \to |f(x,y) - f(a,b)| < \varepsilon$$

And we say that $f$ is continuous if $f$ is continuous at $(a, b), \forall (a, b)
\in D$.

\subsubsection{Uniform Continuity}
Let $f: D \to \varmathbb{R}$, where $D \subseteq \varmathbb{R}^d$. We say $f$ is
uniformally continuous on $D$ if:
$$\forall \varepsilon > 0, \exists\+\delta > 0: \forall \vec{x}, \vec{y} \in D,
||\+\vec{x} - \vec{y}\+|| < \delta \to |f(\vec{x}) - f(\vec{y})| < \varepsilon$$

The difference between this and regular continuity is that the $\delta$ in
regular continuity is defined by both $\varepsilon$ and the specific point we
are considering. Uniform continuity, however, the value $\delta$ is independent
to the point you chose within the domain and is just dependent on $\varepsilon$.

For example, consider $y = \tan{x}$ where $D = (-\frac{\pi}{2}, \frac{\pi}{2})$.
the value required for $\delta$ for a fixed $\varepsilon$ gets smaller and
smaller as $x$ approaches both endpoints. This function is continuous but not
uniformally so. If it were uniformally continuous,
that $\delta$ value would NOT change.

\textbf{Thm}: If $f$ is uniformally continuous on $D$, then $f$ is continuous
for every point in $D$.

%}}}
%{{{ Higher dimension intro
\section{Concepts in Higher Dimensional Math}
\subsection{Definitions and Terms}
\subsubsection{Cartesian Product}
The Cartesian Plane represents the set $$\varmathbb{R}^2 := \{(x, y)\+|\+x, y\in \varmathbb(R)\}$$

This is known as the \textbf{Cartesian Product} of $\varmathbb{R}$ with
itself. The Cartesian Product of two sets $\mathcal{S}$ and $\mathcal{T}$,
$\mathcal{S} \times \mathcal{T} :=  \{(s, t)\+|\+s\in\mathcal{S},
t\in\mathcal{T}\}$.

Similarly, $\varmathbb{R}^3 = \varmathbb{R} \times \varmathbb{R} \times
\varmathbb{R} = \varmathbb{R}^2 \times \varmathbb{R}$.

\subsubsection{Shapes}
\textbf{open-ball}:
$B_r(P) := \{X\+|\+dist(X, P) < r\}$

\textbf{closed-ball}:
$\bar{B}_r := \{X\+|\+ dist(X, P) \leq r\}$

\textbf{Sphere}:
$S_r := \{X\+|\+dist(X, P) = r\}$

\subsubsection{Boundary}

Given $D \subseteq \varmathbb{R}^2, D \neq \emptyset$. We say $(a, b) \in
\partial D$, i.e. $(a, b)$ is on a \textbf{boundry point} of $D$ if $\forall
\varepsilon > 0$, there are points $(x, y)\in D$ and $(u, v) \in D^c$ ($D^c :=
\varmathbb{R}^2 \setminus D$) such that $dist(x, y\+;\+ a, b) < \varepsilon$ and
$dist(u, v\+;\+ a, b) < \varepsilon$

\begin{figure}[ht]
    \centering
    \resizebox{0.4\textwidth}{!} {
    \begin{tikzpicture}[x = 3cm, y = 3cm]
        \begin{axis} [
            xmin = 0, xmax = 6.3,
            ymin = 0, ymax = 6,
        ]
        \addplot[dashed, fill = gray, opacity = 0.3] coordinates {(1,1) (1,5) (5,5) (5,1) (1,1)};
        \addplot[mark=none] coordinates {(3, 5)} node {$\partial D$};
        \addplot[mark=none] coordinates {(3, 3)} node {$D$};
        \addplot[mark=none] coordinates {(5.8, 5.8)} node[anchor=north] {$D^c$};
        \addplot[mark=o] coordinates {(5,3)} node[anchor=north] {$(a, b)$};
        \addplot[mark=*] coordinates {(5.2,3)} node[anchor=south west] {$(u, v)$};
        \addplot[mark=*] coordinates {(4.8, 3)} node[anchor=south east] {$(x,
        y)$};
        \end{axis}
    \end{tikzpicture}
    }
    \caption{$(a,b) \in \partial D$ if we can find $(x, y)$ and $(u, v)$ for all
    $\varepsilon$}
\end{figure}

\textbf{Thm}: Because the definition is symmetrical, $\partial D^c = \partial
D$.

\subsubsection{Interior}
Let $D \subseteq \varmathbb{R}^2$. We say $(a, b)$ is an
\textbf{interior point} of $D$ if $\exists\+ r > 0:B_r(a, b)\subseteq D$.

The set of all interior points is called the \textbf{interior} of $D$ and is
written as $\text{int } D$ or $D^\circ$.

\subsubsection{Exterior}
Let $D \subseteq \varmathbb{R}^2$. We say $(a, b)$ is an
\textbf{exterior point} for $D$ if it is an interior point of $D^c$.
$\exists\+ r > 0: B_r(a, b) \subseteq D^c$.

The set of all exterior points for $D$ is the \textbf{exterior} of $D$,
written as $\text{ext } D$.

\textbf{Thm}: For any $D \subseteq \varmathbb{R}^2$, $\varmathbb{R}^2 =
\text{int } D \cup \partial D \cup \text{ext } D$. And $\text{int } D \cap
\partial D = \emptyset$, $\text{int } D \cap \text{ext } D = \emptyset$,
$\partial D \cap \text{ext } D = \emptyset$.

\textbf{Thm}: $\text{int } D = \text{ext } D^c$ and $\text{ext } D =
\text{int } D^c$

\textbf{Thm}: $\text{ext } D \subseteq D^c$
\subsubsection{Closure}
The \textbf{closure} of $D \subseteq \varmathbb{R}^2$:
$$\bar{D} := D \cup \partial D$$

\textbf{Thm}: $\partial \bar{D} = \partial D$, $\text{int } \bar{D} =
\text{int } D$, and $\text{ext } \bar{D} = \text{ext } D$

\textbf{Thm}: $\text{int } D \subseteq D \subseteq \bar{D}$.

\subsubsection{Ordered Pair}
The ordered pair $(a, b)$ can be thought of as a set, but a set is inheritly
unordered. To express the order, we can do the following: $(a, b) = \{\{a\},
\{a, b\}\}$. Now we know that $a$ is the first element because it appears in
both subsets.

We can then expand this into higher dimensions like the following:
$(a, b, c) = ((a, b), c)$. Note that this means that $((a, b), c) \neq (a, (b,
c))$. But this does not matter to us.

\textbf{Fundamental Postulate of Ordered Pairs}:

$(a_1, a_2, a_3, \dots, a_n) = (b_1, b_2, b_3, \dots, b_n)$ if and only if $a_1
= b_1 \wedge a_2 = b_2 \wedge \dots \wedge a_n = b_n$.
\subsubsection{Vector and Points}
Vectors are quantities of directionality and length, its location does not
matter. Points are just positions in space. In higher dimensions with no
ambiance space (flat space surrounding the surface, i.e. the shortest distance
in the ambiant space is the straight line),  we define a vector as all
the lines with the same direction at a certain point.

However, the nice thing about $\varmathbb{R}^d$ is that there is always ambiance
space, so we will not make any notational distinction between a point and a
vector.

The length of a vector in $d$ space is defined as:
$$||\vec{a}|| := dist(\vec{0}, \vec{a}) = \sqrt{\sum_{i =
1}^d a_i^2}$$


\subsection{Distance}
\subsubsection{Euclidean Distance}
The distance function in one space between two points $a$ and $b$ is simply $|a
- b|$. However, we can also write it in the following way: $\sqrt{(a - b)^2}$

In $\varmathbb{R}^2$, the distance function is:
$$dist(x, y\+\+;\+a,b) := \sqrt{(x - a)^2 + (y - b)^2}$$

And in $\varmathbb{R}^3$, the distance function is:
$$dist(x, y, z\+;\+ a,b,c) := \sqrt{(x - a)^2 + (y - b)^2 + (c - z)^2}$$

The generalized form of Euclidean Distance in $N$ space is:
$$dist(\vec{p}, \vec{q}) = \sqrt{\sum_{j = 1}^N (p_j - q_j)^2}$$

This is known as the \textbf{Euclidean Distance}. We use this specific
definition of distance because this is preserved under an infinite set of rigid
or isometric motions, such as rotation, reflection, translation, etc.

The Euclidean Distance formula is also used because it has its roots within the
Pythagorean Theorem. However, that is a very high level formula, and using it
from the very basics of geometry is cheating. However, there is a clever way to
limit the distance formula down to almost only the Euclidean Distance formula.
We set the following restrictions on a distance function:
\begin{itemize}
    \item Translation-Invariant and symmetric\\
        $\forall T_{h, k} (x, y) \mapsto (x + h, y + k)$\\
        $dist(x + h, y + k\+;\+\hat{x} + h, \hat{y} + h) = dist(x, y \+;\+
        \hat{x}, \hat{y})$\\
        $\therefore dist(x, y \+;\+ \hat{x}, \hat{y}) = f(|x - \hat{x}|, |y -
        \hat{y}|)$\\
        where $f$ is a function defined on $[0, \infty)\times[0, \infty)$
    \item Basic Reflection Symmetry (``isotropy'' or direction independent)\\
        $dist(x, y\+;\+ 0, 0) = dist(y, x\+;\+ 0, 0)$\\
        $\therefore f(u, v) = f(v, u) \+\forall u\geq 0, v \geq 0$
    \item Self-Distance of $(0, 0)$\\
        $dist(0, 0 \+;\+ 0, 0) = 0$\\
        $\therefore f(0, 0) = 0$
    \item Recreate the Standard Distance Function of Each Axis\\
        $dist(x, 0\+;\+\hat{x}, 0) = |x - \hat{x}|$ and $dist(y, 0\+;\+\hat{y},
        0) = |y - \hat{y}|$\\
        $\therefore f(u, 0) = u, f(0, v) = v$, $\forall u \geq 0, v \geq 0$
    \item Asymptotic Flatness
        \begin{figure}[ht]
            \begin{tikzpicture}
            \draw[<->] (-5,0)--(1,0); % l'axe des abscisses
            \draw[<->] (-4,-1)--(-4,3); % l'axe des ordonnées

            \draw (0,2) node[anchor=south] {$(x, v_0)$};
            \draw (0,1) node[anchor=west] {$v_0 \geq 0$, fixed};
            \draw (-2, 0) node[anchor=north] {$u = |x| \geq 0$};
            \draw (-2.4, 1.6) node[anchor=south] {$f(|x|, |y|) = f(u, v_0)$};

            \draw[-] (-4,0)--(0,0) -- (0,2) -- cycle;

            \draw (7, 2) node {$\lim_{u\to\infty} \frac{f(u, v_0)}{u} = 1
            \+\+\+(v_0 \text{ fixed})$};
            \draw (7, 1) node {$\lim_{v_0\to\infty} \frac{f(u, v_0)}{v} = 1
            \+\+\+(u \text{ fixed})$};
            \end{tikzpicture}
        \end{figure}

    \item Continuity\\
        $f$ should be continuous in its two variables
    \item Set of Isometry that Fix $(0, 0)$ is an Infinite Set\\
        An isometry bijective transformation of the plane onto itself that preserves
        distances.
\end{itemize}

Given all of these, we can guess a form of the theorem, we claim that the
form of the theorem is the following:


\begin{align*}
f(u, v) &= F(G(u) + G(v))\\
        &F: [0, \infty]\to\varmathbb{R}\\
        &G: [0, \infty]\to\varmathbb{R}
\end{align*}

\textbf{Thm}: $\exists$ only one suitable pair $F, G$ is
$$G(x) = x^2$$
$$F(x) = \sqrt{x}$$

Then we arrive at the Euclidean Formula
$$\sqrt{(x_1 - x_0)^2 + (y_1 - y-_0)^2}$$

\textbf{Properties}:
\begin{enumerate}
    \item $dist(\vec{p}, \vec{q}) = dist(\vec{q}, \vec{p})$
    \item $dist(\vec{p}, \vec{q}) \geq 0$
    \item $dist(\vec{p}, \vec{q}) = 0 \leftrightarrow \vec{p} = \vec{q}$
\end{enumerate}

\subsubsection{Basic Distance Bounds Lemma}
 $\forall \vec{p}, \vec{q} \in
\varmathbb{R}^d$, and $\forall j \in \{1,2,3,\dots,d\}$:
$$|p_j - q_j| \leq dist(\vec{p}, \vec{q}) \leq \sqrt{d} \max_{1 \leq k \leq
d} |p_k - q_k|$$

\textbf{Proof}

Note that $(p_j - q_j)^2 \leq \sum_{k = 1}^d (p_k - q_k)^2$ is trivial, because
you can only add positive number when you add squares. Now let's take the square
root, and we get
$$\sqrt{(p_j - q_j)^2} = |p_j - q_j| \leq \sqrt{\sum_{k = 1}^d (p_k - q_k)^2} =
dist(\vec{p}, \vec{q})$$

To prove the other inequality, it is trivial as well. We can just factor out the
length of the vector $d$ and multiply that with the maximum value of the
distance vector. Then we get:

$$dist(\vec{p}, \vec{q}) = \sqrt{\sum_{k = 1}^d (p_k - q_k)^2} \leq \sqrt{d \max_{1 \leq k \leq d} (p_k -
q_k)^2} = \sqrt{d} \max_{1 \leq k \leq
d} |p_k - q_k|$$

\textbf{Cor}: Componentwise Nature of Convergence

Let $(\vec{p}_n)_{n = 1}^\infty$ be a sequence in $\varmathbb{R}^d$, and let
$\vec{p} \in \varmathbb{R}^d$. Then $\vec{p}_n \to \vec{p}$ if and only if
$p_{n|j} \to p_j$ ($\vec{p} = (p_1, p_2, p_3, \dots, p_d)$ and $\vec{p}_n =
(p_{n|1}, p_{n|2}, \dots, p_{n|d})$). Otherwise known as convergence of points
can be reduced to conversion of dimensions.

This follows directly from the inequality, because if the total distance goes to
0, then $|p_j - q_j|$ goes to 0. Therefore if the points converge, the
corresponding coordinates must converge.

To prove the converse, we prove using the other side of the distance bounds. If
all $d$ coordinates are going to 0, then if we take the maximum, that would be
going to 0. (the maximum of a sequence is less than the sum of the sequence, but
if every term of the sum is going to 0, then the sum is going to 0, then the
maximum is going to 0). Therefore the distance must also be going to 0. Thus the two points
converges.

\subsubsection{Distances Between Sets}

We define the distance between a point $\vec{p}$ and a set $D$ as:

$$dist(\vec{p}; D) = \inf_{\vec{d} \in D} dist(\vec{d} ; \vec{p})$$

We also define the distance between two sets $D_1$ and $D_2$ as:

$$dist(D_1 ; D_2) = \inf_{\substack{\vec{p} \in D_1\\\vec{q} \in D_2}}
dist(\vec{p} ; \vec{q})$$

\subsection{Coordinate Space}
\subsubsection{Space}
$$\varmathbb{R}^d = \{(x_1, x_2, \dots, x_d) \+|\+ x_1, \dots, x_d \in
\varmathbb{R}\}$$
\subsubsection{Line}
$$l = \{(a_1 + tb_1, a_2 + tb_2, \dots, a_d + tb_d)\+|\+t\in\varmathbb{R}\}$$
Where $b_1^2 + b_2^2 + \dots + b_d^2 > 0$
\subsubsection{Pythagorean Theorem}
\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
    \draw[<->] (-5,0)--(1,0); % l'axe des abscisses
    \draw[<->] (-4,-1)--(-4,3); % l'axe des ordonnées

    \draw (0,2) node[anchor=south] {$(a, b)$};
    \draw (-2, 0) node[anchor=north] {$|a|$};
    \draw (0, 1) node[anchor=west] {$|b|$};
    \draw (-2.4, 1.6) node[anchor=south] {$\sqrt{a^2 + b^2}$};

    \draw[-] (-4,0)--(0,0) -- (0,2) -- cycle;
    \end{tikzpicture}
\end{figure}

We declare the x-axis and y-axis to be $\perp$.

And from the picture we can see that the Pythagorean Theorem is true for all
axial triangles. But note that the Euclidean distance is preserved under
rotation. We therefore deduce that the Pythagorean Theorem is true for any
right triangles.

\subsubsection{Perpendicularity}

Now we can define perpendicularity. Two vectors $\vec{a}$ and $\vec{b}$ are
perpendicular if and only if $dist(\vec{a}\+;\+\vec{b})^2 =
dist(\vec{0}\+;\+\vec{a})^2 + dist(\vec{0}\+;\+\vec{b})^2$, where $\vec{a}
\neq 0, \vec{b} \neq 0$.

Note that we call the two orthogonal if one of the vectors is 0.

\subsubsection{Dot Product}
If we stay in
$\varmathbb{R}^2$, algaberically we get:
$$(a_2 - b_2)^2 + (a_2 - b_2)^2 = (a_1^2 + a_2^2) + (b_1^2 + b_2^2)$$
Now if we subtract both side by $a_1^2 + a_2^2 + b_1^2 + b_2^2$, and divide
by $-\frac{1}{2}$, we get:

$$a_1b_2 + a_2b_2 = 0$$

\textbf{Thm}: $\vec{a} \perp \vec{b}$ iff $a_1b_1 + a_2b_2 = 0$ where
$\vec{a} \neq \vec{0}$ and $\vec{b} \neq \vec{0}$.

We define this product as the \textbf{dot product}.

$$\vec{a} \cdot \vec{b} = a_1b_1 + a_2b_2 \text{ in } \varmathbb{R}^2$$

In $\varmathbb{R}^d$,
$$\vec{a} \cdot \vec{b} = \sum_{j = 1}^d a_jb_j$$

\textbf{Thm} (properties):
\begin{itemize}
    \item $\vec{a} \cdot \vec{b} = \vec{b} \cdot \vec{a}$
    \item $\vec{a} \cdot (\vec{b} + \vec{c}) = (\vec{a} \cdot \vec{b}) + (\vec{a}
        \cdot \vec{c})$ (Distributive law)
\end{itemize}

\subsection{Functions}
\begin{figure}[ht]
\centering
\resizebox{0.5\textwidth}{!}{\begin{tikzpicture}
    \begin{axis} [ 
        scale only axis,
        grid=major,
        axis lines=middle,
        inner axis line style={->},
        ymin=-2.5,
        xmin=-2.5,
        ymax=2.5,
        xmax=2.5,
        zmin=0,
        zmax=2.5,
        y dir=reverse,
    ]
    \addplot3[surf, color=red, fill=gray, fill opacity=0.5, smooth, thick] coordinates {(0,-2,0) (-1,1,0) (1,2,0) (1,0,0) (2,-1,0) (0,-2,0)} node [pos=0.9, below left] {$D$};
    \addplot3[surf, color=blue, fill=gray, fill opacity=0.5, smooth, thick] coordinates {(0,-2,2) (-1,1,2) (1,2,2) (1,0,2) (2,-1,2) (0,-2,2)} node [pos=0.9, below left] {$G_f$};
    \end{axis}
\end{tikzpicture}}
\caption{The constant function $f(x, y) = 2$ over $D$}
\end{figure}


\subsubsection{Domain}
The domain is a subset of the xy plane ($\varmathbb{R}^2$).

Let the domain of the function $f(x, y)$ be an ordered pair within the curve, or
$(x, y) \in D$

\subsubsection{Range}

Let's call the range of function $f$, $G_f = \{x, y, f(x,y)\+|\+(x, y)
\in D\} \subseteq \varmathbb{R}^3$.

\subsubsection{One-to-One}
We define the one-to-one in the following manner, if $f(x, y)$ is one-to-one:

$$(x, y), (u, v) \in D, f(x, y) = f(u, v)\+\+\text{iff}\+\+x = u \wedge y = v$$

There is no vertical line test equivalent in higher dimensions. We can only test
one-to-one-ness in this manner. It is very hard to find a one-to-one function in
higher dimensions, but one of them is the \textit{interlacing} function ($*$).

Let $x, y \in [0, 1] \times [0, 1]$, with $x = 0.d_1d_2d_3\dots$ and $y =
0.e_1e_2e_3\dots$, then the interlacing function $f(x, y) = x * y =
0.d_1e_1d_2e_2d_3e_3\dots$. Fun fact, this function is continuous and
differentiable \textit{nowhere}.

\subsubsection{Graphs of Functions}
In general, the \textbf{graph} of $y = f(x_1, x_2, x_3, \dots, x_d) =
f(\vec{x})$ is the set $G = \{(x_1, x_2, \dots, x_d, y) \+|\+ y = f(\vec{x})\}$.
Note that this exists in $\varmathbb{R}^{d+1}$
%}}}
%{{{ Inequalities
\section{Inequalities}
Before we go futher, we need to take a detour with inequalities, which will
come in very handy later in the curriculum when taking limits.
\subsection{Level of Operations}
Powers/root $\to$ Multiplation/division $\to$ addition/subtraction $\to$
succession/pretrition

\subsection{Triangle Inequality (for Absolute Values)}
The triangle inequality states that:
$$0 \leq |a + b| \leq |a| + |b|$$
With equality iff $ab\geq0$ or $a$ and $b$ have the same signs.

\subsection{AM-GM}

$$\mu = [x_1, x_2, \dots, x_n] \text{ and } x_1, x_2, x_3, \dots, x_n \geq 0$$
``Multiset'' $\mu = \{(x, n), (y, m), \dots\}$

We define the arithmetic mean of a multiset as:
$$A(\mu) = \frac{x_1 + x_2 + \dots + x_n}{n}$$

And the geometric mean as:
$$G(\mu) = \sqrt[\uproot{2}n]{x_1x_2x_3\dots x_n}$$

\subsubsection{AM-GM Inequality}
$A(\mu) \geq G(\mu)$, with equality iff all elements
of $\mu$ are the same.

\subsubsection{Proof}
This is done by mathematical induction. Base case is $n = 2$, then $\mu = [x,
y]$. Then $A(\mu) = \frac{x + y}{2}, G(\mu) = \sqrt{xy}$

We know by the trivial inequality that $(\sqrt{x} + \sqrt{y})^2 \geq 0$, with
equality case happening iff $x = y$. Then we get:
\begin{align*}
    x - 2 \sqrt{xy} + y &\geq 0\\
        \frac{x + y}{2} &\geq \sqrt{xy}\\
                 A(\mu) &\geq G(\mu)
\end{align*}

Now we induce on $n$, we seek to prove that case $n$ implies case $2n$.

$\mu = [x_1, x_2, \dots, x_n, y_1, y_2, \dots, y_n]$

Then we know that
\begin{align*}
    A(\mu) &= \frac{A(\mu_x) + A(\mu_y)}{2}\\
           &\geq \frac{G(\mu_x) + G(\mu_y)}{2}\\
           &\geq \sqrt{G(\mu_x)G(\mu_y)}\\
           &= G(\mu)
\end{align*}

Note that in all inequalities used, the equality case is always when all $x_n$
and $y_n$ are the same element, therefore the equality case holds in all cases
where the length of the list is $2^n$.

Now we prove that case $n$ implies $n-1$

$\mu = [x_1, x_2, x_3, \dots, x_{n - 1}]$

Note that we can construct $\mu' = [x_1, x_2, x_3, \dots, x_{n - 1}, A(\mu)]$

Note that $A(\mu') = A(\mu)$, and since the AM-GM inequality is true for $\mu'$
by the assumption, we know

\begin{align*}
    A(\mu') = A(\mu) &\geq \sqrt[\uproot{2} n]{x_1x_2x_3\dots x_{n - 1}A(\mu)}\\
                     &\geq \sqrt[\uproot{2} n]{G(\mu)^{n-1}A(\mu)}\\
                     &\geq \sqrt[\uproot{2} n]{G(\mu)^{n-1}G(\mu)}\\
                     &\geq G(\mu)
\end{align*}

\subsection{Young's Inequality}
\subsubsection{H\"{o}lder Conjugate}
$q$ is said to be the H\"{o}lder Conjugate of $p$:
$$q := p^* := \frac{p}{p - 1}$$

Note that $q > 1$ and $\frac{1}{p} + \frac{1}{q} = 1$

\subsubsection{Young's Inequality}
$a, b \geq 0$; $p > 1$; $q = p^*$, then Young's Inequality states that:
$$ab \leq \frac{a^p}{p} + \frac{b^q}{q}$$
With equality case iff $a^p = b^q$

\subsubsection{Proof}
We first proof Young's Inequality assuming that $p, q \in \varmathbb{Q}$.
We can rewrite $p = \frac{n + m}{n}$ and $q = \frac{n + m}{m}$ for some $n, m
\in \varmathbb{N}$. Now Young's Inequality turns into:

$$ab \leq \frac{na^{\frac{n + m}{n}} }{n + m} + \frac{mb^{\frac{m + n}{m}} }{n + m}$$

If we let $x = a^{\frac{1}{n}}$ and $y = b^{\frac{1}{m}}$. Then the inequality
turns into:

$$ab = x^ny^m = \leq \frac{nx^{n + m} + my^{n + m}}{n + m}$$

And that is true by weighted AM-GM, with equality iff $x = y$, which equals to
$a^{\frac{1}{n}} = b^{\frac{1}{m}}$, which equals to 

We can prove the inequality for irrational by taking limits, because
$\forall n \geq n_0: f(n) \leq g(n)$, and the limits of both $f(x)$ and $g(x)$ as
$n\to\infty$ exists and are finite, then we know that $\lim_{n\to\infty}f(n)
\leq \lim_{n\to\infty} g(n)$. When $p$ and $q$ are irrational, we construct
$\{p_n\}$ and $\{q_n\}$ as two sequences of rationals that approaches $p$ and
$q$, the left hand side of Young's Inequality is unaffected by the limit, and by
what we've just said about limits, we know that:
$$ab \leq \lim_{n\to\infty} \frac{a^{p_n}}{p_n} + \frac{b^{q_n}}{q_n} =
\frac{a^p}{p} + \frac{b^q}{q}$$

\subsection{H\"{o}lder's Inequality}
\subsubsection{$p$-norm}
In $\varmathbb{R}^2$: let $||(a, b)||_p = (|a^p| + |b^p|)^{1/p}$ for any $p \in
\varmathbb{R} > 1$, this is known as the $p$-norm of a vector. Note that $||(a,
b)||_2 = ||(a, b)|| = \sqrt{a^2 + b^2}$

\subsubsection{H\"{o}lder's Inequality}
$\forall (a, b), (c, d) \in \varmathbb{R^2}, p > 1, q = p^*$:
$$0 \leq |ac| + |bd| \leq ||(a, b)||_p||(c, d)||_q$$

Equality happens iff $(\frac{a}{s})^p = (\frac{c}{t})^q$ and $(\frac{b}{s})^p =
(\frac{d}{t})^q$ where $s = ||(a, b)||_p$ and $t = ||(c, d)||_q$

The whole inequality is only 0 if both points have at least one component on the
axis.

\subsubsection{Proof}
We know that the absolute value of the product is equal to the product of the
absolute value. If we apply Young's Inequality to $|\frac{a}{s}||\frac{c}{t}|$
and $\left|\frac{b}{s}\right|\left|\frac{d}{t}\right|$, we get:

$$\left|\frac{a}{s}\right|\left|\frac{c}{t}\right| \leq \frac{|a|^p}{p|s|^p} + \frac{|c|^q}{q|t|^q}$$
$$\left|\frac{b}{s}\right|\left|\frac{d}{t}\right| \leq \frac{|b|^p}{p|s|^p} + \frac{|d|^q}{q|t|^q}$$

Now we add:

$$\frac{1}{st}(|ac| + |bd|) \leq \frac{|a|^p + |b|^p}{p|s|^p} + \frac{|c|^q +
|d|^q}{q|t|^q}$$

Note that $|a|^p + |b|^p = |s|^p$ and $|c|^q + |d|^q$, so everything cancels

$$\frac{1}{st}(|ac| + |bd|) \leq \frac{1}{p} + \frac{1}{q}$$

But we know that $q = p^*$, therefore, $\frac{1}{p} + \frac{1}{q} = 1$, and we
get:

$$|ac| + |bd| \leq st = ||(a, b)||_p \cdot ||(c, d)||_q$$

The equality case occurs at basically the same way as Young's Inequality's
equality case.

\subsection{Cauchy-Schwarz Inequality}
This is a special case of H\"{o}lder's Inequality, where $p = q = 2$. (This is
very important, 2 is the \textit{only} value that is its own conjugate, this is
why Euclidean distance is so special)

If we plug in 2 for $p$ and $q$ and use the Triangle Inequality:
$$|ac + bd| \leq |ac| + |bd| \leq ||(a, b)||\cdot||(c,d)||$$

Now if $(a, b) = \vec{u}$ and $(c, d) = \vec{v}$, the inequality becomes:
$$|\+\vec{u} \cdot \vec{v}\+| \leq ||\+\vec{u}\+||\cdot||\+\vec{v}\+||$$

With equality iff $\vec{v} \+||\+ \vec{u}$
\subsection{Minkowski's Inequality}
Take two vectors, $\vec{u}$ and $\vec{v}$, the inequality states that:

$$||\+\vec{u} + \vec{v}\+||_p \leq ||\+\vec{u}\+||_p + ||\+\vec{v}\+||_q$$

With equality iff $\vec{v} = t\vec{u}$ or $\vec{u} = t\vec{v}$ for some $t \geq
0$.

\subsubsection{Proof}
The calculation works in any dimension, for simplicity's sake, let's work in
$\varmathbb{R}^2$, let $\vec{u} = (a, b)$ and $\vec{v} = (c, d)$

\begin{align*}
||\+\vec{u} + \vec{v}\+||_p^p = |\+a + c\+|^p + |\+b + d\+|^p &= |\+a + c\+||\+a + c\+|^{p - 1} + |\+b + d\+||\+b + d\+|^{p - 1}\\
\intertext{Now we factor and use the Triangle Inequality for Absolute Value:}
&\leq (|a| + |c|)|\+a + c\+|^{p - 1} + (|b| + |d|)|\+b + d\+|^{p - 1}\\
\intertext{Now we rearrange the terms:}
&= (|a||a + c|^{p - 1} + |b||b + d|^{p - 1}) + (|c||a + c|^{p - 1} + |d||b +
d|^{p - 1})\\
\intertext{Now we apply H\"{o}lder's Inequality, we get:}
&\leq (|a|^p + |b|^p)^{\frac{1}{p}}(|a + c|^{(p - 1)q} + |b + d|^{(p -
1)q})^{\frac{1}{q}} + (|c|^p + |d|^p)^{\frac{1}{p}}(\dots)\\
\intertext{Note that $q = p^*$, therefore $(p - 1)q = p$:}
&= (||\+\vec{u}\+||_p + ||\+\vec{v}\+||_p) ||\+\vec{u} + \vec{v}\+||_p^{\frac{p}{q}}\\
\intertext{Since $q = p^*$, we know that $\frac{p}{q} = p - 1$, and if we bring the
inequality to the original left hand side:}
&\leq (||\+\vec{u}\+||_p + ||\+\vec{v}\+||_p)\+||\+\vec{u} + \vec{v}\+||_p^{p - 1}
\end{align*}
Now we divide:
$$||\+\vec{u} + \vec{v}\+||_p \leq ||\+\vec{u}\+||_p + ||\+\vec{v}\+||_p$$

Now let's consider the equality cases. If one of the vectors is 0, then the
inequality is trivially true.

If neither vectors are the 0 vector, we see the equality cases of all the
inequalities used to prove Minkowski's. First we used the triangle inequality,
which only has equality when $ac \geq 0$ and $bd \geq 0$. Next we applied
H\"{o}lder's, which has equality case when both coordinates are proportional.
Therefore, the two vectors must be positive multiples of one another.

\subsection{Triangle Inequality}
$$||\+\vec{u} + \vec{v}\+|| \leq ||\+\vec{u}\+|| + ||\+\vec{v}\+||$$

Note that this is just a special case of Minkowski's Inequality where $p = 2$.

This can be generalized by mathematical induction to $dist(\vec{p_0}, \vec{q_n})
\leq \sum_{j = 1}^n dist(\vec{p}_{j - 1}, \vec{p}_{j})$ (Otherwise known that the
shortest distance between two points is the straight line, or the \textbf{Generalized
Triangle Inequality} or the ``Broken Line Inequality'')

\subsection{Reverse Triangle Inequality}

From the Triangle Inequality we know:

$$||\+\vec{x} + \vec{y}\+|| \leq ||\+\vec{x}\+|| + ||\+\vec{y}\+||$$

Let $\vec{z} = \vec{x} + \vec{y}$, then we subtract, we get:

$$||\+\vec{z}\+|| \leq ||\+\vec{x}\+|| + ||\+\vec{z} - \vec{x}\+||$$
$$||\+\vec{z}\+|| - ||\+\vec{x}\+|| \leq ||\+\vec{z} - \vec{x}\+||$$

Because $\vec{x}$ and $\vec{z}$ are just variables, we can switch them, and we
get:

$$||\+\vec{x}\+|| - ||\+\vec{z}\+|| \leq ||\+\vec{x} - \vec{z}\+|| = ||\+\vec{z}
- \vec{x}\+||$$

Since the right hand side is greater than both of the above qualities, we can
just say it's greater than the absolute value of the difference. Hence we get
the Reverse Triangle Inequality:

$$||\+\vec{z} - \vec{x}\+|| \geq |||\+\vec{z}\+|| - ||\+\vec{x}\+|||$$

%}}}
%{{{ Linear Algebra
\section{Linear Algebra}
Before we delve deeper into calculus, it is worthwile to take a little detour
into the realm of linear algebra.

\subsection{Linear Mappings/Functions}
Consider a function $\vec{l}: \varmathbb{R}^d \to \varmathbb{R}^e$. $\vec{l}$ is
called a \textbf{linear mapping} if the image of any k-flat in $\varmathbb{R}^d$
($0 \leq k \leq d$) is a \~{k}-flat in $\varmathbb{R}^e$, where $\~{k} \leq k$.
Basically these definitions ``preserve flatness.''

\subsection{Sufficient Conditions for Linear Mapping}
For $\vec{l}$ to be a linear function, it must have the following properties:

\begin{enumerate}
    \item Additivity: $\vec{l}(\vec{x} + \vec{y}) = \vec{l}(\vec{x}) +
        \vec{l}(\vec{y})$
    \item Homogeneity: $\vec{l}(c\vec{x}) = c\vec{l}(\vec{x})$
\end{enumerate}
\vspace{0.2cm}

\textbf{Proof}: Take k-flat in $\varmathbb{R}^d$ $F = \{t_1\vec{a}_1 +
t_2\vec{a}_2 + \dots + t_k\vec{a}_k \+|\+ t_1, t_2, \dots, t_k \in
\varmathbb{R}\}$. Here, $\vec{a}_1, \vec{a}_2, \dots, \vec{a}_k \in
\varmathbb{R}^d$ such that all of them are independent of each other, i.e. no
$\vec{a}_i$ is a linear combination of $\{\vec{a}_j \+|\+ j \neq i\}$. Because
otherwise $\vec{a}_i$ can be broken up and the dimension of $F$ will be reduced.
This can also be phrased as the set of $\vec{a}_i$ must satisfy the following condition:
$F = \vec{0} \implies t_1 = t_2 = \dots = t_k = 0$. Because if there exists a
non-trivial solution, we can subtract all the terms with their coefficient being
0, and divide through the non-zero coefficient, then we would express
$\vec{a}_i$ as a linear combination of others. Therefore, if all the vectors are
independent, the equation $F = \vec{0}$ only has the trivial solution.

So now consider the linear mapping $\vec{l}$. We can distribute because the
mapping is additive, and we can factor out the coefficients because it is
homogeneous.

\begin{align*}
    \vec{l}(F) &= \{\vec{l}(t_1\vec{a}_1 + \dots + t_k\vec{a}_k) \+|\+ t_1, \dots, t_k \in \varmathbb{R}\}\\
               &= \{t_1 \vec{l}(\vec{a}_1) + \dots + t_k \vec{l}(\vec{a}_k)\}
\end{align*}

If we denote $\vec{l}(\vec{a}_i) := b_i$, we can rewrite $\vec{l}(F) = \{t_1
b_1 + \dots + t_k b_k\}$. This is a \~{k}-flat for some \~{k} $\leq$ k, because
we can always pick a subset of $\vec{b}_i$ such that they are all independent of
each other (unless they are all $\vec{0}$, but in that case $\vec{l}(F)$ is just
the 0-flat), and then we reduce, and the final result would be a \~{k}-flat.
\vspace{0.2cm}

\textbf{Example}: Homogeneity but not additive and therefore non-linear.

Note that if we are dealing with one dimension, all functions that have an
unlimit range are linear mappings. However, in $\varmathbb{R}^2$, homogeneity is
not enough.

Consider the function:

\[
    \vec{f}(x, y) =
        \begin{cases}
            (\frac{x^3 + y^3}{x^2 + y^2}, \frac{xy^2}{x^2+y^2}) & \mbox{if } (x,
            y) \neq (0, 0)\\
            (0, 0) & \mbox{if } (x, y) = (0,0)
        \end{cases}
\]

$\vec{f}$ is homogeneous as it is a composition of homogeneous functions, but it
does not preserve flatness for obvious reasons, and that is because this
function is not additive.

\subsection{Matrices}
\subsubsection{Definition}
Matrices are list of vectors, with each column being a single vector. For
example, $((1,2,0),(-1, 3, 4))$ can be rewritten as

$$\left[\begin{array}{cc}
    1 & -1\\
    2 & 3\\
    0 & 4\\
\end{array} \right]$$

This is known as a matrix, and an element of a matrix can be denoted with two
subscripts with the lower case of the matrix' name, with the first subscript denoting the row number, and the second denoting the
column number. If the above matrix is $A$, then $a_{11} = 1$ and $a_{31} = 0$.

\subsubsection{Transposition}
$B = A^T$ ($B$ is $A$ transposed), then $b_{ij} = a_{ji}$

\subsubsection{Determinant}

Geometrically, if you treat the matrix $A \in \varmathbb{R}^{d \times n}$ as an
$d$-dimensional strucutre composed of $n$ $d$-dimensional vectors, then the
determinant is the ``volume'' of said structure.

Algebraically

\begin{align*}
    \det A &= \sum_{\sigma \in S_n} (sgn \+ \sigma) a_{1 \sigma_1} a_{2 \sigma_2}
    \dots a_{n\sigma_n}\\
    \sigma &= (\sigma_1, \sigma_2, \dots, \sigma_n)
\end{align*}

where 

$$sgn \+ \sigma = \frac{\Pi_{i < j} (x_{\sigma_i} - x_{\sigma_j})}{\Pi_{i
< j} (x_i - x_j)}$$

where $x_1, x_2, \dots, x_n$ are distant values. This function, in a more
explicit/less percise form is:

$$sgn = (-1)^N$$

where $N$ is the number of ordered pairs $(i, j)$ where $i < j$ but $\sigma_j <
\sigma_i$.

And $S_n$ is the set of permutations of $\sigma$

\textbf{Properties of the Determinant Function}:
\begin{enumerate}
    \item $\det I = 1$ (unit property)
    \item $\det[\vec{a}_1, \dots, c\vec{a}_j, \dots, \vec{a}_n] = c \det A$
\end{enumerate}

\subsubsection{Dot Product}
If we have $A = [\vec{a}_1 \vec{a}_2 \dots \vec{a}_n]
\in \varmathbb{R}^{d \times n}$ and $B = [\vec{b}_1 \vec{b}_2 \dots \vec{b}_m]
\in \varmathbb{R}^{d \times m}$, then we say that the dot product of the two is:

\[
    A \cdot B = \left[\begin{array}{cccc}
        \vec{a}_1 \cdot \vec{b}_1 & \vec{a}_1 \cdot \vec{b}_2 & \dots &
        \vec{a}_1 \cdot \vec{b}_m \\
        \vec{a}_2 \cdot \vec{b}_1 & \vec{a}_2 \cdot \vec{b}_2 & \dots &
        \vec{a}_2 \cdot \vec{b}_m \\
        \vdots & \vdots & \ddots & \vdots\\
        \vec{a}_n \cdot \vec{b}_1 & \vec{a}_n \cdot \vec{b}_2 & \dots &
        \vec{a}_n \cdot \vec{b}_m
    \end{array}\right]
\]

This operation is distributive, which means that $A \cdot (B + C) = A \cdot B +
A \cdot C$. However, this product is not associative, i.e. $A \cdot (B \cdot C)
\neq (A \cdot B) \cdot C$.

To solve this problem, we have the matrix multiplication operator.

\subsubsection{Matrix Multiplation}
For $A \in \varmathbb{R}^{n\times d}$, $B \in
\varmathbb{R}^{d\times m}$, we define the matrix multiplication product to be:

$$\boxed{AB := A^T \cdot B} \in \varmathbb{R}^{n\times m}$$

If we call $C = AB$, then we can say that

$$c_{ij} = \sum_{k = 1}^d a_{ik} b_{kj} \text{  } (1 \leq i \leq n, 1 \leq j
\leq m)$$

As a whole, the $C$ column would look like this (here we denote $\vec{\alpha}_i$
as the $i^{th}$ row of $A$ and suppose $A$ has $p$ rows). Then:

\[
    C = \left[\begin{array}{ccc}
        \vec{\alpha}_1 \cdot \vec{b}_1 & \dots & \vec{\alpha}_1 \cdot
        \vec{b}_m\\
        \vdots & \ddots & \vdots\\
        \vec{\alpha}_p \cdot \vec{b}_1 & \dots & \vec{\alpha}_m \cdot
        \vec{b}_m
    \end{array}\right]
\]

Note this operation is both distribut and associative, quick proof:

$$A (BC) \overset{?}{=} (AB) C$$

$$[A(BC)]_{ij} = \sum_{k} a_{ik} [BC]_{kj} = \sum_k a_{ik} (\sum_l b_{kl} c_{lj})
= \sum_{(k, l)} a_{ik} b_{kl} c_{lj}$$

$$[(AB)C]_{ij} = \sum_l [AB]_{il} c_{li} = \sum_l(\sum_k a_{ik} b_{kl})c_{lj} =
\sum_{(l, k)} a_{ik} b_{kl} c_{lj}$$

Therefore the values of the two matrices are the same. We also have to prove
that they are the same size. If $A \in \varmathbb{R}^{n \times d}$, $B \in
\varmathbb{R}^{d \times e}$ and $C \in \varmathbb{R}^{e \times m}$. $BC \in
\varmathbb{R}^{d \times m}$ and $A(BC) \in \varmathbb{R}^{n \times m}$. $AB \in
\varmathbb{R}^{n \times e}$ and $(AB)C \in \varmathbb{R}^{n \times m}$.
Therefore matrix multiplication is associative.

\textbf{Examples}

\[
    \left[\begin{array}{cc}
        0 & 0 \\
        0 & 1
    \end{array}\right]
    \left[\begin{array}{cc}
        1 & 0 \\
        0 & 0
    \end{array}\right]
    =
    \left[\begin{array}{cc}
        0 & 0 \\
        0 & 0
    \end{array}\right]
    = O_{2 \times 2}
    = O
\]

Note that in the world of matrices, the product of two non-zero matrices can
result in the zero matrix.

\[
    \left[\begin{array}{cc}
        1 & 0 \\
        1 & 0
    \end{array}\right]
    \left[\begin{array}{cc}
        1 & 0 \\
        0 & 0
    \end{array}\right]
    =
    \left[\begin{array}{cc}
        1 & 0 \\
        1 & 0
    \end{array}\right]
\]

\[
    \left[\begin{array}{cc}
        1 & 0 \\
        0 & 0
    \end{array}\right]
    \left[\begin{array}{cc}
        1 & 0 \\
        1 & 0
    \end{array}\right]
    =
    \left[\begin{array}{cc}
        1 & 0 \\
        0 & 0
    \end{array}\right]
\]

Note that the communicative property does not hold for matrix multiplication.

\subsubsection{Norm}

Given $A \in \varmathbb{R}^{e \times d} = [\vec{a}_1 \+ \vec{a}_2 \+ \dots \+
\vec{a}_d]$, we define the norm of $A$, $||A||$ as:

\[
    ||A|| := \sqrt{\sum_{j = 1}^d ||\+\vec{a}_j\+||^2} = \sqrt{\sum_{j=1}^d
    \sum_{i=1}^e a_{ij}^2}
\]

In other words, the norm of a matrix is the sqareroot of the sum of the squares
of every element in the matrix.

\textbf{Properties of the Norm}:
\begin{enumerate}
    \item $||A|| > 0, ||A|| = 0 \leftrightarrow A = O$
    \item $||cA|| = |C|||A||$
    \item $||A+B|| \leq ||A|| + ||B||$ (Triangle Inequality)
    \item $||AB|| \leq ||A||||B||$ (Generalized Cauchy-Schwarz Inequality)
\end{enumerate}

To prove these, we first establish a correspondence between any matrix $A \in
\varmathbb{R}^{e \times d}$ and a vector in $\varmathbb{R}^{ed}$, we define the
mapping $\Psi$ from $\varmathbb{R}^{e \times d}$ to $\varmathbb{R}^{ed}$:

\[
    A \overset{\Psi}{\longleftrightarrow} (a_{11}, \dots, a_{1d}, a_{21}, \dots,
    a_{2d}, \dots, a_{e1}, \dots, a_{ed})
\]

Note that under this operation, all vector space properties of the matrix are
preserved. Note that $||A|| = ||\Psi(A)||$, $\Psi(cA) = cA$, and $\Psi(A + B) =
A + B$.

However, note that the product is not exactly preserved with this transformation
to vector, since the size of the matrix plays an integral part in matrix
multiplication. To prove property 4 of the norm, we need to do a bit of work.

Let $C = AB$, then wwe know that
\begin{align*}
    ||AB||^2 = \sum_i \sum_j \left(\sum_k a_{ik}b_{kj}\right)^2 &= \sum_i \sum_j
    (\vec{\alpha}_i \cdot \vec{b}_j)^2 \\
    &\leq \sum_i \sum_j ||\+\vec{\alpha}_i\+||^2 ||\+\vec{b}_j\+||^2\\
    &= (\sum_i ||\+\vec{\alpha}_i\+||^2)(\sum_j ||\+\vec{b}_j\+||^2)\\
    &= ||A^T||^2 ||B||^2\\
    &= ||A||^2 ||B||^2
\end{align*}

Therefore, $||AB|| \leq ||A|| ||B||$.

\subsubsection{Inverses}

Suppose there exists $A \in \varmathbb{R}^{n \times n}$ and $B \in
\varmathbb{R}^{n \times n}$, we say that $B$ is a \textbf{two sided inverse} of
$A$ if $AB = BA = I (= I_n)$, where $I$ is the identity matrix, which functions
like the number one in real number multiplication.

\textbf{Thm}: If $A$ has a two-sided inverse, then it has exactly one, namely
$A^{-1}$.

Suppose that $B$ and $C$ are both two-sided inverses for $A$, i.e. $AB = BA = I$
and $AC = CA = I$. We know we can represent $B = BI = B(AC) = BA(C) = IC = C$.
Therefore $B = C$.

\textbf{Thm}: $A$ is invertible iff $\det A \neq 0$.

Let $B = A^{-1}$, $AB = I$. Now we take the determinant of both sides, we get:

$$\det (AB) = \det I$$

Since the determinant is distributive, we get:

$$(\det A)(\det B) = 1$$

Now it's clear that $\det A \neq 0$

CONVERSE = TODO

\subsubsection{Matrix Valued Functions}

It is entirely possible for functions to give out matrices as its output.
Suppose $A : D \subseteq \varmathbb{R}^d \to \varmathbb{R}^{e \times k}$.
Similar to how vector valued functions have component functions, matrix valued
functions have entry functions. $A$ would look something like this:

\[
    A(\vec{x}) = \left[\begin{array}{ccc}
            a_{11}(\vec{x}) & \dots & a_{1k}(\vec{x})\\
            \vdots & \vdots & \vdots \\
            a_{e1}(\vec{x}) & & a_{ek}(\vec{x})
    \end{array}\right]
\]

Continuity for such functions is an entrywise property: $A$ is continuous at
$\vec{p}$ iff $a_{ij}$ is continuous at $\vec{p}$ $\forall i, j$

\subsection{Components of Linear Functions}
\subsubsection{Theorem}

Given a linear mapping $\vec{l}(\vec{x}) \varmathbb{R}^d \to \varmathbb{R}^e = (l_1(\vec{x}), l_2(\vec{x}),
\dots, l_e(\vec{x}))$, then we know that each $l_i$ is linear ($\varmathbb{R}^d
\to \varmathbb{R}$).

\subsubsection{Proof}

Let us first check additivity:

\[
    \vec{l}(\vec{x} + \vec{y}) = (l_1(\vec{x} + \vec{y}), \dots, l_e(\vec{x} +
    \vec{y}))
\]
\[
    \vec{l}(\vec{x}) + \vec{l}(\vec{y}) = (l_1(\vec{x}) + l_1(\vec{y}), \dots,
    l_e(\vec{x}) + l_e(\vec{y}))
\]

Now if we inspect each element, each $l_i$ is additive. A similar argument can
be made to prove homogeneity. Therefore, each $l_i$ is a linear mapping.

\subsection{Cancellation Law of the Dot Product}
\subsubsection{Theorem}

If $\vec{a} \cdot \vec{x} = \vec{b} \cdot \vec{x}$ for all $\vec{x} \in
\varmathbb{R}^d$, where $\vec{a}, \vec{b} \in \varmathbb{R}^d$, then $\vec{a} =
\vec{b}$.

\subsubsection{Proof}

Take $\vec{x} = \vec{e}_1 = (1, 0, 0, \dots, 0)$, this yields that $a_1 = b_1$.
Then we can take $\vec{x}$ to any of the basic component vectors, we get that
$a_2 = b_2, a_3 = b_3, \dots, a_d = b_d$. Therefore, $\vec{a} = \vec{b}$.

\subsection{Expression of Linear Functions}
\subsubsection{Theorem}
For any linear map $\vec{l}: \varmathbb{R}^d \to \varmathbb{R}^e$.
There is an unique matrix $A \in \varmathbb{R}^{e \times d}$ ($d$ columns and
$e$ rows) such that $\vec{l}(\vec{x}) = A\vec{x}$ where the right hand size is a
matrix product, where $\vec{x}$ is regarded as a $d \times 1$ column matrix.

\subsubsection{Proof}
A real valued linear function $\lambda : \varmathbb{R}^d \to \varmathbb{R}$ has
the form $\lambda(\vec{x}) = \vec{a} \cdot \vec{x}$ for some $\vec{a} \in
\varmathbb{R}^d$.

Note that for any vector $\vec{x}$, we can write it as:

\[
    \vec{x} = x_1\vec{e}_1 + x_2\vec{e}_2 + \dots + x_d\vec{e}_d
\]

Now consider $\lambda(\vec{x})$:
\begin{align*}
    \lambda(\vec{x}) &= \lambda(x_1 \vec{e}_1 + \dots + x_d \vec{e}_d)\\
                     &= \lambda(\vec{e}_1) x_1 + \lambda(\vec{e}_2) x_2 + \dots
    + \lambda(\vec{e}_d)x_d
\end{align*}

Note that if we write $a_i = \lambda(\vec{e}_i)$, we get that $\lambda(\vec{x})
= \vec{a} \cdot \vec{x}$.

Now let's consider $\vec{l}$, we can write each of its component function as a
dot product between a vector and $\vec{x}$. So we get:

\[
    \vec{l} = (\vec{\alpha}_1 \cdot \vec{x}, \vec{\alpha}_2 \cdot \vec{x}, \dots
    , \vec{\alpha}_d \cdot \vec{x}) = \left[\begin{array}{c}
        \vec{\alpha}_1 \cdot \vec{x}\\
        \vec{\alpha}_2 \cdot \vec{x}\\
        \vdots\\
        \vec{\alpha}_d \cdot \vec{x}
    \end{array}\right] = \left[\begin{array}{c}
        \vec{\alpha}_1^T \\
        \vec{\alpha}_2^T \\
        \vdots \\
        \vec{\alpha}_d^T
    \end{array}\right] \vec{x} = A\vec{x}
\]


%}}}

%{{{ Topology
\section{Topology of $\varmathbb{R}^d$}
\subsection{Bounded}
\subsubsection{Definition}
Definition: $D \subseteq \varmathbb{R}^2$ is bounded if
$\exists\+M > 0: D \subseteq [-M, M]\times[-M,M]$.

For higher dimensions, we simply extend the square to a box,
hyperbox, etc.

\subsection{Closed}
\subsubsection{Definition}
\textbf{Definition}: $D \subseteq \varmathbb{R}^2$ is closed if
$\forall ((x_n, y_n))_{n = 1}^\infty \in D$ that converges, the limit point $(a,
b)$ of the sequence also lies in $D$.

\subsubsection{Theorems}
\textbf{Thm}: The intersection of two closed sets is also a closed set.

\textbf{Proof}:
Take $C, D \in \varmathbb{R}^d$ as the two closed sets. $C \cap D$ either $=
\emptyset$ or $\neq \emptyset$, If $C \cap D = \emptyset$, then the statement is
true. Otherwise, $\forall ((\vec{p}_n))_{n=1}^\infty$ within $C \cap D$, we seek
to prove that its convergence point $\vec{p} \in C \cap D$. Because
$((\vec{p}_n))_{n=1}^\infty \in C \cap D \in C$ and the closure of $C$, we know
that $\vec{p} \in C$. By a similar logic we know that $\vec{p} \in D$. Therefore
$\vec{p} \in C \cap D$. Therefore $C \cap D$ is closed.

\subsection{Open}
\subsubsection{Definition}
Define $D \subseteq \varmathbb{R}^d$ is \textbf{open} if $\forall (a, b) \in D$,
$\exists\+ r > 0: B_r(a, b)\subseteq D$

Another definition of is that $D$ is open if $D = \text{int } D$. This means
that every point of $D$ is also an interior point.

\subsubsection{Theorems}
\textbf{Thm}: If $D$ is open, then $D^c$ is closed. If $D$ is closed, then $D^c$
is open.

\begin{figure}[ht]
    \centering
    \resizebox{0.4\textwidth}{!} {
    \begin{tikzpicture}[x = 3cm, y = 3cm]
        \begin{axis} [
            xmin = 0, xmax = 6.3,
            ymin = 0, ymax = 6,
        ]
        \addplot[dashed, fill = gray, opacity = 0.3] coordinates {(1,1) (1,5) (5,5) (5,1) (1,1)};
        \addplot[mark=none] coordinates {(3, 5)} node {$\partial D$};
        \addplot[mark=none] coordinates {(3, 3)} node {$D$};
        \addplot[mark=none] coordinates {(5.8, 5.8)} node[anchor=north] {$D^c$};
        \addplot[mark=*, color = black] coordinates {(5.2, 3)} node[anchor=south]
        {$(x_1, y_1)$};
        \addplot[mark=*, color = black] coordinates {(5.2, 2.7)}
        node[anchor=west] {$(x_2, y_2)$};
        \addplot[mark=*, color = black] coordinates {(5.1, 2.5)}
        node[anchor=north] {$(a, b)$};
        \end{axis}
    \end{tikzpicture}
    }
\end{figure}

\textbf{Proof}:
Assume $D$ is open, we prove $D^c$ is closed. Choose any convergent sequence
$((x_n, y_n))_{n = 1}^\infty$, converging to $(a, b)$, where $(x_n, y_n) \in
D^c$ for all $n \geq 1$.


We prove this by contradiction. Assume that $(a, b) \in D$. Since $D$ is open,
$\exists\+ r > 0:B_r(a, b)\subseteq D$. $\exists\+ N, \forall n \geq
\varmathbb{N}, (x_n, y_n) \in B_r(a, b)$ since $(x_n, y_n)\to(a,b)$. But we
assumed that $(x_n, y_n) \in D^c$, and $(x_n, y_n) \in D$. But $D \cap D^c =
\emptyset$. Therefore $D^c$ is closed under taking limits.

\textbf{Thm}: An open-ball $B_r(\vec{p}) = \{\vec{x} \in \varmathbb{R} \+|\+
dist(\vec{x}, \vec{p}) < r\}$, $r > 0$, is an open set.

%\begin{figure}[ht]
%    \begin{tikzpicture}
%        \draw circle(10, 10)
%        \draw circle(5,5)
%    \end{tikzpicture}
%\end{figure}
\textbf{Proof}:
$\forall \vec{q} \in B_r(\vec{p})$, $B_\varepsilon(\vec{q}) \subseteq
B_r(\vec{p})$, $\varepsilon > 0$.

$d = dist(\vec{p}, \vec{q}) < r$. Therefore, $r - d > 0$. Take $\varepsilon =
\frac{1}{2} (r - d) > 0$. Let $\vec{x} = B_\varepsilon(\vec{q})$, show $\vec{x}
\in B_r(\vec{p})$.

$dist(\vec{x}, \vec{q}) < \varepsilon$

$dist(\vec{x}, \vec{p}) \leq dist(\vec{x}, \vec{q}) + dist(\vec{q}, \vec{p}) <
\varepsilon + d = d + \frac{1}{2} r - \frac{1}{2} d = \frac{1}{2}r +
\frac{1}{2}d < \frac{1}{2} \cdot 2 \cdot r = r$

\textbf{Thm}: The union of any number of open sets is an open set.

\textbf{Proof}:
Let $U = u_1 \cup u_2 \cup u_3 \cup \dots \cup u_n$ where $u_i$ is an open set.
We know that $\forall \vec{p} \in U$, $\vec{p} \in$ some $u_i$, which means
$\exists\+ r > 0 : B_r(\vec{p}) \in u_i \in U$. Therefore $U$ is an open set.

\subsection{Compact}
\subsubsection{Definition}
We say that a set $D \subseteq \varmathbb{R}^2$ is \textbf{compact} if it is both bounded and closed

\subsection{Connected}
\subsubsection{Definition}
$D \subseteq \varmathbb{R}^d$ is said to be connected if $\forall \vec{p},
\vec{q} \in D$, $\exists$ a continuous function $\vec{\gamma}:[a, b] \to
\varmathbb{R}^d$ that $\vec{\gamma}(a) = \vec{p}$, $\vec{\gamma}(b) = \vec{q}$,
and $\forall t \in [a, b], \vec{\gamma}(t) \in D$
\begin{figure}[ht]
    \centering
    \resizebox{0.4\textwidth}{!} {
    \begin{tikzpicture}[x = 3cm, y = 3cm]
        \begin{axis} [
            xmin = 0, xmax = 6.3,
            ymin = 0, ymax = 6,
        ]
        \addplot[dashed, fill = gray, opacity = 0.3] coordinates {(1,1) (1,5) (5,5) (5,1) (1,1)};
        \addplot[mark=none] coordinates {(1,5) (3,5)};
        \addplot[mark=none] coordinates {(1,1) (1,4)};
        \addplot[mark=none] coordinates {(5,5) (5,3)};
        \addplot[mark=none] coordinates {(2,1) (4,1)};
        \addplot[mark=none] coordinates {(3, 3)} node {$D$};
        \addplot[mark=*] coordinates{(1.2, 4.3)} node[anchor=north] {$\vec{p}$};
        \addplot[mark=*] coordinates{(4.2, 1.4)} node[anchor=west] {$\vec{q}$};
        \addplot[mark=none] coordinates {(1.2, 4.3) (3.5, 1.3) (2.2, 4.5) (4, 3)
        (4.2,1.4)};
        \addplot[mark=none] coordinates {(2.2, 4.5)} node[anchor=north east]
        {$\vec{\gamma}$};
        \end{axis}
    \end{tikzpicture}
    }
\end{figure}

\subsubsection{Theorems}

Let $D \subseteq \varmathbb{R}^d$ be connected, $D \neq \emptyset$. Suppose $D =
A \cup B$, where $A$ and $B$ are open. Such that $A \cap B = \emptyset$. Then $A
= \emptyset$ or $B = \emptyset$.

\textbf{Proof}:

Assume that $D$ can be broken up into two open, non-empty sets $A$ and $B$ such
that $D = A \cup B$. Because $A \neq \emptyset$, we can pick $\vec{p} \in A$ and
similarly we can pick $\vec{q} \in B$. Clearly, $\vec{p}, \vec{q} \in D$. Since
$D$ is connected, $\exists$ continuous $\vec{\gamma}: [a, b] \to D$ such that
$\vec{\gamma}(a) = \vec{p}$, $\vec{\gamma}(b) = \vec{q}$. Let $S = \{t \in [a,b]
\+|\+ \vec{\gamma}(t) \in A\}$. We know that $a \in S$, so $S \neq \emptyset$.
Note that $S \leq b$, and since it has a bound, it has a supremum. Let $t_0 =
\sup S$, note that $a \leq t_0 \leq b$. Since $\vec{\gamma}(t_0) \in D$, it is
either in $A$ or in $B$ (since $A \cap B = \emptyset$). Suppose
$\vec{\gamma}(t_0) \in A$. Since $A$ is open, we can draw an open ball
($B_\varepsilon$) around $\vec{\gamma}(t_0)$. Now consider $\vec{\gamma}(t_0 +
\delta)$ for some small positive $\delta$. If $\delta$ is small enough, $\delta
\in B_\varepsilon \in A$. This is a contradiction, as then it means that $t_0 +
\delta \in S$. This is a contradiction, as then $t_0 \neq \sup S$. A similar
contradiction can be made for the case that $\vec{\gamma}(t_0) \in B$. Therefore
the original assumption was false.

%}}}
%{{{ Sequences in R^n
\section{Sequences in $\varmathbb{R}^d$}

\subsection{Bounded Sequence Theorem}
\subsubsection{Theorem}
Any bounded sequence in $\varmathbb{R}^d$ (bounded within the box of $[-M, M]
\times [-M, M] \times [-M, M] \times \dots$) has a convergent subsequence.

\subsubsection{Lemma}
If $(x_n)_{n = 1}^\infty$ converges to $x \in \varmathbb{R}$, then every
subsequence $(x_{n_k})_{k = 1}^\infty$ also converges to $x$

\textbf{Proof}: $\forall \varepsilon > 0, \exists\+ N, \forall n \geq N: |\+x_n
- x\+| < \varepsilon$.

$\therefore \forall k > K: n_k \geq N \longrightarrow |\+x_{n_k} - x\+| <
\varepsilon$

\subsubsection{Proof}
$P_n = (x_n, y_n)$, consider $(x_n)_{n = 1}^\infty$ in $\varmathbb{R}$. Since
$-M \leq x_n \leq M$, $(x_n)_{n=1}^\infty$ is bounded. Pick some
convergent subsequence $(x_{n_k})_{k=1}^\infty$. Consider $(y_{n_k})_{k =
1}^\infty$. Since $-M \leq y_{n_k} \leq M$, $(y_{n_k})_{k = 1}^\infty$ is
bounded. Therefore it has a convergent subsequence,
$(y_{n_{k_j}})_{j=1}^\infty$, which converges to $y \in \varmathbb{R}$. Note
that $(x_{n_{k_j}})_{j=1}^\infty \to x$ because it is a subsequence of
$(x_{n_k})_{k = 1}^\infty$, which converges to $x$. Thus $P_{n_{k_j}} =
(x_{n_{k_j}}, y_{n_{k_j}}) \to (x, y) = P$, which is the result of a corollary
of the Basic Distance Bounds Lemma.

\subsection{Cauchy's Convergence Theorem}

\subsubsection{Cauchy Sequence}
A sequence $(x_n)_{n=1}^\infty$ in $\varmathbb{R}$ is a cauchy sequence if 
$$\forall \varepsilon, \exists\+ n, m \geq N: |\+x_n - x_m\+| < \varepsilon$$

\subsubsection{Convergence $\to$ Cauchy}
Note that any convergent sequence is cauchy, because as terms get together
to a limit, they also go very closely together.

\textbf{``Formal'' Proof}:

Let $(x_n)_{n = 1}^\infty$ be convergent, with limit $x \in \varmathbb{R}$.
Then, by definition, $\forall \varepsilon > 0, \exists\+ N_\varepsilon, \forall
n \geq N_\varepsilon :
|x_n - x| < \varepsilon$. Note that we can replace $\varepsilon$ with
$\frac{\varepsilon}{2}$, all we have to change is the cutoff point from
$N_{\varepsilon}$ to $N_{\frac{\varepsilon}{2}}$. Now if we take two subscripts
$n, m \geq N_{\frac{\varepsilon}{2}} \longrightarrow |\+x_n - x_m\+| = |\+(x_n -
x) + (x - x_m)\+| \leq |\+x_n - x\+| + |\+x_m - x\+|$ because of the Triangle
Inequality for Absolute Values. However, note that $|\+x_n - x\+| \leq
\frac{\varepsilon}{2}$ and $|\+x_m - x\+| \leq \frac{\varepsilon}{2}$.
Therefore, $|\+x_n - x_m\+| \leq |\+x_n - x\+| + |\+x_m - x\+| \leq \varepsilon
\+\blacksquare$

\subsubsection{Cauchy's Convergence Theorem}

In $\varmathbb{R}$, every cauchy sequence converges to a limit in $\varmathbb{R}$.

\vspace{0.3cm}

\textbf{Lemma \#1}: Every cauchy sequence is bounded.

Let us take $\varepsilon = 1$, then the definition of ``cauchiness'' becomes:
$$\exists\+ N_1, \forall n, m \geq N_1 : |\+x_n - x_m\+| < 1$$

Let $M := \max\{|x_1|, |x_2|, \dots, |x_{N_1 - 1}|, |x_{N_1}| + 1\}$. We claim
that $|x_n| \leq M$, for all $n \geq 1$. This is true because when $n \in \{1, 2, \dots, N_1
- 1\}$, the statement is true by definition of $M$. When $n \geq N_1$, we know
that $|x_n| \leq |x_{N_1}| + 1 \leq M$ because we can let $m = N_1$, then by the
definition of ``cauchiness,'' we know that $|x_n - x_{N_1}| < 1$.

Now we see that $M$ is a bound on the sequence for all $n \geq 1$. Therefore the
sequence is bounded.

\textbf{Lemma \#2}: If a subsequence of a cauchy sequence converges to $x \in \varmathbb{R}$, the
whole sequence must converge to $x$.

Say $(x_n)_{n=1}^\infty$ is cauchy, and $(x_{n_k})_{k = 1}^\infty$ converges to
$x$. For any arbitrary $\varepsilon > 0$, we try to find $N$ such that $\forall
n \geq N: |x_n - x| < \varepsilon$. If we prove the existence of $N$ for all
$\varepsilon$, we will have proven that the original sequence converges.

We know that $\forall \varepsilon > 0, \exists \+ K_\varepsilon, \forall k \geq
K_\varepsilon: |x_{n_k} - x| < \varepsilon$. We add and subtract $x_{n_k}$ and
group terms, and use the Triangle Inequality: $|x_n - x| = |(x_n - x_{n_k}) + (x_{n_k} - x)| \leq
|x_n - x_{n_k}| + |x_{n_k} - x|$. Note that $|x_{n_k} - x| < \varepsilon$
provided $k \geq K_{\varepsilon}$ from the convergent subsequence condition. We
also know that $|x_n - x_{n_k}| < \varepsilon$ provided that $n, n_k \geq
N_{\varepsilon}$, which we call the ``cauchy cutoff.'' This is true from the
``cauchiness'' condition.

We know that $k\to\infty$ implies $n_k \to \infty$. This means eventually
$n_k > N_\varepsilon$ provided that $k > L_{N_\varepsilon}$. Now let $k =
\max\{L_{N_\varepsilon}, K_\varepsilon\}$ and $n \geq N_\varepsilon$, which
implies $|x_n - x_{n_k}| < \varepsilon$ and $|x_{n_k} - x| < \varepsilon$.
Now we know: $|x_n - x| \leq 2\varepsilon$ provided $n \geq N_{\varepsilon}$.
Therefore the cauchy sequence converges to $x$.

\vspace{0.3cm}

With these two lemmas, the theorem becomes very easy to prove:

Because of Lemma \#1 and the Bolzano-Weierstrass Theorem, we know that for all
cauchy sequences, there is a bounded subsequence that converges to some value
$x$. Then by Lemma \#2, we know that the entire cauchy sequence converges to $x$
as well, therefore the sequence converges. 

\subsubsection{Higher Dimensions}

$(P_n)_{n=1}^\infty$ is cauchy if $\forall \varepsilon > 0, \exists \+
N_\varepsilon, \forall n, m \geq N_\varepsilon: dist(P_n, P_m) < \varepsilon$.
This is easy to prove due to the coordinate nature of $\varmathbb{R}^d$.

\subsection{Heine-Borel Theorem}
\subsubsection{Theorem}
Let $K$ be a compact set in $\varmathbb{R}^d$. Let $\{u_\lambda \+|\+ \lambda
\in \Lambda\}$ be a family of open sets in $\varmathbb{R}^d$ which covers $K$ in
the sense that $K \subseteq \cup_{\lambda \in \Lambda} u_\lambda = \{\vec{p}
\+|\+ \exists\+ \lambda \in \Lambda: \vec{p} \in u_\lambda\}$. Then $\exists
\+ \lambda_1, \lambda_2, \dots, \lambda_n \in \Lambda$ such that $K \subseteq
u_{\lambda_1} \cup u_{\lambda_2} \cup \dots \cup u_{\lambda_n}$ (for some finite $n$)

\begin{figure}[ht]
\centering
\begin{tikzpicture}
    \begin{axis} [
        scale only axis,
        grid=major,
        axis lines=middle,
        inner axis line style={->},
        ymin=-2.5,
        xmin=-2.5,
        ymax=2.5,
        xmax=2.5,
    ]
    \addplot[color=red, fill=gray, fill opacity=0.5, smooth, thick] coordinates
    {(0,-2) (-1,1) (1,2) (1,0) (2,-1) (0,-2)} node[anchor = south] {$K$};
    \draw[dashed](axis cs:1,1) circle[radius=100] node {$u_{\lambda_1}$};
    \draw[dashed](axis cs:0.5,1.7) circle[radius=100] node {$u_{\lambda_2}$};
    \draw[dashed](axis cs:-1,0.8) circle[radius=100] node {$u_{\lambda_3}$};
    \draw[dashed](axis cs:-1,0) circle[radius=100] node {$u_{\lambda_4}$};
    \draw[dashed](axis cs:-1,-0.7) circle[radius=100] node {$u_{\lambda_5}$};
    \draw[dashed](axis cs:1.2,-0.5) circle[radius=100] node {$u_{\lambda_6}$};
    \draw[dashed](axis cs:1,-1.4) circle[radius=100] node {$u_{\lambda_7}$};
    \draw[dashed](axis cs:0.3,-1.6) circle[radius=100] node {$u_{\lambda_8}$};
    \draw[dashed](axis cs:0,0) circle[radius=100] node {$u_{\lambda_8}$};
    \end{axis}
\end{tikzpicture}
\caption{Here the closed set $K$ is completed covered by the open sets
$u_{\lambda_n}$}
\end{figure}

\subsubsection{Proof}

Because $K$ is compact, we can surround $K$ in a box ($R$) and divide that box into 4
congruent sections, $R_1, R_2, R_3$ and $R_4$. Assume that $K$ cannot be covered
by a finite amount of $u_\lambda$'s. That implies that at least one of the 4
quadrants ($R_i$) cannot be covered by a finite collection of $u_\lambda$. Then we
divide that into 4 quadrants, and then one of the further qudrants $R_{i_1i_2}$
cannot be covered by a finite collection of $u_\lambda$'s, and this go on
forever. Say that $K\cap R_{i_1i_2i_3\dots i_n}$ cannot be covered by finitely
many $u_\lambda$'s. Note that this cannot be $\emptyset$ for any $n$.

Let $(\vec{p_n})_{n=1}^\infty$ be any sequence of points in $K\cap
R_{i_1i_2i_3\dots i_n}$. This is a bounded sequence because $K$  is bounded.
Then we know that there exists $(\vec{p_{n_k}})_{k=1}^\infty$ that converges to
some point $\vec{p}$. Note that $\vec{p} \in K$ because $K$ is closed. Thus
$\vec{p} \in u_{\lambda^*}$ for some $\lambda^* \in \Lambda$. Since
$u_{\lambda^*}$ is open, $\exists\+r > 0$ s.t. $B_r(\vec{p}) \in u_{\lambda^*}$.
But when $n$ is large:
$$diam\+ R_{i_1i_2\dots i_n} = \frac{diam\+ R}{2^n} < \frac{r}{2}$$

Therefore, when $n_k$ is big enough ($n_k \to \infty$ as $n \to \infty$)
$$R_{i_1i_2\dots i_n} \subseteq B_r(\vec{p})$$

Therefore, we know that $dist(\vec{p}, \vec{p_{n_k}}) < \frac{r}{2}$. Therefore,
no point within $K\cap R_{i_1i_2i_3\dots i_n}$ is greater than $r$ away from
$\vec{p}$.

Now $K\cap R_{i_1i_2i_3\dots i_n} \subseteq u_{\lambda^*}$ because $K\cap
R_{i_1i_2i_3\dots i_n}$ is within a ball. Therefore we can reject this entire
process and we have just proved that $K$ can be covered by a finite collection
of $u_{\lambda_n}$ for any given $\Lambda$.
%}}}
%{{{ R^d -> R
\section{Real Valued Functions}
\subsection{Extreme Value Theorem in $\varmathbb{R}^d$}
\subsubsection{Theorem}
Let $f : D \to \varmathbb{R}$ be a continuous function mapping from the compact
set $D$ to the reals. Then $\exists\+ P, Q \in D$, not necessarily distinct,
such that $\exists\+ x \in D: f(P) \leq f(x) \leq f(Q)$.

\subsubsection{Proof}
First we prove that $f$ must be bounded from above. Assume that it is not, take
a sequence $(\vec{P_n})_{n=1}^\infty \in D$, the assumption implies that
implies for every positive integer $n$, $f(\vec{P_n}) > n$. Because it is bounded by
$D$, we can pick a convergent subsequence $\vec{P_{n_k}}$, which converges to
$\vec{P}$. However, since $D$ is closed, we know that $\vec{P} \in D$. However, now we have a contradiction. Because
$f$ is continuous, $\lim_{n\to\infty}f(\vec{P_n}) \to \vec{P}$. This is a contradiction,
because the right hand side $\vec{P}$ is finite (it's within $D$), but the left hand
side goes to infinity by the assumption. Therefore the assumption is false,
thus the function $f$ is bounded from above.

Now we know that $f$ is bounded from above, we know $M := \sup{(f(D))}$ exists
where $0\leq M < \infty$. We can chose a sequence $(P_n)_{n = 1}^\infty \in D$
such that $f(P_n) \to M$. The sequence is bounded, so it has a convergent
subsequence, $\vec{P_{n_k}} \to \vec{P}$. By the closure of $D$, $\vec{P} \in
D$. Note that $f(\vec{P}) = \lim_{k\to\infty} f(\vec{P_{n_k}}) = M$ by the
continuity of $f$. Therefore, the function actually takes on its maximum value
at that point.

\subsection{Uniform Continuity Theorem}
\subsubsection{Theorem}
If $f: K \to \varmathbb{R}$, where $K \subseteq \varmathbb{R}^d$ is compact, and
$f$ is continuous at each $\vec{x} \in K$. Then $f$ is uniformally continuous on
$K$.

\subsubsection{Proof}
Fix $\varepsilon > 0$. For each $\vec{x} \in K$, let $u_{\vec{x}}$ be an open ball, centered at
$\vec{x}$ such that for any $\vec{y} \in K \cap 2u_{\vec{x}}$ [$2u_{\vec{x}}$ is
an open ball centered at $\vec{x}$ with twice the radius of $u_{\vec{x}}$], $|f(\vec{x}) -
f(\vec{y})| < \frac{\varepsilon}{2}$. Because the function is continuous at
$\vec{x}$, there is a radius $2\delta$ around $\vec{x}$ such that $|f(\vec{x}) -
f(\vec{y})| < \frac{\varepsilon}{2}$. Now we see that every $\vec{x} \in K$ is
covered by at least one such open ball, namely $u_{\vec{x}}$. The collection
$\{u_{\vec{x}} \+|\+ \vec{x} \in K\}$ is an open covering of $K$. By
Heine-Borel, we can select a finite set of points $\vec{x}_1, \vec{x}_2, \dots,
\vec{x}_n$ such that $K$ is covered by $u_{\vec{x}_1} \cup u_{\vec{x}_2} \cup \dots
\cup u_{\vec{x}_n}$. Take $\delta = \min\{\delta_1, \delta_2, \dots, \delta_n\}
> 0$  where $\delta_j$ is the radius of $u_{\vec{x}_j}$ for $j = 1,2,3,\dots,n$.

Let $\vec{p}, \vec{q} \in K$ such that $||\+\vec{p} - \vec{q}\+|| < \delta$. We
need to prove that $|f(\vec{p}) - f(\vec{q})| < \varepsilon$. $\vec{p} \in K \to
\exists \+ j : \vec{p} \in u_{\vec{x}_j} \to ||\+\vec{p} - \vec{x}_j\+|| <
\delta_j$. But we know that $||\+\vec{p} - \vec{q}\+|| < \delta_j$. Now by the
Triangle Inequality, we get:

$$||\+\vec{q} - \vec{x}_j\+|| \leq ||\+\vec{q} - \vec{p}\+|| + ||\+\vec{p} - \vec{x}_j\+||
\leq 2 \delta_{\vec{x}_j}$$

This means that $\vec{q} \in 2u_{\vec{x}_j}$. By the way we picked our $\delta$,
we know that $|f(\vec{q}) - f(\vec{x}_j)| < \frac{\varepsilon}{2}$. Similarly,
we also have $|f(\vec{p}) - f(\vec{x}_j)| < \frac{\varepsilon}{2}$. Now if we
apply the triangle inequality again, we get:

$$|f(\vec{p}) - f(\vec{q})| = |f(\vec{p}) - f(\vec{x}_j) + f(\vec{x}_j)- f(\vec{q})|\leq |f(\vec{p}) - f(\vec{x}_j)| + |f(\vec{q}) -
f(\vec{x}_j)| < \varepsilon$$

Therefore, $f$ is uniformally continuous over $K$.

\subsection{Intermediate Value Theorem}
\subsubsection{Theorem}

If $f : D \to \varmathbb{R}$ where $D \subseteq \varmathbb{R}^d$ is connected
and continuous. $\forall \vec{p}, \vec{q} \in D$ such that $f(\vec{p}) \neq
f(\vec{q})$ and if $y \in \varmathbb{R}$ is between $f(\vec{p})$ and $\vec{q}$,
then $\exists \vec{r} \in D$ such that $f(\vec{r}) = y$.

\subsubsection{Proof}

WLOG, assume $f(\vec{p}) < f(\vec{q})$

Because $D$ is connected, there is some path $\vec{\gamma}(t) = (x(t), y(t)) \in
D$, $a \leq t \leq b$ such that $\vec{\gamma}(a) = \vec{p}$ and $\vec{\gamma}(b)
= \vec{q}$ and is continuous over $[a, b]$. Now we construct $g(t) =
f(\vec{r}(t)) = (f\cdot\vec{r})(t) \in varmathbb{R}$. Because $g(t)$ is a
composition of continuous functions, $g(t)$ is also continuous. $g(a) =
f(\vec{\gamma}(a)) = f(\vec{p})$ and $g(b) = f(\vec{\gamma}(b)) = f(\vec{q})$.
Now we see that $g(a) < y < g(b)$. Therefore, by the IVT for single-variable
functions, $\exists\+t_0$ such that $g(t_0) = y$. Now we plug $t_0$ into
$\vec{\gamma}$, $\vec{r} = \vec{\gamma}(t_0) \in D$. Then $f(\vec{r}) =
f(\vec{\gamma}(t_0)) = g(t_0) = y$.

%}}}
%{{{ R^d -> R^e
\section{Vector Valued Functions}

\subsection{Definition}
$\vec{f}: D \to \varmathbb{R}^e$, $D \subseteq \varmathbb{R}^d$ is known as
a \textbf{vector-valued/point-valued} function.

$\vec{f}(\vec{p}) = (f_1(\vec{p}), f_2(\vec{p}), \dots, f_e(\vec{p}))$ where
$f_1, f_2, \dots, f_e$ are real-valued and are called the \textbf{component
functions} of $f$.

\subsection{Continuity}
$f: \varmathbb{R}^d \to \varmathbb{R}^e$ is continuous at $\vec{a} = (a_1, a_2,
a_3, \dots, a_d) \in D$ iff $\forall \varepsilon, \exists\+\delta > 0 : \forall \vec{x} \in D$:

$$||\+\vec{x} - \vec{a}\+||_d < \delta \to ||\+f(\vec{x}) - f(\vec{a})\+||_e <
\varepsilon$$

\subsubsection{Component-wise Nature of Continuity}
$f: D \to \varmathbb{R}^e$ is continuous at a point $\vec{a} \in D$ iff $f_1,
f_2, \dots, f_e$ are all continuous at $\vec{a}$.

\textbf{Proof}: First fix an $\varepsilon$, then by the basic distances bound lemma, we get a
bunch of inequalities:

\begin{align*}
    |f_1(\vec{x}) - f_1(\vec{p})| < \frac{\varepsilon}{\sqrt{e}}, &\forall \vec{x} \in D \cap B_{\delta_1}(\vec{p})\\
    |f_2(\vec{x}) - f_2(\vec{p})| < \frac{\varepsilon}{\sqrt{e}}, &\forall \vec{x} \in D \cap B_{\delta_2}(\vec{p})\\
    ... \text{ } ... \text{ } & \text{ } ... \text{ }...\\
    |f_e(\vec{x}) - f_e(\vec{p})| < \frac{\varepsilon}{\sqrt{e}}, &\forall
    \vec{x} \in D \cap B_{\delta_e}(\vec{p})
\end{align*}

Then let $\delta = \min\{\delta_1, \delta_2, \dots, \delta_e\} > 0$, which must
exist and satisfy all the distance inequalities, specifically $\max_{1 \leq j
\leq e}(|f_j(\vec{x}) - f_j(\vec{p})|)$. Then again, by the basic distance
bounds lemma, we know that $||\+\vec{f}(\vec{x}) - \vec{f}(\vec{p})\+||_e \leq \max_{1 \leq j
\leq e}(|f_j(\vec{x}) - f_j(\vec{p})|)$. Therefore we get that for any fixed
$\varepsilon$, we can find a $\delta$ such that $||\+\vec{x} - \vec{p}\+||_d <
\delta \to ||\+f(\vec{x}) - f(\vec{p})\+||_e < \varepsilon$

\subsubsection{Composition of Continuous Functions}
If $\vec{f}: D \to E$, where $D \subseteq \varmathbb{R}^d$, $E
\subseteq \varmathbb{R}^e$ and $\vec{g}: E \to \varmathbb{R}^k$ are both
continuous on their respective domains. Then $\vec{h} = \vec{g} \circ \vec{f}$
is continuous on $D$

\textbf{Proof}:
To prove this, fix $\varepsilon > 0$. There's a $\eta > 0$ such that
$\forall \vec{y} \in E \cap B_\eta(\vec{f}(\vec{p}))$, because we know that
$\vec{g}$ is continuous at $\vec{f}(\vec{p})$, we get:

$$||\+\vec{g}(\vec{y}) - \vec{g}(\vec{f}(\vec{p}))\+|| < \varepsilon$$

To guarantee that $\vec{y} = \vec{f}(\vec{x})$ lies within $\eta$ units of
$\vec{f}(\vec{p})$ i.e. $||\+\vec{f}(\vec{x}) - \vec{f}(\vec{p})+|| < \eta$, we
can take $\vec{x} \in D \cap B_\delta(\vec{p})$ where $\delta > 0$ corresponding
to $\eta$ [using the continuity of $\vec{f}$ at $vec{p} \in D$].

Now, as long as $\vec{x} \in D \cap B_\delta(\vec{p})$, we have
$\vec{f}(\vec{x}) \in E \cap B_\eta(\vec{f}(\vec{p}))$. Thus, if we take
$\vec{y} = \vec{f}(\vec{p})$, we get: $||\+\vec{g}(\vec{f}(\vec{x})) -
\vec{g}(\vec{f}(\vec{p}))\+|| < \varepsilon$, or $||\+\vec{h}(\vec{x}) -
\vec{h}(\vec{p})\+|| < \varepsilon$. So $\vec{h}$ is continuous at $\vec{p}$.

\subsection{Compactness Theorem}

\subsubsection{Theorem}

Let $\vec{f} : D \to \varmathbb{R}^e$ be a contnuous function, where $D \subseteq
\varmathbb{R}^d$ is compact. Then its \textit{range} $\vec{f}(D) := \{f(\vec{p}) \+|\+
\vec{p} \in D\}$ is also compact. In other words: compactness is preserved under
continuous mappings. Note that this is the generalization of the Extreme Value
Theorem.

\subsubsection{Proof}

To prove this, write $R := f(D)$. We need to show that $R$ is closed and bounded
in $\varmathbb{R}^e$. Boundedness is easy. Since each component function $f_j$
of $f$ is real valued, by EVT each component function $f_j$ has an absolute
bound $M_j$, so that $|\+f_j(\vec{p})\+| \leq M_j$ for all $\vec{p} \in D$. 
Take $M := \max\{M_1, M_2, \dots, M_e\}$. Then for all $\vec{p} \in D$, we have

$$||\+\vec{f}(\vec{p})\+|| = \sqrt{f_1(\vec{p})^2 + \dots + f_e(\vec{p})^2} \leq
\sqrt{e \times M^2} = M \sqrt{e}$$

This says that the range $R$ lies within the closed ball of radius $M\sqrt{e}$
centered at $\vec{0}$ in $\varmathbb{R}^e$. It therefore certainly lies within
some closed cube centered at $\vec{0}$, and hence is bounded.

To prove closedness, let $(\vec{y}_n)_{n = 1}^\infty$ be a convergent sequence in
$\varmathbb{R}^e$ with limit $\vec{y}$, such that $\vec{y}_n \in R$ for each $n
\geq 1$. We need to prove that $\vec{y} \in R$. Since $R$ is the range of $f$,
we must have $\vec{y} = \vec{f}(\vec{x_n})$, where $\vec{x_n} \in D$. Because
$D$ is bounded, we can pick a convergent subsequence $\vec{x}_{n_k} \to
\vec{x}$. But because $D$ is closed, we know that $\vec{x} \in D$. Because
$\vec{f}$ is continuous, we get that $\vec{y}_{n_k} = \vec{f}(\vec{x}_{n_k}) \to
\vec{f}(\vec{x})$. However, we know that $\vec{y}_{n_k} \to \vec{y}$. But since
each sequence converges to one point, we know that $\vec{y} = \vec{f}(\vec{x})$,
where $\vec{x} \in D$. Therefore, $\vec{y} \in R$.

Therefore, $R$ is closed and bounded.

\subsection{Connectedness Theorem}

\subsubsection{Theorem}

$\vec{f}: D \to \varmathbb{R}^e$, $D \in \varmathbb{R}^d$ is continuous on $D$.
If $D$ is connected, $E = \vec{f}(D)$ (the range of the domain), is also
connected. Note that this is the generalization of the Intermediate Value
Theorem.

\subsubsection{Proof}

$\forall \vec{u}, \vec{v} \in E$, we can find two points $\vec{p}, \vec{q}$ such
that $\vec{f}(\vec{p}) = \vec{u}$ and $\vec{f}(\vec{q}) = \vec{v}$. Now because
$D$ is connected, $\exists\+ \vec{\gamma}: [a, b] \to \varmathbb{R}^d$,
$\vec{\gamma}([a, b]) \subseteq D$. Now we consider $\vec{\delta} =
\vec{f}(\vec{\gamma}(t))$, $\vec{\delta} : [a, b] \to \varmathbb{R}^e$. Since
it's a composition of continuous functions, $\vec{\delta}$ is also continuous.
And $\forall t \in [a, b]$, $\vec{\delta}(t) = \vec{f}(\vec{\gamma}(t)) \in E$.
We also know that $\vec{\delta}(a) = \vec{f}(\vec{\gamma}(a)) = \vec{f}(\vec{p})
= \vec{u}$, and $\vec{\delta}(b) = \vec{f}(\vec{\gamma}(b)) = \vec{f}(\vec{q})
= \vec{v}$. Therefore, $\forall \vec{u}, \vec{v} \in E$, there is a
continuous path that connects $\vec{u}$ to $\vec{v}$ and stays within $E$.
Therefore, $E$ is connected.
%}}}
%{{{ Sequence of Functions
\section{Sequences of Functions}
\subsection{Infinity Norm}
For $f: D \to \varmathbb{R}$ that is bounded, we say:
$||\+f\+||_\infty := \sup_{x\in D} |f(x)|$

We can also express this as:
$||\+f\+||_D := \sup_{x\in D} |f(x)|$

\subsection{Pointwise Convergence}
$f_n : [a,b] \to \varmathbb{R}$ for $n \geq 1$, and assume that they are all
bounded on $[a,b]$. ($\exists\+ M_n > 0 : |f_n(x)| \leq M_n$ for all $x \in
[a,b]$).

$f:[a,b] \to \varmathbb{R}$, $f$ is bounded ($\exists\+ M > 0: |f(x)| \leq M$
for all $x \in [a,b]$).

If $\forall x \in [a,b]: \lim_{n\to\infty} f_n(x) = f(x)$, then we say that $f_n
\overset{p}{\to} f(x)$ ($f_n$ converges ``pointwise'' to $f$).

\begin{figure}
\centering
\begin{tikzpicture}
    \begin{axis} [
        scale only axis,
        grid=major,
        axis lines=middle,
        inner axis line style={->},
        ymin=-0.2,
        xmin=-0.2,
        ymax=1.2,
        xmax=1.2,
    ]
    \addplot[mark=none, thick, domain=0:1, samples=20, smooth]{x};
    \addplot[mark=none, thick, domain=0:1, samples=20, smooth]{x * x};
    \addplot[mark=none, thick, domain=0:1, samples=20, smooth]{x * x * x};
    \addplot[mark=none, thick, domain=0:1, samples=20, smooth]{x * x * x * x};
    \addplot[mark=none, thick, domain=0:1, samples=20, smooth]{x * x * x * x * x};
    \addplot[mark=none, thick, domain=0:1, samples=20, smooth]{x * x * x * x * x * x};
    \end{axis}
\end{tikzpicture}
\end{figure}

\subsubsection{Example}

$f_n(x) = x^n$, $n \geq 1$ on $[0, 1]$

If the domain is $[0, 1)$, then $f_n \overset{p}{\to} f$ where $f(x) = 0$.

On $[0, 1]$, $f_n \overset{p}{\to} \~{f}$, where $\~{f}$ is $0$ for $0 \leq x < 0$, and $1$
when $x = 1$


\subsection{Uniform Convergence}

\subsubsection{In Single Variable Calculus}
$f_n : [a,b] \to \varmathbb{R}$ for $n \geq 1$, and assume that they are all
bounded on $[a,b]$. ($\exists\+ M_n > 0 : |f_n(x)| \leq M_n$ for all $x \in
[a,b]$).

$f:[a,b] \to \varmathbb{R}$, $f$ is bounded ($\exists\+ M > 0: |f(x)| \leq M$
for all $x \in [a,b]$).

We then claim that $f_n \overset{u}{\to} f$ as $n \to \infty$ if $\forall
\varepsilon$, $\exists\+ N_\varepsilon$ such that $\forall n \geq
N_\varepsilon : ||\+f_n - f\+||_\infty < \varepsilon$. In other words, this
forces that the greatest vertical difference between the two functions will be
arbitrarily small after $N_\varepsilon$. This forces the two functions to be
``close'' as a whole.

\subsubsection{General Case}

Let $D \in \varmathbb{R}^d$ be a non-empty set, and let $\vec{f}:
D\to\varmathbb{R}^e$ and $\vec{f}_n: D \to \varmathbb{R}^e$ for $n \geq 1$. We
say that $\vec{f_n} \uconv \vec{f}$ on $D$ if:

$$||\+f_n - f\+||_D \to 0 \text{ as } n \to \infty$$

\subsubsection{$f_n \overset{u}{\to} f$ over $D$ implies $f_n \overset{p}{\to}
f$ over $D$}

Pick any $x \in D$, $0 \leq ||\+\vec{f}_n(\vec{x}) - \vec{f}(\vec{x})\+|| \leq
||\+\vec{f}_n - \vec{f}\+||_D \to 0$ as $n \to \infty$. Then by the squeeze theorem,
$||\vec{f}_n(\vec{x}) - \vec{f}(\vec{x})| = 0$, therefore $\vec{f}_n(\vec{x})
\to \vec{f}(\vec{x})$ as $n \to \infty$.

Note the the converse is NOT true.

\subsection{Uniform Convergence Theorem}
\subsubsection{Theorem}

If $\vec{f}_n \uconv \vec{f}$ on $D$ and each $\vec{f}_n$ is continuous on $D$,
then $\vec{f}$ is also continuous on $D$. In other words, a uniform limit of
continuous functions must also be continuous.

\subsubsection{Proof}

Pick any $\vec{p} \in D$, and fix $\varepsilon > 0$. We need to find $\delta$
such that $\forall \vec{x} \in D$ with $||\+\vec{x} - \vec{p}\+|| < \delta$,
then $||\+\vec{f}(\vec{x}) - \vec{f}(\vec{p})\+|| < \varepsilon$. 
We can apply the triangle inequality and we get:

$$||\+\vec{f}(\vec{p}) - \vec{f}(\vec{p})\+|| \leq ||\+\vec{f}(\vec{x}) -
\vec{f}_n(\vec{x})\+|| + ||\+\vec{f}_n(\vec{x}) - \vec{f}_n(\vec{p})\+|| +
||\+\vec{f}_n(\vec{p}) - \vec{f}(\vec{p})\+||$$

For large enough $n$, we know that $||\+\vec{f}_n - \vec{f}\+||_D < \frac{\varepsilon}{3}$
because $\vec{f}_n \uconv \vec{f}$. Now we know that the first and third term
are bounded by $\frac{\varepsilon}{3}$. The second term is bounded by
$\frac{\varepsilon}{3}$ because $\vec{f}_n$ is uniformally continuous.

Therefore, we know that $||\+\vec{f}(\vec{p}) - \vec{f}(\vec{p})\+|| \leq
\varepsilon$ for any $\delta$ we pick. $\therefore \vec{f}$ is continuous on
$D$.
%}}}
%{{{ Differentiability of Multivariate Functions
\chapter{Differential Calculus}
\section{Differentiability of Multivariate Functions}
\subsection{In Single Variable Calculus}
\subsubsection{Definition}

$f: [a,b] \to \varmathbb{R}$ is said to be \textbf{differentiable} at $p \in [a,b]^\circ$
(interior of $[a,b]$) if $\exists\+ a \in \varmathbb{R}$ such that:

$$\boxed{a = \lim_{h\to 0} \frac{f(p + h) - f(p)}{h}}$$

Now we can try to expand this to higher dimensions.

If we try to apply the same definition, we would supposedly get

$$a = \lim_{\vec{h}\to\vec{0}}\frac{f(\vec{p} + \vec{h}) - f(\vec{p})}{\vec{h}}$$

But this makes no sense! As we are dividing a number by a vector within the
limit. Therefore we must reconsider the definition of a derivative.

Let's rearrange some terms in our original equation, we get:
$$\lim_{h\to 0} \left|\frac{f(p + h) - f(p)}{h} - a\right| = 0$$

Let what's inside the limit be $\alpha(p, h)$. Now we multiply both sides by
$|h|$. We get:

$$|h| \left|\frac{f(p + h) - f(p)}{h} - a\right| = |f(p + h) - f(p) - ah|$$

Now we reach a new definition of differentiability. We say that $f$ is
differentiable at $p$ if $\exists\+ a \in \varmathbb{R}$ such that:

$$\boxed{\forall \varepsilon > 0, \exists \+ \delta: |h| < \delta \implies |f(p + h) -
f(p) - ah| < \varepsilon|h| \text{ }\text{ }\text{ }\text{ }(*)}$$

Then $a$ is known as the \textbf{derivative} of $f$ at $p$, we write it as $a =
f'(p) = \frac{df}{dx}(p)$

\subsubsection{Geometric Interpretation}
Basically we are trying to find a line with slope $a$ that mimics the behavior
of function $f$ locally around $p$, in some open ball $B_r(p)$. 
Let the difference between $p$ and a point close to $p$ along $f(x)$ as $g(x) =
f(p + x) - f(p)$ for $x \in (r, -r)$. Let the function of the line be $l(x) =
f(p) + a(x - p)$ be a line that approximates $f$ over $B_r(p)$. Note that this requires that
the vertical distance between $g(x)$ and $l(x)$ goes to 0 faster than $x$
grows, in other words the follow condition must be true:

$$\boxed{\frac{|g(x) - l(x)|}{|x|} \to 0 \text{ as } x \to 0}$$

We define local similar behavior as \textbf{superlinear decay or approximation}.

Note that this is simply a restatement of condition $(*)$, Note that $f(p + h) -
f(p)$ is simply $g(x) - l(x)$, and the difference between that and $a$ has
to be arbitarily small ($< \varepsilon$, note that the $|h|$ cancels out if you
rearrange the terms this way). Therefore the two definitions are equivalent.

\subsubsection{Uniqueness of the Derivative}
\textbf{Thm}: Either $\nexists\+ a \in \varmathbb{R}$ that satisfies condition
$(*)$, or $\exists!\+ a \in \varmathbb{R}$ such that condition $(*)$ is true. In
the latter case we denote the unique value of $a$ by $f'(p)$ or $\frac{df}{dx}(p)$

\textbf{Proof}: Say both $l$ and $\~{l}$ superlinearly approximate to $f(x)$ at point $p$, $l(x) = ax$ and
$\~{l}(x) = bx$. Then by definition we know that $\frac{|g(x) - ax|}{|x|} \to 0$ and
$\frac{|g(x) - bx|}{|x|} \to 0$ as $x \to 0$. Now we seek to find the difference
between $l$ and $\~{l}$:

\begin{align*}
    0 &\leq \frac{|ax - bx|}{|x|} = |a - b|\\
      &= \frac{|(g(x) - bx) - (g(x) - ax)|}{|x|}\\
      &\leq \frac{|g(x) - bx|}{|x|} + \frac{|g(x) - ax|}{|x|} \to 0 \text{ as }
    x \to 0
\end{align*}

Therefore by the squeeze theorem $|a- b| = 0 \implies a = b \implies l = \~{l}$.
Therefore there is only one line $l$ that can superlinearly approach $g$ at $x =
0$.

\subsection{The Gradient}
\subsubsection{Differentiability in Real Valued Functions}
We can easily generalize the definition into higher
dimensions, except we replace the concept of the product with dot product. We
say that
$f: D \subseteq \varmathbb{R}^d \to \varmathbb{R}$ is \textbf{differentiable} at
$\vec{p} \in D^\circ$ if $\exists\+ \vec{a}$ such that:

$$\boxed{\forall \varepsilon > 0, \exists \+ \delta: 0 < ||\+\vec{h}\+|| < \delta \implies
|f(\vec{p} + \vec{h}) - f(\vec{p}) - \vec{a} \cdot \vec{h}| <
\varepsilon||\+\vec{h}\+||\text{ }\text{ }\text{ }\text{ }(**)}$$

Here $\vec{a}$ is known as the \textbf{gradient} of $f$ at point $\vec{p}$. It is
commonly written as $\vec{\nabla}f(\vec{p})$. Note that even though in the
definition looks like $\vec{\nabla}$ is an operator applied to $f(\vec{p})$, it
is not. It only modifies $f$ to create a different function.

\subsubsection{Uniqueness of the Gradient}
\textbf{Thm}: For the function $f: D \to \varmathbb{R}$, $D \subseteq
\varmathbb{R}^d$, either no $\vec{a}$ satisfies
$(**)$ at point $\vec{p}$, in which case $f$ is not differentiable at $\vec{p}$.
Or $\exists!\+ \vec{a}$ that satisfies $(**)$, in which case $\vec{a} =
\vec{\nabla}f(\vec{p})$

\textbf{Proof}: This is very similar to how we proved that the derivative is
unique if it exists. Similarly, we assume that $\vec{a}$ and $\vec{b}$ are both
valid gradients for $f$ around $\vec{p}$. Then we attempt to find the difference
between the vertical distances between the two approximations created by the
gradients:

\begin{align*}
    |(\Delta f - \vec{b} \cdot \vec{h}) - (\Delta f - \vec{a} \cdot \vec{h})|
    &= |\vec{a} \cdot \vec{h} - \vec{b} \cdot \vec{h}| < 2 \varepsilon ||\+\vec{h}\+||\\
    ||\+(\vec{a} - \vec{b}) \cdot \vec{h}\+|| &< 2\varepsilon ||\+\vec{h}\+||\\
    \left|(\vec{a} - \vec{b}) \cdot \frac{\vec{h}}{||\+\vec{h}\+||}\right| &<
    2\varepsilon\\
\end{align*}

Let $\vec{u} = \frac{\vec{h}}{||\+\vec{h}\+||}$. Note, however, that
$||\+\vec{u}\+|| = 1$, and that is the only restriction on $\vec{u}$. Therefore
we create a sequences of possible $\vec{u}$: $\vec{e}_i, \forall 0 < i \leq d$,
with each $\vec{e}_i$ composed of all 0's except for the $i^{\text{th}}$ component.
This means that for every single component of $|(\vec{a} - \vec{b})|$ is bounded
by $2\varepsilon$. Therefore, by the componentwise nature of distance, we know
that $\vec{a} - \vec{b} = 0$. Which means that then $\vec{a}$ and $\vec{b}$ are
not distinct. Therefore $\vec{\nabla}f(\vec{p})$ is unique.

\subsubsection{Geometric Interpretation of the Definition}

In single variable calculus, we can say that the derivative defines a linear
function that approximates the function at a given point $\vec{p}$. In higher
dimensions, we need a function that provides us a superlinear approximation of
$f$. In higher dimensions, we find the \textbf{tangent plane or hyperplane}.

A hyperplane in $\varmathbb{R}^{d+1}$ is a set of the form $\Pi = \{\vec{x}
\in \varmathbb{R}^{d+1} \+|\+ (\vec{x} - \vec{p}_0) \cdot \vec{n} = 0\}$
where $\vec{p}_0 \in \varmathbb{R}^{d+1}$ and $\vec{n} \in \varmathbb{R}^{d+1}$
with $\vec{n} \neq 0$. In a more geometric sense, this definition says that we
want the set of all $\vec{x}$'s such that $(\vec{x} - \vec{p}_0)\perp\vec{n}$
(recall that the dot product of two perpendicular vectors is 0). In this
definition, $\vec{p}_0$ would be the foot of $\vec{n}$ and $\vec{x} - \vec{p}$
would be every point's displacement vector from the ``center'' of the plane. 
We call $\vec{n}$ the \textbf{normal} to $\Pi$, we write it as $\vec{n}\perp\Pi$.

But how do we relate this to the definition of the gradient? For this we go back
to definition $(**)$, specifically $|f(\vec{p} + \vec{h}) - f(\vec{p}) - \vec{a}
\cdot \vec{h}| < \varepsilon \+||\+\vec{h}\+||$. If we replace $\vec{p} + \vec{h}$
by $\vec{x}$, the definition becomes:

$$|\+f(\vec{x}) - [f(\vec{p}) + \vec{a}\cdot(\vec{x}-\vec{p})]\+| <
\varepsilon\+||\+\vec{x} - \vec{p}\+||$$

Note that $|\+f(\vec{x}) - [f(\vec{p}) + \vec{a} \cdot (\vec{x} - \vec{p})]\+|$
looks like the distance between $f$ and the graph $y = f(\vec{p}) + \vec{a} \cdot
(\vec{x} - \vec{p})$ at $\vec{x}$. Now we need to prove that the graph of $y =
f(\vec{p}) + \vec{a} \cdot (\vec{x} - \vec{p})$ is indeed a hyperplane. Let us
rearrange the terms and we get:

$$-\vec{a} \cdot (\vec{x} - \vec{p}) + (y - f(\vec{p})) = 0$$

Now we define a few vectors in $\varmathbb{R}^{d+1}$:
\begin{align*}
    \vec{N} &= (-a_1, -a_2, \dots, -a_d, 1)\\
    \vec{P}_0 &= (p_1, p_2, \dots, p_d, f(\vec{p}))\\
    \vec{X} &= (x_1, x_2, \dots, x_d, y)\\
    \vec{X} - \vec{P}_0 &= (x_1 - p_1, x_2 - p_2, \dots x_d - p_d, y - f(\vec{p}))
\end{align*}

Therefore, we can rearrange the previous equation as:

$$(\vec{X} - \vec{P}_0) \cdot \vec{N} = 0$$

Which is the general form of a hyperplane, and note that $\vec{N} \neq 0$
because its last component is 1.

Now if we go back to the definition of a gradient, we see that all that it is
saying is that there exists one hyperplane $f(\vec{p}) + \vec{a}\cdot(\vec{x} -
\vec{p})$ that approximates $f$ superlinearly near $\vec{p}$, and by the
uniqueness of the gradient, the hyperplane is unique.

We now know that $\vec{\nabla} f(\vec{p})$ defines a hyperplane that is tangent to the
graph of $f(\vec{x})$ at point $\vec{p}$, but what does this value represent? If
we look back to how we got to the hyperplane, we defined $\vec{N}$, the normal
to the tangent hyperplane as $(-\vec{\nabla}f(\vec{p}), 1)$. Therefore we can
interpret the gradient as the projection of the negation of the normal vector
onto the domain. Its direction is the direction of fastest increase in the
function at point $\vec{p}$ and the magnitude of the gradient is the rate of
fastest increase at $\vec{p}$. Note that the opposite direction of the gradient
(the direction of the normal vector projected onto the domain) is therefore the
direction of fastest decrease in the function at point $\vec{p}$ and the
magnitude of that vector is the rate of fastest decrease at $\vec{p}$.

\subsubsection{Computation of $\vec{\nabla}f$}

Take $\vec{h} = h\vec{e}_j$ , where $h\to 0$ and the set of $\vec{e}_j$ is known
as the \textit{standard basis vectors} in $\varmathbb{R}^d$:

\begin{equation*}
    \vec{e_j}=
    \begin{cases}
        \vec{e}_1 &= (1,0,0,\dots,0)\\
        \vec{e}_2 &= (0,1,0,\dots,0)\\
        \vdots & \vdots \hfill \vdots \hfill \vdots \hfill \vdots\\
        \vec{e}_d &= (0,0,0,\dots,1)\\
    \end{cases}
\end{equation*}

Note that because $||\+\vec{e}_j\+|| = 1$, $||\+\vec{h}\+|| =
|h|\+||\+\vec{e}_j\+|| = |h|$. Now if we fix a $j \in \{1,2,3,\dots,d\}$ and
apply the definition of differentiability, the unique gradient ($\vec{a}$) must
satisfy:

$$\forall \varepsilon > 0, \exists \+ \delta > 0: \forall h \text{ with } |h| <
\delta \implies |f(\vec{p} + h\vec{e}_j) - f(\vec{p}) - \vec{a}\cdot h\vec{e}_j|
< \varepsilon\+|h|$$

However, note that when we dot $\vec{a}$ with $\vec{e}_j$, the result is
the $j^{th}$ component of $\vec{a}$, or $a_j$. Now if we divide through by
$|h|$, we get:

$$\left|\frac{f(\vec{p} + h\vec{e}_j) - f(\vec{p})}{h} - a_j\right| <
\varepsilon$$

What this says is that $\left|\frac{f(\vec{p} + h\vec{e}_j) -
f(\vec{p})}{h}\right|$ approaches $a_j$ indefinitely, therefore, we can rewrite
the relationship as a limit statement:

$$\boxed{a_j = \lim_{h \to 0} \left|\frac{f(\vec{p} + h\vec{e}_j) -
f(\vec{p})}{h}\right|}$$

We call this $a_j$ as a \textbf{partial derivative} of $f(\vec{x})$ at $j^{th}$
component, which can be written as $\partial_{x_j} f(\vec{p})$ or $\frac{\partial
f}{\partial x_j} (\vec{p})$.

Note that:

$$\partial_{x_j}f(\vec{p}) = \frac{d}{dx_j}\bigg|_{x_j = p_j} f(p_1, p_2, \dots,
p_{j-1}, x_j, p_{j+1}, \dots, p_d)$$

In other words, we can hold all other components of $f$ constant and
differentiate based on only one component, and plug in the value $p_j$ after the
differentiation.

Now we know how to compute the gradient of $f$, it is simply the vector of all
the partial derivatives:

$$\vec{\nabla}f = (\partial_{x_1} f(\vec{p}), \partial_{x_2} f(\vec{p}),
\dots, \partial_{x_d}f(\vec{p}))$$

\subsubsection{Example}
Let's compute $\vec{\nabla}f(1,0,2)$ of

$$f(x, y, z) = x^2\sin(y + xz)$$

Let's calculate the partials first:

\begin{align*}
    \partial_{x} f &= x^2 \cos(y + xz)z + 2x\sin(y + xz)\\
    \partial_{y} f &= x^2 \cos(y + xz)\\
    \partial_{z} f &= x^3 \cos(y + xz)\\
\end{align*}

And if we plug in the numbers we get:
\begin{align*}
    \partial_x f(1,0,2) &= 2\cos(2) + 2\sin(2)\\
    \partial_y f(1,0,2) &= \cos(2)\\
    \partial_z f(1,0,2) &= \cos(2)\\
\end{align*}

Therefore, $\boxed{\vec{\nabla}f(1,0,2) = (2 \cos(2) + 2\sin(2), \cos(2),
\cos(2))}$

\subsubsection{Directional Derivative}

Take $\vec{u} \in \varmathbb{R}^d$, $\vec{u} \neq \vec{0}$ and a function $f: D
\subseteq \varmathbb{R}^d \to \varmathbb{R}$, and take a point on
the function $f$, $\vec{p}$ such that $f$ is continuous at $\vec{p}$, we define
the \textbf{directional derivative} $\partial_{\vec{u}}f(\vec{p})$ as:

\begin{align}
    \partial_{\vec{u}}f(\vec{p}) &:= \lim_{h\to0} \frac{f(\vec{p} + h\vec{u}) - f(\vec{p})}{h}\\
                                 &= \frac{d}{dt}\bigg|_{t=0} f(\vec{p} + t\vec{u})
\end{align}

This quality describes how fast the function $f$ is changing at $\vec{p}$ in the
direction of $\vec{u}$. The numerator of definition $(1)$ is the difference
between the function value at $\vec{p}$ and at $\vec{p} + h\vec{u}$, i.e. a
little increment in the direction of $\vec{u}$, and we divide that by $h$.

Definition $(2)$ gives a single variable definition of a directional derivative.
We can write $g(t)$ as the change in function value as we move away $\vec{p}$ in
the direction $\vec{u}$. Note that the curve of the form $\vec{\gamma}(t) =
\vec{p} + t\vec{u}$ ($\vec{u} \neq \vec{0}$) has a special name, it is called
\textbf{uniform rectilinear motion}. It is rectilinear because it is a line and
it is uniform because the speed of the curve is constant. The speed of a curve
is represented as $||\vec{r}'(t)||$

However, the magnitude of $\vec{u}$ also has meaning, its magnitude is the rate
of change of the function in the direction of $\vec{u}$.

\textbf{Thm}: Let $\vec{u}$ be any unit vector, then:
$$\partial_{\vec{u}}f(\vec{p}) = \vec{\nabla}f(\vec{p}) \cdot \vec{u}$$

\textbf{Proof}: %TODO

\subsubsection{Geometric Interpretation of the Gradient}
If $f: D \subseteq \varmathbb{R}^d \to \varmathbb{R}$ is differentiable at
$\vec{p} \in D^\circ$, then:

$$\vec{u}_0 = \frac{\vec{\nabla}f(\vec{p})}{||\+\vec{\nabla}f(\vec{p})\+||}$$

i.e. the unit vector of the gradient is the direction of steepest ascent for $f$
at $\vec{p}$.

$$||\+\vec{\nabla}f(\vec{p})\+|| = \max_{||\+\vec{u}\+|| = 1}
\partial_{\vec{u}}f(\vec{p})$$

And the magnitude of the gradient is the rate at which the function is ascending
at the point.

This is a combination of the Cauchy-Schwarz Inequality and the theorem which states that
$\partial_{\vec{u}}f(\vec{p}) = \vec{\nabla}f(\vec{p}) \cdot \vec{u}$.
By the Cauchy-Schwarz Inequality, we know that the length of the gradient is an
upper bound of the directional derivative. This maximum is also obtained,
because in Cauchy-Schwarz inequality, the equality case happens when the two
vector have the same direction. Therefore, we know that the gradient is the direction of
fastest ascent of a function at any given point $\vec{p}$

However, the Cauchy-Schwarz inequality also states that the dot product is
bounded from below by negative of the product of the lengths, which is achieved
when the two vector are anti-directional. Therefore we also know that the
negative of the gradient points in the direction of steepest descent and the
magnitude can be written as:

$$-||\+\vec{\nabla}f(\vec{p})\+|| = \min_{||\+\vec{u}\+|| = 1} \partial_{\vec{u}}
f(\vec{p})$$

\subsection{Basic Rules of the Gradient Operator}
\begin{enumerate}
    \item $\gradient (f + g) = \gradient f + \gradient g$ (If $f$ and
        $g$ are both differentiable, then $f + g$ is differentiable as well)
    \item $\gradient (cf) = c \gradient f$ (floaty constant rule)
    \item $\gradient (fg) = f \gradient g + g \gradient f$ (product rule)
    \item $\gradient \frac{f}{g} = \frac{g \gradient f - f\gradient g}{g^2}$ (quotient rule, as long as $g \neq 0$)
\end{enumerate}

These rules are true because in each component, the partial operator obeys these
rules, therefore the gradient also obeys these rules.
%}}}
%{{{ Theorems about Gradient

\section{Theorems about Gradient}

\subsection{Rolle's Theorem}
\subsubsection{Theorem}
If $f$ is differentiable on $(a - r, b + r)$ where $r > 0$ and $a < b$, and
$f(a) = f(b)$. Then $\exists \+ c \in (a, b)$ such that $f'(c) = 0$.

\subsubsection{Proof}

$f$ is either constant or it is not on $[a, b]$. If it is constant, then the
theorem is trivially true. If not, then there exists $p \in [a,b]$ such that
$f(p) \neq f(a) = f(b)$. If $f(p) > f(a)$, then take $c$ to be a global maximum
point for the function $f$ on the interval $[a, b]$ (EVT). Note that $c \neq a$,
$c \neq b$, so $c \in (a, b)$. Let us then consider $f'(c)$. We are given that

$$\lim_{h \to 0} \frac{f(c + h) - f(c)}{h}$$

But if we consider the limits from two directions, we get:

$$\lim_{h \to 0^+} \frac{f(c + h) - f(c)}{h} \leq 0$$
$$\lim_{h \to 0^-} \frac{f(c + h) - f(c)}{h} \geq 0$$

Therefore, $f'(c) = 0$. A similar argument can be made if $f(p) < f(a}$

\subsection{Mean Value Theorem}
\subsubsection{Review of the Single Variable Theorem}
If $f : [a, b] \to \varmathbb{R}$ is differentiable on $(a - r, b + r)$ where $r
> 0$ and continuous
on $[a, b]$, then $\Delta f = f(b) - f(a) = f'(c)(b - a)$ where $c \in (a, b)$.

\textbf{Proof}

For the given function $f$, let us create a function $l: y = f(a) + \frac{f(b) -
f(a)}{b - a} (x - a)$, or a secant line connecting $a$ to $b$. Let $g(x) = f(x)
- l(x)$. Then, $g'(x) = f'(x) - \frac{f(b) - f(a)}{b - a}$. Note also $g(a) = 0
= g(b)$. Then by Rolle's Theorem, $\exists \+ c \in (a, b)$ such that $g'(c) = 0$.
This implies that at $c$, $f(c)$ has the same derivative as $l(c)$, or
$\frac{f(b) - f(a)}{b - a}$.

\subsubsection{Theorem}
Exists $f: D \subseteq \varmathbb{R}^d \to \varmathbb{R}$, $\vec{p}, \vec{q} \in
D^\circ$, $[\vec{p}, \vec{q}] \subseteq D^\circ$ \footnote{This represents a
\textbf{closed segment} with the end points of $\vec{p}$ and $\vec{q}$, which can be
expressed as $[\vec{p}, \vec{q}] = \{(1 - t)\vec{p} + t \vec{q} \+|\+ 0 \leq t
\leq 1\}$. An \textbf{open segment} is written as $(\vec{p}, \vec{q}) = \{(1 -
t) \vec{p} + t \vec{q} \+|\+ 0 < t < 1\}$}. Also assume that $f$ is
differentiable on $D$. Then $\exists\+\vec{r} \in (\vec{p}, \vec{q})$ such that:

$$\boxed{f(\vec{q}) - f(\vec{p}) = \gradient f(\vec{r}) \cdot (\vec{q} - \vec{p})}$$

\subsubsection{Proof}

Let $g(t) = f((1 - t)\vec{p} - t\vec{q})$, where $-\varepsilon \leq t \leq 1 +
\varepsilon$ for some $\varepsilon > 0$. We can do this because $\vec{p},
\vec{q} \in D^\circ$. Now we can try to take the derivative of $g(t)$. To do
this we use the chain rule: $g'(t) = \gradient f((1 - t) \vec{p} + t\vec{q})
\cdot (\vec{q} - \vec{p})$. We now apply the Mean Value Theorem to $g$, we know
that $\exists \+ c \in (0, 1)$ such that $g'(c) = \frac{g(1) - g(0)}}1 - 0 =
f(\vec{q}) - f(\vec{p})$. Now we have:

$$f(\vec{q}) - f(\vec{p}) = \gradient f((1 - c) \vec{p} + c\vec{q}) \cdot
(\vec{q} - vec{p})$$

Now we simply call $(1 - c)\vec{p} + c\vec{q}$ $\vec{r}$, and we're done.

\subsubsection{For Vector Valued Functions}

Sadly there is no generalization of the MVT to vector valued functions. If we
apply the MVT to each of its component functions, we see that for each $\vec{r}$
may be different. Therefore there is no easy, consistent generalization.

In order to actually generalize this, we will need to make the statement weaker.

We can at least provide a bound for $|f(\vec{p}) - f(\vec{q})|$. By
Cauchy-Schwarz inequality, we get:

$$|f(\vec{q}) - f(\vec{p})| \leq ||\+\gradient f(\vec{r})\+||\+ ||\+\vec{q} -
\vec{p}\+||$$

for some $\vec{r} \in (\vec{p}, \vec{q})$. However, we can write the most
general case as:

$$\boxed{|f(\vec{q}) - f(\vec{p})| \leq \left(\sup_{\vec{x} \in [\vec{p} , \vec{q}]}
||\+\gradient f(\vec{x})\+||\right) ||\+\vec{q} - \vec{p}\+||}$$

This is known as the \textbf{Mean Value Inequality}. And as a corollary, we can
take any $B$ such that if $ ||\+\gradient f\+|| \leq B$ on $[\vec{p}, \vec{q}]$
then:

$$|f(\vec{q}) - f(\vec{p})| \leq B ||\+\vec{q} - \vec{p}\+||$$

To generalize the Mean Value Inequality to vector valued function, let $\vec{f}:
D \subseteq \varmathbb{R}^d \to \varmathbb{R}^e$, and $[\vec{p}, \vec{q}]
\subseteq D^\circ$. If $\vec{f}$ is $C^1$ \footnote{For vector valued function,
$C^n$ over $U$ means that every $\frac{\partial^n f_i}{(\partial x_j)^n}(\vec{x})$ is
continuous for all $\vec{x} \in U$, $1 \leq i \leq e$ and $q \leq j \leq d$} in
some open set $U$ such that $[\vec{p}, \vec{q}] \subseteq U \subseteq D^\circ$.

$$\boxed{\therefore ||\+\vec{f}(\vec{q}) - \vec{f}(\vec{p})\+|| \leq
\left(\max_{[\vec{p}, \vec{q}]} ||\+D\vec{f}\+||\right) ||\+\vec{q} -
\vec{p}\+||}$$

FIXME moving needed!

Let $M := \max_{[\vec{p}, \vec{q}]} ||\+D\vec{f}\+||$. Fix $\varepsilon > 0$.
Let $D = \{t \in [0, 1] \+|\+ ||\+\vec{f}(\vec{q}_t) - \vec{f}(\vec{p})\+|| \leq
(M + \varepsilon) ||\+\vec{q}_t = \vec{p}\+||\}$ where $\vec{q}_:= (1 -
t)\vec{p} + t\vec{q}$. Note that $\vec{q}_0 = \vec{p}$ and $\vec{q}_1 =
\vec{q}$.

Note that $0 \in S$, which means that $S \neq \emptyset$. Also note that $S \leq
1$ because $t \in [0, 1]$. Therefore $\sup S$ exists and finite (bounded by $[0,
1]$), let us call this value $T$. This means that $\exists\+ t_n \in S$ such
that $t_n \to T^-$. Therefore, $\forall n, ||\+\vec{f}(\vec{q}_{t_n}) -
\vfofpv\+|| \leq (M + \varepsilon) ||\+\vec{q}_{t_n} - \vec{p}\+||$. Now take the
limit as $n \to \infty$ and we get:

$$ ||\+\vec{f}(\vec{q}_T) - \vfofpv\+|| \leq (M + \varepsilon) ||\+\vec{q}_T
- \vec{p}\+||$$

This means that $T \in S$ as well. Assume for contradiction that $T < 1$. It
will follow that $T = 1$. Because $\vec{q}_1 = \vec{q}$, it follows that:

$$ ||\+\vec{f}(\vec{q}) - \vfofpv\+|| < (M + \varepsilon) ||\+\vec{q} - \vec{p}\+||$$

Now take the limit as $\varepsilon \to 0^+$. We can then conclude:

$$ ||\+\vec{f}(\vec{q}) - \vfofpv\+|| \leq M ||\+\vec{q} - \vec{p}\+||$$

Say $T < 1$. Choose $\delta > 0$ such that $\tau = T + \delta \leq 1$. We now
seek to show:

$$ ||\+\vec{f}(\vec{q}_{\tau}) - \vec{f}(\vec{p})\+|| \overset{?}{\leq} (M +
\varepsilon) ||\+\vec{q}_{\tau} - \vec{p}\+||$$

We first add a bunch of terms to the left hand side and use the triangle
inequality.

$$ ||\+\vec{f}(\vec{q}_{\tau}) - \vec{f}(\vec{p})\+|| =
||\+\vec{f}(\vec{q}_{\tau}) - \vec{f}(\vec{q}_T) - A_T(\vec{q}_{\tau} -
\vec{q}_T)\+||$$

\subsection{Caution}
Before we move on, it is important to note that the mere existence of the
partial derivatives is \textbf{NOT} sufficient to guarantee differentiability at
point $\vec{p}$

In other words, there may be a function $f$ and a point $\vec{p} \in D_f$, such
that $\partial_{x_1} f(\vec{p}), \partial_{x_2} f(\vec{p}), \dots,
\partial_{x_d} f(\vec{p})$ all exist in the sense of limits, the function may
still not be differentiable at $\vec{p}$. This is because differentiability
means that the function can be locally approximated by a hyperplane, and it is
entirely possible that a function behaves nicely on every axis but still fails
to behave nicely between the axis. In other words, the partials tells you the
smoothness of the section curves, and nothing in between.

For example, consider a hemisphere with an octave cut out. TODO later

They don't even imply continuity at $\vec{p}$!

\subsection{Sufficient Condition for Differentiability}
\subsubsection{Theorem}
If $f: D \subseteq \varmathbb{R}^d \to \varmathbb{R}$ and $\vec{p} \in D^\circ$
and if $\vec{\nabla}f(\vec{x})$ exists for all points $\vec{x} \in
B_r(\vec{p})$ $(r > 0)$ (in that the vector of partial derivatives exists and is
defined) and is continuous at $\vec{p}$, then $f$ is
differentiable at $\vec{p}$.

In other words, the gradient not only have to exist at $\vec{p}$, it also has to
exist around $\vec{p}$. In other words, we need to define each partial as a
function of $\vec{x}$ and prove that all those are continuous, we do so by the
theorem that states the composition of continuous functions is continuous.

\subsubsection{Example}
\begin{align*}
    f(x, y) &= xy \sin(x + y^2)\\
    \partial_x f(x, y) &= xy \cos(x + y^2) + y \sin(x + y^2)\\
    \partial_y f(x, y) &= 2xy^2\cos(x + y^2) + x\sin(x + y^2)
\end{align*}

In this case, take $\vec{p} = (0, 1)$, we know that the partials all exist
around $\vec{p}$ and is definitely continuous at $\vec{p}$, since both partials
are compositions of continous functions. Therefore we can say that the gradient
exists at $\vec{p}$.

\subsubsection{Proof}
Let $\vec{h} = (h_1, h_2, \dots, h_d)$ and consider the point $\vec{p} +
\vec{h}$. We will construct a path from $\vec{p}$ to $\vec{p} + \vec{h}$ such
that in each ``step'' we move $h_d$ in the $d^{th}$ axial direction, and we call
each intermediate point $\vec{p}_d$ in the following manner where $\vec{e}_d$ is
the $d^{th}$ standard basis vector.
\begin{align*}
    \vec{p}_0 &= \vec{p}\\
    \vec{p}_1 &= \vec{p}_0 + h_1 \vec{e}_1\\
    \vec{p}_2 &= \vec{p}_1 + h_2 \vec{e}_2\\
    \vdots &= \text{     } \vdots \text{     } \vdots \text{     } \vdots\\
    \vec{p}_d &= \vec{p}_{d-1} + h_d \vec{e}_d
\end{align*}

Now consider the function $\Delta f = f(\vec{p} - \vec{h}) - f(\vec{p})$, we can
write this as a telescoping sum:

$$\Delta f = \sum_{j=1}^d \{f(\vec{p}_j) - f(\vec{p}_{j-1})\}$$

Within each term, we can use the mean value theorem from one dimensional
calculus. We can do so because $\vec{p}_j$ and $\vec{p}_{j - 1}$ only differ in
1 coordinate, like the following:

\begin{align*}
    \vec{p}_{j - 1} &= (p_1 + h_1, \dots p_{j-1} + h_{j-1}, \boxed{p_j}, \dots p_d)\\
    \vec{p}_{j} &= (p_1 + h_1, \dots p_{j-1} + h_{j-1}, \boxed{p_j + h_j}, \dots p_d)
\end{align*}

We also know that the partial derivatives of $f$ exists for all points within
$B_r(\vec{p})$, therefore we can indeed apply the Mean Value Theorem.

Now we apply the MVT:

$$\Delta f = \sum_{j=1}^d \partial_{x_j} f(\vec{q}_j) h_j$$

Where $\vec{q}_j = (p_1 + h_1, \dots, p_{j-1} + h_{j-1}, p_j + \theta h_j,
\dots, p_d)$ where $0 < \theta < 1$. In other words, $\vec{q}_j$ is somewhere
between $\vec{p}_{j-1}$ and $\vec{p}_{j}$ in the $j^{th}$ coordinate.

Now let us consider the definition of differentiability, we need to prove that 


$$|\Delta f - \vec{\nabla}f(\vec{p}) \cdot \vec{h}| = |\sum_{j = 1}^d
\partial_{x_j} f(\vec{q}) h_j - \sum_{j = 1}^d \partial_{x_j} f(\vec{p}) h_j|$$

Now we can apply the triangle inequality on the summations and get:

$$|\Delta f - \vec{\nabla}f(\vec{p}) \cdot \vec{h}| < \sum_{j=1}^d |\partial_{x_j} f(\vec{q}_j) - \partial_{x_j} f(\vec{p})|
\+ |h_j|$$

Now we divide both sides by the length of $\vec{h}$:

\begin{align*}
    \frac{|\Delta f - \gradient f(\vec{p}) \cdot \vec{h}|}{||\+\vec{h}\+||} &\leq
    \sum_{j=1}^d |\partial_{x_j} f(\vec{q}_j) - \partial_{x_j} f(\vec{p})| \+
    \frac{|h_j|}{||\+\vec{h}\+||}\\
    &\leq \sum_{j=1}^d |\partial_{x_j} (\vec{q}_j) - \partial_{x_j} f(\vec{p})|
\end{align*}

Now let $\vec{h} \to \vec{0}$. Then each $\vec{q}_j$ approaches $\vec{p}$. Then
we know:

$$\sum_{j=1}^d |\partial_{x_j} (\vec{q}_j) - \partial_{x_j} f(\vec{p})| \to 0$$

Since the number of terms in a sum is fixed and each term within the sum is
going to 0, the entire sum is going to 0, which means $||\+\vec{h}\+|| <
\delta(\varepsilon)$, we can say that the sum is less than $\varepsilon$, which
means that

$$\frac{|\Delta f - \gradient f(\vec{p}) \cdot \vec{h}|}{||\+\vec{h}\+||} \leq
\sum_{j=1}^d |\partial_{x_j} f(\vec{q}_j) - \partial_{x_j} f(\vec{p})| \+
\frac{|h_j|}{||\+\vec{h}\+||} \leq \varepsilon$$

Which means that a superlinear decay is possible, which means that $f$ is
differentiable.

\subsection{Differentiability $\implies$ Continuity}
\subsubsection{Theorem}
If $f: D \to \varmathbb{R}$, $D \subseteq \varmathbb{R}^d$ is differentiable at
$\vec{p} \in D^\circ$ then $f$ is continuous at $\vec{p}$.

\subsubsection{Proof}
Let us examine our definition of differentiability and continuity. $f$ is said
to be differentiable if: $\forall \varepsilon > 0, \exists\+ \delta > 0, \forall
\vec{h} \text{ with } ||\+\vec{h}\+||$:
$$|f(\vec{p} + \vec{h}) - f(\vec{p}) - \gradient f(\vec{p}) \cdot h| <
\varepsilon ||\+\vec{h}\+||$$

To prove continuity, we need to extablish a bound on $|f(\vec{p} + \vec{h}) -
f(\vec{p})|$:

\begin{align*}
    |f(\vec{p} + \vec{h}) - f(\vec{p})| &= |f(\vec{p} + \vec{h}) - f(\vec{p}) -
    \gradient f(\vec{p}) \cdot \vec{h} + \gradient f(\vec{p})\cdot \vec{h}|\\
    &\leq |f(\vec{p} + \vec{h}) - f(\vec{p})| + |\gradient f(\vec{p}) \cdot
    \vec{h}|\\
    &\leq \varepsilon ||\+\vec{h}\+|| + ||\+\gradient
    f(\vec{p})\+||\+||\+\vec{h}\+||\\
    &= (\varepsilon + |||\+\gradient f(\vec{p})\+||) \times ||\+\vec{h}\+||
\end{align*}

Now we are done because $(\varepsilon + ||\+\gradient f(\vec{p})\+||) \times
||\+\vec{h}\+||$ is a constant which we can make arbitarily small. Therefore,
$f$ is continuous if it is differentiable.

\subsection{Equality of Mixed Partials (Clairaut's/Schwarz' Theorem)}
\subsubsection{Notations -- Higher Order of Partials}
Classically, we also define second order partial derivatives as follows:

\begin{align*}
    \partial_{x_i} \partial_{x_i} &= \partial_{x_i}^2 f = \frac{\partial^2 f}
    {\partial x_i^2}\\
    \partial_{x_i} \partial_{x_j} &= \partial_{x_i} \partial_{x_j} f =
    \frac{\partial^2 f}{\partial x_i \partial x_j}
\end{align*}

\subsubsection{Notation -- Composition of Functions}
We define a self-application of a function by an ``exponent'' statement, for
example, $f(f(x))$ is sometimes written as $f^2(x)$. To differentiate this from
regular exponentiation, $(f \circ f)(x) = f^{\circ 2}(x)$.

\subsubsection{Special Set of Functions -- $C^k$}
$f \in C^2$ if over some set, $\partial_{x_i}\partial_{x_j} f$ is continuous for any $i, j \in
\{1, 2, \dots, d\}$.

We can also define $C^k$ ($k \geq 2$), which is defined as the set of
functions $f$ for which for any $i_1, i_2, \dots, i_k \in \{1,2,3,\dots,k\}$,
$\partial_{x_{i_1}}\partial_{x_{i_2}}\dots\partial_{x_{i_k}}f$ is continuous
over some set.

And we define $C^0$ on a region if the function itself (or the $0^{th}$
derivative) is continuous on that region, and $C^1$ is defined as the set of
functions if its partials are continuous over some region.

Note that $C^1$ implies differentiability because $C^1$ implies that the
partials not only exist, but are also continuous around the point that is being
differentiated.

\textbf{Thm}; If $f$ is $C^k$ ($k \geq 1$), then $f$ is also $C^{k-1}, C^{k-2},
\dots, C^0$. In other words, $C^k(D) \subseteq C^{k-1}(D) \subseteq C^{k-2}
\subseteq \dots \subseteq C^1(D) \subseteq C^0(D)$.

We prove this by induction, with the base case being $k = 1$, which implies
$C^0$. This is trivial because $C^1$ implies differentiability which implies
continuity over the set.

The inductive step is trivial, suppose we have $f \in C^{k+1}$, we wish to prove
that $f$ is also $C^{k}, C^{k-1}, \dots, C^0$. Note that we actually only have
to prove $f$ is $C^k$, and the inductive assumption will take care of it from
there. For every possible combination of
partials,$\partial_{x_{i_1}}\partial_{x_{i_2}}\dots\partial_{x_{i_{k+1}} }f$,
consider the function $g :=
\partial_{x_{i_1}}\partial_{x_{i_2}}\dots\partial_{x_{i_k}}f$. Note that the
combination of partials is simply $\partial_{x_{i_{k+1} }}g$. Now we seek to
prove that $g$ is continuous, which is just the base case. Therefore, if $f \in
C^{k+1}$, $f \in C^k, C^{k-1}, \dots, C^0$.

\subsubsection{Theorem}
$f: D \to \varmathbb{R}$, $D \subseteq \varmathbb{R}^d$ and $\vec{p} \in D^\circ$, if
$\partial_{x_j}\partial_{x_i}$ and $\partial_{x_i}\partial_{x_j}$ both exist and
are continuous in the neighborhood of $\vec{p}$ then $\partial_{x_j}\partial_{x_i} f(\vec{p}) =
\partial_{x_i}\partial_{x_j} f(\vec{p})$.

In other words, if $f \in C^2(B_r(\vec{p}))$, $r > 0$, for $\vec{p} \in D^\circ$, then
$\partial_{x_j}\partial_{x_i} f(\vec{p}) = \partial_{x_i}\partial_{x_j}
f(\vec{p})$

\subsubsection{Proof}
WLOG, assume $i \leq j$. Let $a = p_i$ and $b = p_j$.

Define $g(x, y) = f(p_1, \dots, p_{i-1}, x, p_{i+1}, \dots, p_{j-1}, y, p_{j+1},
\dots, p_d)$.

%In terms of $g$, we
%can rewrite the partials:
%
%\begin{align*}
%    \partial_{x_j}\partial_{x_i} f(p_1, \dots, x, \dots, y, \dots p_d) &= \partial_y
%    \partial_x g(x, y)\\
%    \partial_{x_i}\partial_{x_j} f(p_1, \dots, x, \dots, y, \dots p_d) &= \partial_x
%    \partial_y g(x, y)\\
%\end{align*}
%
%We know that both partials exist in some neighborhood of $p_i, p_j$ and both are
%continuous at $p_i, p_j$.

Conside the function $\Delta(h) = g(a + h, b + h) + g(a, b) - g(a + h, b) - g(a,
b + h)$. Let us rearrange some terms, $\Delta(h) = \{g(a + h, b + h) - g(a + h,
b)\} - \{g(a, b + h) - g(a, b)\}$. If we define $G(x) = g(x, y+h) - g(x, y)$, we can
rewrite $\Delta(h) = G(a+h) - G(a)$. Now apply the MVT, and we get:

$$\Delta(h) = G'(a + \theta_h h) \times h \text{        where } 0 < \theta_h < 1$$

Note that $G'(x) = \partial_x g(x, y+h) - \partial_x g(x, y)$. The derivates
must exist because $C^2$ implies differentiability. Now if we expand $\Delta(h)$, we
get:

$$\Delta(h) = [\partial_{x} g(a + \theta_h h, b + h) - \partial_{x} g(a + \theta_hh,
b)] h$$

Note that here only the second argument of $g$ is changing, so let us apply the MVT again ($C^2$
implies that the partials are differentiable), and we get:

$$\Delta(h) = \partial_{y} (\partial_{x} g) (a + \theta_h h, b + \phi_h h)h^2 \text{
where } 0 < \phi_h < 1$$

Now take the limit of $\frac{\Delta(h)}{h^2}$ as $h \to 0$, we get:

$$\lim_{h\to0} \frac{\Delta(h)}{h^2} = \partial_{y} \partial_{x} g(a, b)$$

But note that if we made $G$ a function of $y$ instead of $x$ ($G(y) := g(x + h,
y) - g(x, y)$), we could have written $\Delta(h) = \{g(a, b+h) - g(a, b+h)\} -
\{g(a + h, b) - g(a, b)\}$, which is just $G(b + h) - G(b)$. Now we can
apply the same procedure except we first differentiate with respect to $y$ and
then with respect to $x$, and we get that the limit of $\frac{\Delta(h)}{h^2}$
as $h \to 0$ is $\partial_{x} \partial_{y} g(a, b)$. Therefore,
$\partial_{y} \partial_{x} g(a, b) = \partial_{x} \partial_{y} g(a,
b)$. Now we can convert $g(x, y)$ back into $f(\vec{f})$:
\begin{align*}
    \partial_y \partial_x g(x, y) &= \partial_{x_j}\partial_{x_i} f(p_1,
    \dots, x, \dots, y, \dots p_d)\\
    \partial_x \partial_y g(x, y) &= \partial_{x_i}\partial_{x_j} f(p_1, \dots, x, \dots, y, \dots p_d)
\end{align*}

If we plug back in the definition of $a$ and $b$, we get that $\partial_{x_j}
\partial{x_i} f(\vec{p}) = \partial_{x_i}\partial_{x_j} f(\vec{p})$. $\blacksquare$


\subsubsection{Corollary and Generalization}

If $f: D \to \varmathbb{R}$, $D \subseteq \varmathbb{R}^d$ is $C^k$ on
$B_r(\vec{p})$ for some $k \geq 2$, then if $(i_1, i_2, \dots, i_k)$ and $(j_1,
j_2, \dots, j_k)$ are permutations of each other, then
$\partial_{x_{i_1}}\partial_{x_{i_2}}\dots\partial_{x_{i_k}} f(\vec{p}) =
\partial_{x_{j_1}}\partial_{x_{j_2}}\dots\partial_{x_{j_k}} f(\vec{p})$

\textbf{Proof}: We again prove this via induction, base case is the original
theorem. We now seek to prove that if the Clairaut theorem applies for all
functions that are $C^0, C^2, \dots, C^k$, then it is also applicable to
functions that are $C^{k+1}$. We do so by proving that we can switch the location of any two
partials and the result would not change. Consider
$\partial_{x_{i_1}}\partial_{x_{i_2}}\partial_{x_{i_3}}\dots\partial_{x_{i_k}}\partial_{x_{i_{k+1}
}} f(\vec{p})$ and
$\partial_{x_{i_1}}\partial_{x_{i_2}}\partial_{x_{i_3}}\dots\partial_{x_{i_{k+1}
}}\partial_{x_{i_k}} f(\vec{p})$. Note that because $f \in C^2$, $\partial_{x_{i_{k+1}
}}\partial_{x_{i_k}} f(\vec{p}) = \partial_{x_{i_k}}\partial_{x_{i_{k+1} }} f(\vec{p})$.
Therefore, the two original statements are equal. Now we consider the what if
a pair in the middle were switched, like the following:
$$\partial_{x_{i_1}}\partial_{x_{i_2}}\partial_{x_{i_3}}\dots\partial_{x_{i_m}}\partial_{x_{i_n}}\dots
\partial_{x_{i_k}}\partial_{x_{i_{k+1}} } f(\vec{p})$$
$$\partial_{x_{i_1}}\partial_{x_{i_2}}\partial_{x_{i_3}}\dots\partial_{x_{i_n}}\partial_{x_{i_m}}\dots
\partial_{x_{i_k}}\partial_{x_{i_{k+1} }} f(\vec{p})$$
Note that since $f \in
C^k$, therefore, $\partial_{x_{i_{n+1}} }\dots\partial_{x_{i_{k+1} }}f(\vec{p})$
is $C^{k - n}$, which means it is $C^{2}$ as well as long as $n \leq k - 2$. (The
case of $n > k - 2$ has already been considered). Therefore, denote $g(\vec{p})
= \partial_{x_{i_{n+1}} }\dots\partial_{x_{i_{k+1} }}f(\vec{p})$ and now we just
apply the base case again to get that if we switch two partials in the middle
the end result in fact does not change.
%}}}
%{{{ Differentiability of Vector Valued Functions
\section{Differentiability of Vector Valued Functions}
\subsection{Definition of Differentiability}
Let $\vec{f} : D \subseteq \varmathbb{R}^d \to \varmathbb{R}^e$. Let $\vec{p}
\in D^\circ$. Then $\vec{f}$ is differentiable at $\vec{p}$ if $\exists \+A \in
\varmathbb{R}^{e \times d}$ such that

$$\boxed{\forall \varepsilon > 0, \exists \+ \delta > 0: 0 < ||\+\vec{h}\+|| < \delta
\implies ||\+\vec{f}(\vec{p} + \vec{h}) - \vec{f}(\vec{p}) - A
\vec{h}\+|| < \varepsilon ||\+\vec{h}\+||}$$

If $A$ exists, then we call it the derivative of $\vec{f}$ at point $\vec{p}$,
we write it as $D\vec{f}(\vec{p}) \in \varmathbb{R}^{e \times d}$. This is called the
\textbf{Jacobian Derivative} of the function $\vec{f}$ at the point $\vec{p}$.

\subsection{Uniqueness of the Jacobian Derivative}
\subsubsection{Theorem}

Either there does not exist a matrix $A$ such that satisfies the condition for
differentiation, or  there exists only one $A$. In the latter case, $\vec{f}$ is
said to be differentiable at $\vec{p}$ and $A = D\vec{f}\vec{p}$.

\subsubsection{Proof}

First we seek to prove that the inequality decomposes componentwise. We know
that the $i^{th}$ component of a vector difference is the difference of the
$i^{th}$ component of each vector. If we express $\vec{\alpha}_i^T$ as the
$i^{th}$ row of $A$, then we get that for the $i^{th}$ component,
the definition of a derivative reduces to:

$$|f_i(\vec{p} + \vec{h}) - f_i(\vec{p}) - \vec{\alpha}_i \cdot \vec{h}| <
\varepsilon ||\+\vec{h}\+||$$

Note that here, $\vec{\alpha}_i = \gradient f_i(\vec{p})$. Therefore we know
that $\alpha_i$ is uniquely determined because of the uniqueness of the
gradient. This then means that $A$ is unique because each row is uniquely
determined.

\subsection{Form of the Jacobian Derivative}

We also get an expression for $D\vec{f}$:

\[
    D\vec{f} = \left[\begin{array}{c}
    (\gradient f_1)^T\\
    (\gradient f_2)^T\\
    \vdots\\
    (\gradient f_e)^T
    \end{array}\right] = \left[
        \begin{array}{c}
            Df_1 \\
            Df_2 \\
            \vdots\\
            Df_e
        \end{array}
    \right] = 
    \left[\begin{array}{cccc}
            \partial_{x_1} f_1 & \partial_{x_2} f_1 & \cdots &
            \partial_{x_d} f_1\\
            \partial_{x_1} f_2 & \partial_{x_2} f_2 & \cdots &
            \partial_{x_d} f_2\\
            \vdots & \vdots & \ddots & \vdots\\
            \partial_{x_1} f_e & \partial_{x_2} f_e & \cdots &
            \partial_{x_d} f_e
    \end{array}\right] =
    [\partial_{x_1} \vec{f} \+\+ \partial_{x_2} \vec{f} \+\+ \dots \+\+ \partial_{x_d}
    \vec{f}]
\]

In other words, $[D\vec{f}]_{ij} = \partial_{x_j} f_i = \frac{\partial
f_i}{\partial x_j}$

%}}}
%{{{ Theorems about Differentiability
\section{Theorem about Differentiability}
\subsection{Chain Rule}
\subsubsection{Theorem}
Take $\vec{f} : D \subseteq \varmathbb{R}^d_{\vec{x}} \to
\varmathbb{R}^e_{\vec{y}}$ and $\vec{g}: E
\subseteq \varmathbb{R}^e_{\vec{y}} \to \varmathbb{R}^k_{\vec{z}}$. \footnote{This is a naming convention, the subscript
is how we denote the axis in the space. For example, dimensions within
$\varmathbb{R}^d_{\vec{x}}$ are denoted by $\vec{x}_1, \vec{x}_2, \dots,
\vec{x}_d$}And suppose $\vec{f}(D) \cap E
\neq \emptyset$. Take a point $\vec{p} \in D^\circ$ and $\vec{q} = \vec{f}(\vec{p}) \in
E^\circ$. If $\vec{f}$ is differentiable at $\vec{p}$ and $\vec{g}$ is
differentiable at $\vec{q}$, then $\vec{g} \circ \vec{f}$ is
ferentiable at $\vec{p}$ and $D(\vec{g} \circ \vec{f}) (\vec{p}) =
D\vec{g}(\vec{f}(\vec{p})) D\vec{f}(\vec{p})$. In other words, $[D(\vec{g} \circ
\vec{f})]_{ij} = \partial_{x_j}(g_i \circ \vec{f})(\vec{p})$. Classically, this
has also been written as $\frac{\partial z_i}{\partial x_j} (\vec{p})$. Note
that $[D\vec{g}(\vec{q}) D\vec{f}(\vec{p})]_{ij} = \sum_{l = 1}^e \partial_{y_l} g_i
(\vec{q}) \partial_{x_j} f_l (\vec{p})$. This has been classically been written
as:

\[
    \frac{\partial z_i}{\partial x_j} = \sum_{l = 1}^e \frac{\partial z_i}{\partial y_l} (\vec{f}(\vec{p}))
    \frac{\partial y_l}{\partial x_j} (\vec{p})
\]

The ``legit'' definition of the chain rule is:

\[
    \partial_{x_j} (g_i \circ \vec{f})(\vec{p}) = \sum_{l = 1}^e \partial_{y_l}
    g_i(\vec{q}) \partial_{x_j f_l (\vec{p})}
\]

This is also clasically written as:

\[
    \frac{\partial z_i}{\partial x_j} = \sum_{l=1}^e \frac{\partial
    z_i}{\partial y_l} \frac{\partial y_l}{\partial x_j}
\]

\subsubsection{Proof}

For simplicity's sake, let $B := D\vec{g}(\vec{f}(\vec{p}))$ and $A :=
D\vec{f}(\vec{p})$. We know that $A$ satisfies $\forall
\varepsilon > 0, \exists\+\delta(\varepsilon) > 0, \forall \vec{h}$ with
$||\+\vec{h}\+|| < \delta(\varepsilon)$:

\[
    ||\+\Delta \vec{f}(\vec{p}, \vec{h}) - A \vec{h}\+|| < \varepsilon
    ||\+\vec{h}\+||
\]

Let us call this condition $*_{\vec{f}}$

We also know that $B$ satisfies $\forall \varepsilon' > 0, \exists \+
\delta'(\varepsilon') > 0, \forall \vec{k}$ with
$||\+\vec{k}\+|| < \delta'(\varepsilon')$:

\[
    ||\+\Delta \vec{g}(\vec{q}, \vec{k}) - B \vec{k}\+|| < \varepsilon'
    ||\+\vec{k}\+||
\]

Let us call this condition $*_{\vec{g}}$

We seek to prove the following:

$\forall \varepsilon_2 > 0, \exists\+\delta_2(\varepsilon_2) > 0, \forall
\vec{h}$ with $||\+\vec{h}\+|| < \delta_2(\varepsilon_2)$:

\[
    ||\+(\vec{g} \circ \vec{f})(\vec{p} + \vec{h}) - (\vec{g} \circ
    \vec{f})(\vec{p}) - (BA) \vec{h}\+||
    \overset{?}{<} \varepsilon_2 ||\+\vec{h}\+||
\]

Let's define a vector $\vec{k} := \Delta \vec{f} = \vec{f}(\vec{p} + \vec{h}) -
\vec{f}(\vec{p})$. Note that since $\vec{q} = \vec{f}(\vec{p})$, $\vec{q} +
\vec{k} = \vec{f}(\vec{p} + \vec{h})$. We can therefore rewrite the equation as:

\[
    ||\+\vec{g}(\vec{q} + \vec{k}) - \vec{g}(\vec{q}) - B\vec{k} + B\vec{k} - (BA) \vec{h}\+||
    \overset{?}{<} \varepsilon_2 ||\+\vec{h}\+||
\]

Now we left factor $B$ out of the left hand side:

\[
    ||\+\vec{g}(\vec{q} + \vec{k}) - \vec{g}(\vec{q}) - B\vec{k} + B(\vec{k} - A
    \vec{h})\+|| \overset{?}{<} \varepsilon_2 ||\+\vec{h}\+||
\]

Now we can apply the triangle inequality and the generalized Cauchy-Schwarz
Inequality to get the following:

\[
    ||\+\vec{g}(\vec{q} + \vec{k}) - \vec{g}(\vec{q}) - B\vec{k} + B(\vec{k} - A
    \vec{h})\+|| \leq ||\+\vec{g}(\vec{q} + \vec{k}) - \vec{g}(\vec{q}) -
    B\vec{k}\+|| + ||B||\+||\+\vec{k} - A\vec{h}\+||
\]

But note that the first part of the right hand side is condition $*_{\vec{g}}$,
so we get:

\[
||\+\vec{g}(\vec{q} + \vec{k}) - \vec{g}(\vec{q}) - B\vec{k}\+|| + ||B||\+
||\+\vec{k} - A\vec{h}\+|| < \varepsilon' ||\+\vec{k}\+|| + ||B||\+||\+\vec{k} -
A\vec{h}\+|| = \varepsilon' ||\+\vec{k} - A\vec{h} + A\vec{h}\+|| + ||B||\+
||\+\vec{k} - A\vec{h}\+||
\]

Now we can apply the same trick, except WLOG we assume that $||\+\vec{k}\+|| <
\delta'(\varepsilon')$ and assume $\varepsilon' \leq 1$.  we get:
\[
    \varepsilon' ||\+\vec{k} - A\vec{h} + A\vec{h}\+|| + ||B||\+
    ||\+\vec{k} - A\vec{h}\+|| \leq \varepsilon' ||\+\vec{k} - A\vec{h}\+|| +
    \varepsilon'||A||\+||\+\vec{h}\+|| + ||B||\+||\+\vec{k} - A\vec{h}\+||
\]

Now we factor and apply the bounds on $\varepsilon'$:
\[
    \varepsilon' ||\+\vec{k} - A\vec{h}\+|| +
    \varepsilon'||A||\+||\+\vec{h}\+|| + ||B||\+||\+\vec{k} - A\vec{h}\+|| \leq
    (||B|| + 1) ||\+\vec{k} - A\vec{h}\+|| + ||A||\+||\+\vec{h}\+||\varepsilon'
\]

Note that $||\+\vec{k} - A\vec{h}\+|| = ||\+\vec{f}(\vec{p} + \vec{h}) -
\vec{f}(\vec{p}) - A\vec{h}\+|| < \varepsilon ||\+\vec{h}\+||$. From this we
get:

\[
    (||B|| + 1) ||\+\vec{k} - A\vec{h}\+|| + ||A||\+||\+\vec{h}\+||\varepsilon'
    \leq (||B|| + 1) \varepsilon ||\+\vec{h}\+|| + ||A|| \+ ||\+\vec{h}\+||
    \varepsilon'
\]

If we take $\varepsilon = \varepsilon'$, we get:

\[
    (||B|| + 1) \varepsilon ||\+\vec{h}\+|| + ||A|| \+ ||\+\vec{h}\+||
    \varepsilon' \leq (||A|| + ||B|| + 1) \varepsilon ||\+\vec{h}\+||
\]

Note that $(||A|| + ||B|| + 1) \varepsilon$ can be arbitarily small because it
is a constant times an arbitarily small value. If we call this value
$\varepsilon_2$, we get that

\[
    ||\+(\vec{g} \circ \vec{f})(\vec{p} + \vec{h}) - (\vec{g} \circ
    \vec{f})(\vec{p}) - (BA) \vec{h}\+||
    < \varepsilon_2 ||\+\vec{h}\+||
\]

\subsubsection{Special Cases}

Consider the following ``chain'': $\varmathbb{R}^d_{\vec{x}} \supseteq D
\overset{\vec{f}}{\to} \varmathbb{R}^e_{\vec{y}} \supseteq E \cap \vec{f}(D)
\overset{\vec{g}}{\to} \varmathbb{R}^k_{\vec{z}}$. The special case we're
interested in is if $d = 1$ and $k = 1$. In this case, the original $\vec{x}$-axis can be
thought of as the time axis, and $\vec{f}$ can be thought of as a path
$\vec{\gamma}: I \to \varmathbb{R}^e$ where $D = I$, an interval, $I \subseteq
\varmathbb{R}$. The difference between this ans the general chain rule is that
the domain is connected (because the domain is time). This way, the ``chain''
reduces to: 
$$\varmathbb{R} \supseteq D \overset{(g \circ
\vec{\gamma})}{\longrightarrow} \varmathbb{R}$$

Therefore the derivative is an ordinary single-calculus derivative. However, to
calculate the derivative with the chain rule, we have to do the following;

$$(g \circ \vec{\gamma})'(t) = Dg(\vec{\gamma}(t))D\vec{\gamma}(t)$$

This can be written as:

$$(g \circ \vec{\gamma})'(t) = \gradient g(\vec{\gamma}(t))^T \vec{\gamma}'(t) =
\boxed{\gradient g(\vec{\gamma}(t))\cdot\vec{\gamma}'(t)}$$

Here, $\vec{\gamma}'(t)$ is known as the \textbf{velocity vector} of the curve,
which is the column matrix (vector) of the derivatives of the component
functions of $\vec{\gamma}$ evaluated at $t$. It turns out that this value is
also equal to:

$$\vec{\gamma}'(t) = \lim_{h \to 0} \frac{\vec{\gamma}(t + h) -
\vec{\gamma}(t)}{h}$$

This is because all the operations used here distributes across components. This
is the physics-y interpretation of the derivative. The magnitude of this vector
is the instataneous speed of the curve.

$$ ||\+\vec{\gamma}'(t)\+|| = \left|\left|\+\lim_{h \to 0} \frac{\vec{\gamma}(t + h) -
\vec{\gamma}(t)}{h}\+\right|\right| = \lim_{h \to 0^+} \frac{||\+\vec{\gamma}(t + h) -
\vec{\gamma}(t)\+||}{h}$$

However, this physical interpretation is also the same as the Jacobian
Derivative of $\vec{\gamma}$. But since there is only one input variable, there
is only one partial to compute, $\partial_t \gamma_1$, otherwise known as
$\gamma_1'$. Therefore, the Jacobian Derivative becomes a column vector of the
derivatives of the component functions.
%On a sidenote, 
%
%\[
%    \begin{cases}
%        (a, b), [a, b), (a, b], [a,b], (a, \infty), [a, \infty), (-\infty, b),
%        (-\infty, b], (-\infty, \infty) \text{ where } a < b\\
%        \{a\}, \emptyset
%    \end{cases}
%\]
%
%are exactly the connected subsets of $\varmathbb{R}$.
\subsection{Implicit Function Theorem}
\subsubsection{Theorem}
Let $F: D \subseteq \varmathbb{R}^{d + 1} \to \varmathbb{R}$ be $C^1$ in
$D^\circ$. Suppose $F(\vec{A}) = 0$, where $\vec{A} = (\vec{a}, \alpha)$,
$\vec{a} \in \varmathbb{R}^d, \alpha \in \varmathbb{R}$. Also, suppose
$\partial_{y} F(\vec{A}) \neq 0$\footnote{Here, we write the function $F$ as
$F(\vec{x}, y) = F(x_1, x_2, \dots, x_d, y) = F(\vec{X})$, so we are
differentiating with respect to the last variable.}. Then $\exists$ an open set
$U \subseteq \varmathbb{R}^d$ and an open interval $I \subseteq \varmathbb{R}$
such that $\vec{a} \in U$ and $\alpha \in I$ and $U \times I \subseteq D^\circ$,
and $\exists$ a function $f : U \to I$ such that $\{\vec{X} \in U \times I \+|\+
F(\vec{X}) = 0\} = G_f := \{(\vec{x}, f(\vec{x})) \+|\+ \vec{x} \in U\}$. Also,
$f$ is also $C^1$ and

$$\gradient f(\vec{x}) = - \frac{1}{\partial_y F(\vec{x}, f(\vec{x}))}
\gradient_{\vec{x}} F(\vec{x}, f(\vec{x}))$$

where $\gradient_{\vec{x}} F = (\partial_{x_1} F, \partial_{x_2} F, \dots,
\partial_{x_d} F)$ is called the \textbf{partial gradient} of $F$.

In other words, locally, in some neighborhood of a solution point $\vec{A}$,
$F(\vec{x}, y) \leftrightarrow y = f(\vec{x})$.

\subsubsection{Proof Part I -- Existence of $f$}

Without loss of generality let us assume that $\partial_y F(\vec{A}) > 0$. Since
we know that $F$ is $C^1$, $\partial_y F(\vec{X})$ is continuous. This means
that around the solution point $\vec{A}$, we can find a convex set centered at
$\vec{A}$ such that $\partial_y F > 0$. The projection of $\vec{A}$ onto the
$y$-axis is $\alpha$, and the projection ``down'' onto the $\vec{x}$-space is
$\vec{a} \times {0}$.

Now let us consider a function $g_{\vec{a}}(y) = F(\vec{a}, y)$, or the value of
$F$ as we move $A$ along the $y$-axis. By definition $g'_{\vec{a}}(y) =
\partial_y F(\vec{a}, y)$. Note that this is positive when $y = \alpha$,
therefore the function $g_{\vec{a}}$ is increasing at $\alpha$. Since
$g_{\vec{a}}(\alpha) = 0$, it means that there exists a positive $\delta$ such
that $g_{\vec{a}}(\alpha + \delta) > 0$ and $g_{\vec{a}}(\alpha - \delta) < 0$.

But note that $g_{\vec{a}}(\alpha + \delta) = F(\vec{a}, \alpha + \delta)$, and
since $F$ is $C^1$, $F$ takes on a positive value near $(\vec{a}, \alpha +
\delta)$, specifically, we can draw a square centered at this point with
constant $y$ coordinate with edge length $r^+$. A similar square can be drawn
around $(\vec{a}, \alpha - \delta)$ that have negative $F$ values with edge
length $r^-$. Now we can draw an open square $U$ around $\vec{a}$, with edge
length $r = \min(r^+, r^-)$, and let the interval $(\alpha - \delta, \alpha +
\delta)$ be $I$.

Now pick any point $\vec{X}$ on $U$ and consider the function $g_{\vec{X}} (y) =
F(\vec{X}, y)$. We know that $g'_{\vec{X}}(y) = \partial_y F(\vec{X}, y) > 0$,
so we know that $g_{\vec{X}}$ is strictly increasing over the interval $I$.
Consider $g_{\vec{X}} (\alpha - \delta)$, it is negative by how we picked
$\vec{X}$ and $\delta$. Now consider $g_{\vec{X}} (\alpha + \delta)$. Now by the
Intermediate Value Theorem, $g_{\vec{X}} (c) = 0$ for some unique $c$ between
$\alpha - \delta$ and $\alpha + \delta$. Note that the value of $c$ is uniquely
determined by the location of $X$, therefore we can write it as a function
$f(\vec{X})$. This uniquely defines $f$ because there is exactly one point of
the form $(\vec{x}, y)$ such that $g_{\vec{x}}(y) = 0$, $y \in I$.

Note that we now have the following identity:

$$\boxed{F(\vec{x}, f(\vec{x})) \equiv 0 \text{ for all } \vec{x} \in U}$$

\subsubsection{Proof Part II -- Continuity of $f$}

Now pick an arbitrary vector $h$ such that $\vec{x} + \vec{h} \in U$, by the
above identity, we get that $F(\vec{x} + \vec{h}, f(\vec{x} + \vec{h})) = 0$.
Now we can subtract the two:

$$F(\vec{x} + \vec{h}, f(\vec{x} + \vec{h})) - F(\vec{x}, f(\vec{x})) = 0$$

Now we can apply the Mean Value Theorem, we can rewrite the left hand side as:
$\gradient F(\vec{X}^*) \cdot \vec{H}$ where $\vec{X}^* \in [(\vec{x},
f(\vec{x})), (\vec{x} + \vec{h}, f(\vec{x} + \vec{h}))]$ and $\vec{H} =
(\vec{h}, \Delta f) = (\vec{h}, f(\vec{x} + \vec{h}) - f(\vec{x}))$. We can
rewrite this as

$$\gradient_{\vec{x}} F(\vec{X}^*) \cdot \vec{h} + \partial_y F(\vec{X}^*)
\Delta f = 0$$

$$- \partial_{y} F(\vec{X}^*) \Delta f = \gradient_{\vec{x}} F(\vec{X}^*) \cdot
\vec{h}$$

Now we take absolute value of both sides, and apply the Cauchy-Schwarz
Inequality to the right hand side

\begin{align*}
    |\+-\partial_{y} F(\vec{X}^*) \Delta f\+| &= |\+\gradient_{\vec{x}}
    F(\vec{X}^*) \cdot \vec{h}\+|\\
    &\leq ||\+\gradient_{\vec{x}} F(\vec{X}^*)\+||\+ ||\+\vec{h}\+||\\
    &\leq ||\+\gradient_{\vec{x}} F(\vec{X}^*)\+||\+ ||\+\vec{h}\+||\\
    &\leq M ||\+\vec{h}\+||
\end{align*}

where $M := \max ||\+\gradient F\+||$ over the closure of $U \times I$. Now we
can break the absolute value on the left hand side, and we get

$$\partial_y F(\vec{X}^*) \+ |\Delta f| \leq M \+ ||\+\vec{h}\+||$$

we can drop the absolute value because we picked the convex set such that
$\partial_y F > 0$. Now we can get a bound on $\Delta f$

$$|\Delta f| \leq \frac{M \+ ||\+\vec{h}\+||}{\partial_y F(\vec{X}^*)}$$

Now if we shrink the initial convex set such that $\partial_y F > \frac{1}{2}
\partial_y F(\vec{A})$ (we can do this due to $F$'s continuity), then we can say

$$0 \leq |\Delta f| \leq \frac{M \+ ||\+\vec{h}\+||}{\partial_y F(\vec{X}^*)}
\leq \frac{2M \+ ||\+\vec{h}\+||}{\partial_y F(\vec{A})}$$

Now we take the limit as $\vec{h} \to \vec{0}$, note that then the rightmost upper
bound tends to $0$ as $\vec{h}$ is the only variable present. Therefore $|\Delta
f|$ is squeezed between $0$ and a value that approaches $0$, therefore it is
$0$. Therefore $f$ is continuous.

\subsubsection{Proof Part III -- $C^1$ Nature and Form of $f$}

Let us go back to

$$\gradient_{\vec{x}} F(\vec{X}^*) \cdot \vec{h} + \partial_y F(\vec{X}^*)
\Delta f$$

Now let us take $\vec{h}$ to be of a specific form, let $\vec{h} = h\vec{e}_j$
where $\vec{e}_j$ is the $j^{th}$ elementary component vector. In this form, we
can rewrite the above expression as

$$h\{-\partial_{\vec{x}} F(\vec{X}^*) \cdot \vec{e}_j\} = \partial_y
F(\vec{X}^*) \Delta f$$

$$- \partial_{\vec{x}} F(\vec{X}^*) \cdot \vec{e}_j = \partial_y
F(\vec{X}^*)\left(\frac{\Delta f}{h}\right)$$

Now let us isolate $\left(\frac{\Delta f}{h}\right)$

$$\frac{\Delta f}{h} = - \frac{\gradient_{\vec{x}} F(\vec{X}^*) \cdot
\vec{e}_j}{\partial_y F(\vec{X^*})}$$

Now we can use the $C^1$ property of $F$ and we get

$$\frac{\Delta f}{h} = -\frac{\gradient_{\vec{x}} F(\vec{x},
f(\vec{x}))\cdot \vec{e}_j}{\partial_y F(\vec{x}, f(\vec{x}))}$$

But since when you dot any vector with $\vec{e}_j$, you simply pick out its
$j^{th}$ component, so we get:

$$\frac{\Delta f}{h} = -\frac{\partial_{x_j} F(\vec{x}, f(\vec{x}))}{\partial_y
F(\vec{x}, f(\vec{x}))}$$

Now if we take the limit as $h \to 0$, we get the $j^{th}$ partial derivative of
$f$, which has the form of

$$\partial_{x_j} f(\vec{x}) = - \frac{\partial_{x_j} F(\vec{x}, f(\vec{x}))}{\partial_y
F(\vec{x}, f(\vec{x}))}$$

All the partials exist, but to prove that $f$ is differentiable we also have to
prove they are all continuous. This follows directly from the $C^1$ nature of
$F$ and the fact that compositions of continuous functions is continuous.
Therefore, $f$ is also differentiable and we can write a form for the gradient
of $f$:

$$\boxed{\gradient f = -\frac{1}{\partial_y F \circ (Id \times f)} \gradient_{\vec{x}}
F \circ (Id \times f)}$$

(where $Id$ is the identity function, so $(Id \times f)(\vec{x}) = (\vec{x},
f(\vec{x}))$)

\subsubsection{Generalization}

$F(\vec{x}, y) = 0$ is equivalent to $y = f(\vec{x})$ locally in a neighborhood
of some solution point, $(\vec{a}, \alpha)$ if $F$ is $C^1$, $F(\vec{a}, \alpha) = 0$,
$(\vec{a}, \alpha) \in D_F^\circ \subseteq \varmathbb{R}^{d + 1}$ and
$\partial_y F(\vec{a}, \alpha)$

However, if $\vec{F}$ is a vector-valued function, we also have this type of
equivalency ($\vec{F}(\vec{x}, \vec{y}) = \vec{0}$ is equivalent to $\vec{y} =
\vec{f}(\vec{x})$ for $\vec{x} \in U \subseteq \varmathbb{R}^d$ and $\vec{y} \in
V \subseteq \varmathbb{R}^e$) locally around some solution point $\vec{F}(\vec{a}, \vec{b})
= \vec{0} \in \varmathbb{R}^e$ if $\vec{F}$ is $C^1$ on $D_{\vec{F}}^\circ$\footnote{A vector valued
function is $C^1$ if all of its component functions are $C^1$} and $(\vec{a},
\vec{b}) \in D_{\vec{F}}^\circ \subseteq \varmathbb{R}^{d + e}$ and
$\partial_{\vec{y}} \vec{F}(\vec{a}, \vec{b}) \neq 0$ where $\partial_{\vec{y}}
\vec{F} = \det D_{\vec{y}} \vec{F}$ where $D_{\vec{y}}$ (known as the
\textbf{partial Jacobian}) can be described as:

$$D_{\vec{y}} F = \left[\frac{\partial F_i}{\partial y_j}\right]_{\substack{1 \leq i \leq
e \\ 1 \leq j \leq e}}$$

Similarly, we know that if all the conditions are met, we know that $\vec{f}$ is
$C^1$. We also have a semi-explicit formula for the Jacobian of $f$:

$$\boxed{D\vec{f} = -(D_{\vec{y}} \vec{F})^{-1} D_{\vec{x}} \vec{F}}$$

By
definition we know $\vec{F}(\vec{x}, \vec{f}(\vec{x})) \equiv \vec{0}$ for all
$\vec{x} \in U$. Now we can differentiate:

$$D_{\vec{x}} \vec{F}(\vec{x}, \vec{f}(\vec{x})) \equiv D_{\vec{x}} \+ \vec{0} = O
\in \varmathbb{R}^{e \times d}$$

But we can rewrite the left hand side as $D_{\vec{x}} \+ [\vec{F} \circ (Id
\times \vec{f})]$, here we can apply the chain rule:

$$(D_{\vec{x}} \vec{F})(\vec{x}, \vec{f}(\vec{x})) \+ (D_{\vec{x}}(Id \times
\vec{f})) (\vec{x}) \equiv 0$$

We can express $(Id \times \vec{f})(\vec{x})$ as $(\vec{x}, \vec{f}(\vec{x}))$,
which can be written as

$$\left[\begin{array}{c}
        x_1 \\
        \vdots \\
        x_d \\
        f_1 (\vec{x})\\
        \vdots \\
        f_e (\vec{x})
\end{array}\right] =
\left[\begin{array}{c}
        \vec{x}\\
        \vec{f}(\vec{x})
\end{array}\right]$$

Since the Jacobian Operator works row by row, we can distribute it

$$D_{\vec{x}} \left[\begin{array}{c}
        \vec{x}\\
        \vec{f}(\vec{x})
\end{array}\right] =
\left[\begin{array}{c}
        D_{\vec{x}} \vec{x} \\
        D_{\vec{x}} \vec{f}(\vec{x})
\end{array}\right] =
\left[\begin{array}{c}
        I_d \\
        D \vec{f}(\vec{x})
\end{array}\right]$$
%}}}
\end{document}

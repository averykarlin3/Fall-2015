\documentclass[11 pt, twoside]{article}
\usepackage{textcomp}
\usepackage[margin=1in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{color}
\usepackage{setspace}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

\begin{document}

\title{Multivariable Calculus}
\author{Avery Karlin}
\date{Fall 2015}

\maketitle
\newpage
\tableofcontents
\vspace{11pt}
\noindent
\underline{Teacher}: Stern
\newpage

\input{unit1.tex}

\section{Higher Dimensional Mathematics}

\subsection{Higher Dimensions}
\subsubsection{Cartesian Product}
The Cartesian Plane represents the set $\mathbb{R}^2 := \{(x, y)+|+x, y\in \mathbb(R)\}$. This is known as the \textbf{Cartesian Product} of $\mathbb{R}$ with
itself. The Cartesian Product of two sets $\mathcal{S}$ and $\mathcal{T}$,
$\mathcal{S} \times \mathcal{T} :=  \{(s, t)+|+s\in\mathcal{S},
t\in\mathcal{T}\}$.

\subsubsection{Shapes}
\underline{Open-ball}:
$B_r(P) := \{X+|+dist(X, P) < r\}$
\underline{Closed-ball}:
$\bar{B}_r := \{X+|+ dist(X, P) \leq r\}$
\underline{Sphere}:
$S_r := \{X+|+dist(X, P) = r\}$

\subsubsection{Boundary}
Given $D \subseteq \mathbb{R}^2, D \neq \emptyset$. We say $(a, b) \in
\partial D$, i.e. $(a, b)$ is on a \underline{boundry point} of $D$ if $\forall
\epsilon > 0$, there are points $(x, y)\in D$ and $(u, v) \in D^c$ ($D^c :=
\mathbb{R}^2 \setminus D$) such that $dist(x, y+;+ a, b) < \epsilon$ and
$dist(u, v+;+ a, b) < \epsilon$. Since the definition is symmetrical, $\partial D^c = \partial D$.

\subsubsection{Interior}
Let $D \subseteq \mathbb{R}^2$. We say $(a, b)$ is an
\underline{interior point} of $D$ if $\exists+ r > 0:B_r(a, b)\subseteq D$.
The set of all interior points is called the \underline{interior} of $D$ and is
written as $\text{int } D$.

\subsubsection{Exterior}
Let $D \subseteq \mathbb{R}^2$. We say $(a, b)$ is an
\underline{exterior point} for $D$ if it is an interior point of $D^c$.
$\exists+ r > 0: B_r(a, b) \subseteq D^c$.
The set of all exterior points for $D$ is the \underline{exterior} of $D$,
written as $\text{ext } D$.
\textbf{Thm}: For any $D \subseteq \mathbb{R}^2$, $\mathbb{R}^2 =
\text{int } D \cup \partial D \cup \text{ext } D$. And $\text{int } D \cap
\partial D = \emptyset$, $\text{int } D \cap \text{ext } D = \emptyset$,
$\partial D \cap \text{ext } D = \emptyset$.
\textbf{Thm}: $\text{int } D = \text{ext } D^c$ and $\text{ext } D =
\text{int } D^c$
\textbf{Thm}: $\text{ext } D \subseteq D^c$

\subsubsection{Closure}
The \textbf{closure} of $D \subseteq \mathbb{R}^2$:
$$\bar{D} := D \cup \partial D$$
\textbf{Thm}: $\partial \bar{D} = \partial D$, $\text{int } \bar{D} =
\text{int } D$, and $\text{ext } \bar{D} = \text{ext } D$
\textbf{Thm}: $\text{int } D \subseteq D \subseteq \bar{D}$.

\subsubsection{Ordered Pair}
The ordered pair $(a, b)$ can be thought of as a set, but a set is inheritly
unordered. To express the order, we can do the following: $(a, b) = \{\{a\},
\{a, b\}\}$. Now we know that $a$ is the first element because it appears in
both subsets.
We can then expand this into higher dimensions like the following:
$(a, b, c) = ((a, b), c)$. Note that this means that $((a, b), c) \neq (a, (b,
c))$. But this does not matter to us.

\textbf{Fundamental Postulate of Ordered Pairs}:
$(a_1, a_2, a_3, \dots, a_n) = (b_1, b_2, b_3, \dots, b_n)$ if and only if $a_1
= b_1 \wedge a_2 = b_2 \wedge \dots \wedge a_n = b_n$.

\subsubsection{Vector and Points}
Vectors are quantities of directionality and length, its location does not
matter. Points are just positions in space. In higher dimensions with no
ambiance space (flat space surrounding the surface, i.e. the shortest distance
in the ambiant space is the straight line),  we define a vector as all
the lines with the same direction at a certain point.

However, the nice thing about $\mathbb{R}^d$ is that there is always ambiance
space, so we will not make any notational distinction between a point and a
vector.

The length of a vector in $d$ space is defined as:
$$||\vec{a}|| := dist(\vec{0}, \vec{a}) = \sqrt{\sum_{i =
1}^d a_i^2}$$

\subsection{Dot Product}
\subsubsection{Definition}
The dot product arises naturally through the idea of geometric distance, such that if $a \neq \emptyset, b \neq \emptyset$, then $a \perp b iff dist(a; b)^2 = dist(\emptyset, a)^2 + dist(\emptyset, b)^2$, where $a = (a_1, a_2), b = (b_1, b_2)$. Thus, by expanding out, $a \perp b iff a_1b_1 + a_2b_2 = 0$ where $a \neq \emptyset, b \neq \emptyset$. In addition, orthoganal refers to both perpendicular vectors and where $a = \emptyset$ and/or $b = \emptyset$, so that no vector can be perpendicular to itself.

By extension, in $\mathbb{R^d}$, $a \dot b = a_1b_1 + a_2b_2 + a_3b_3 +...+ a_db_d$, such that it forms a scalar, rather than a vector.

\subsubsection{Properties}
The dot product is:
\begin{itemize}
\item Commutative
\item Distributive over Vector Sums
\end{itemize}

\subsection{Functions in Higher Dimensions}
The domain is a subset of $\mathbb{R^n}$.
Let the function of f(x, y) be an ordered pair within some curve, such that $(x, y) \in D$. Thus, the range of f, G = ${x, y, f(x,y) | (x, y) \in D} \subseteq{R^{n+1}}$.
Functions are defined as one-to-one if for f(x, y), $(x, y), (u, v) \in D, f(x, y) = f(u, v)$ iff (if and only if) x = u $\wedge$ (and) y = v (such that for every z value, there is only one point that will create it.

\subsubsection{Bolzano-Weirstrauss in Higher Dimensions}
\underline{Theorem:} A bounded sequence $\in \mathbb{R^d}$ has a convergent subsequence.

\underline{Lemma:} If $(x_n)_{n=1}^\infty$ converges to $x \in \mathbb{R}$, then every subsequence $(x_{n_k})_{k=1}^\infty$ also converges to x, following from the definition of convergence and limits ($\forall \epsilon > 0, \exists N, \forall n \geq N: |x_n - x| < \epsilon,$ thus $\exists K, \forall k \geq K: n_k \geq N$, since $n_k \to \infty$ as $k \to \infty$, such that $|x_{n_k} - x| < \epsilon$).

\underline{Proof for $d = 2$:} Let $P_n = (x_n, y_n)$. Consider $(x_n)_{n=1}^\infty \in \mathbb{R}$. Since $\forall x_n, -M \leq x_n \leq M, (x_n)_{n=1}^\infty$ is bounded. For some $(x_{n_k})_{k=1}^\infty$, converging to some $x \in \mathbb{R}$. Consider $(y_{n_k})_{k=1}^\infty$ is bounded by the same rationale, thus $(y_{n_{k_j}})^\infty_{j=1}$ converges to some $y \in \mathbb{R}$. Since $(x_{n_{k_j}})_{j=1}^\infty$ is a subsequence of a converging sequence, it converges to the same value, x. Thus, $P_{n_{k_j}} = (x_{n_{k_j}}, y_{n_{k_j}}) \to (x, y) = P$.

\subsubsection{Cauchy Sequence}
A cauchy sequence in $\mathbb{R}$ is a sequence $(x_n)^\infty_{n=1}$ such that: $\forall \epsilon > 0, \exists N, \forall n, m \geq N: |x_n - x_m| < \epsilon$. This defines a sequence where as $n \to \infty$, the distance between values of points on the sequence decreases.

\subsubsection{Convergence $\to$ Cauchy}
Note that any convergent sequence is cauchy, because as terms get together
to a limit, they also go very closely together.

\textbf{``Formal'' Proof}:

Let $(x_n)_{n = 1}^\infty$ be convergent, with limit $x \in \mathbb{R}$.
Then, by definition, $\forall \epsilon > 0, \exists+ N_\epsilon, \forall
n \geq N_\epsilon :
|x_n - x| < \epsilon$. Note that we can replace $\epsilon$ with
$\frac{\epsilon}{2}$, all we have to change is the cutoff point from
$N_{\epsilon}$ to $N_{\frac{\epsilon}{2}}$. Now if we take two subscripts
$n, m \geq N_{\frac{\epsilon}{2}} \longrightarrow |+x_n - x_m+| = |+(x_n -
x) + (x - x_m)+| \leq |+x_n - x+| + |+x_m - x+|$ because of the Triangle
Inequality for Absolute Values. However, note that $|+x_n - x+| \leq
\frac{\epsilon}{2}$ and $|+x_m - x+| \leq \frac{\epsilon}{2}$.
Therefore, $|+x_n - x_m+| \leq |+x_n - x+| + |+x_m - x+| \leq \epsilon
+\blacksquare$

\subsubsection{Cauchy's Convergence Theorem}

In $\mathbb{R}$, every cauchy sequence converges to a limit in $\mathbb{R}$.

\vspace{0.3cm}

\textbf{Lemma \#1}: Every cauchy sequence is bounded.

Let us take $\epsilon = 1$, then the definition of ``cauchiness'' becomes:
$$\exists+ N_1, \forall n, m \geq N_1 : |+x_n - x_m+| < 1$$

Let $M := \max\{|x_1|, |x_2|, \dots, |x_{N_1 - 1}|, |x_{N_1}| + 1\}$. We claim
that $|x_n| \leq M$, for all $n \geq 1$. This is true because when $n \in \{1, 2, \dots, N_1
- 1\}$, the statement is true by definition of $M$. When $n \geq N_1$, we know
that $|x_n| \leq |x_{N_1}| + 1 \leq M$ because we can let $m = N_1$, then by the
definition of ``cauchiness,'' we know that $|x_n - x_{N_1}| < 1$.

Now we see that $M$ is a bound on the sequence for all $n \geq 1$. Therefore the
sequence is bounded.

\textbf{Lemma \#2}: If a subsequence of a cauchy sequence converges to $x \in \mathbb{R}$, the
whole sequence must converge to $x$.

Say $(x_n)_{n=1}^\infty$ is cauchy, and $(x_{n_k})_{k = 1}^\infty$ converges to
$x$. For any arbitrary $\epsilon > 0$, we try to find $N$ such that $\forall
n \geq N: |x_n - x| < \epsilon$. If we prove the existence of $N$ for all
$\epsilon$, we will have proven that the original sequence converges.

We know that $\forall \epsilon > 0, \exists + K_\epsilon, \forall k \geq
K_\epsilon: |x_{n_k} - x| < \epsilon$. We add and subtract $x_{n_k}$ and
group terms, and use the Triangle Inequality: $|x_n - x| = |(x_n - x_{n_k}) + (x_{n_k} - x)| \leq
|x_n - x_{n_k}| + |x_{n_k} - x|$. Note that $|x_{n_k} - x| < \epsilon$
provided $k \geq K_{\epsilon}$ from the convergent subsequence condition. We
also know that $|x_n - x_{n_k}| < \epsilon$ provided that $n, n_k \geq
N_{\epsilon}$, which we call the ``cauchy cutoff.'' This is true from the
``cauchiness'' condition.

We know that $k\to\infty$ implies $n_k \to \infty$. This means eventually
$n_k > N_\epsilon$ provided that $k > L_{N_\epsilon}$. Now let $k =
\max\{L_{N_\epsilon}, K_\epsilon\}$ and $n \geq N_\epsilon$, which
implies $|x_n - x_{n_k}| < \epsilon$ and $|x_{n_k} - x| < \epsilon$.
Now we know: $|x_n - x| \leq 2\epsilon$ provided $n \geq N_{\epsilon}$.
Therefore the cauchy sequence converges to $x$.

\vspace{0.3cm}

With these two lemmas, the theorem becomes very easy to prove:

Because of Lemma \#1 and the Bolzano-Weierstrass Theorem, we know that for all
cauchy sequences, there is a bounded subsequence that converges to some value
$x$. Then by Lemma \#2, we know that the entire cauchy sequence converges to $x$
as well, therefore the sequence converges. 

\subsubsection{Cauchy Sequences in Higher Dimensions}

$(P_n)_{n=1}^\infty$ is cauchy if $\forall \epsilon > 0, \exists +
N_\epsilon, \forall n, m \geq N_\epsilon: dist(P_n, P_m) < \epsilon$.
This is easy to prove due to the coordinate nature of $\mathbb{R}^d$.


\subsection{Distance Functions}
In axiomatic geometry, certain axioms including the definition of euclidean distance are taken as assumed. In actuality, standard distance functions must qualify under several non-geometric requirements, of which only the Euclidean distance qualifies.

Distance functions must be \underline{translation-iniant}, or for any translation of two points, the distance must remain the same, such that $T_{h, k}: (x, y) \mapsto (maps to) (x+h, y+k), then dist(x+h, y+k; x'+h, y'+k) = dist(x, y; x', y')$. 

Thus, $dist(x,y; \tilde(x), \tilde(y)) = f(|x-\tilde(x)|, |y-\tilde(y)|)$, where f the distance function defined on $[0, \infty) x [0, \infty).$ As a result, it must be \underline{symmetrical}, such that $dist(x, y; \tilde(x), \tilde(y)) = dist(\tilde(x), \tilde(y); x, y)$.

In addition, it must have \underline{basic reflection symmetry (isotropy)}, such that $dist(x, y; 0, 0) = dist(y, x; 0, 0)$. Thus, f(u, v) = f(v, u) for any $u \geq 0, v \geq 0$. It must also have the \underline{self-distance of (0, 0)}, such that dist(0,0; 0,0) = 0.

It must \underline{recreate the standard distance function on each axis}, such that $dist(x,0; \tilde(x), 0) = |x - \tilde(x)|, dist(0, y; 0, \tilde(y)) = |y - \tilde(y)|. Therefore, f(u, 0) = u, f(0, v) = v \forall u \geq 0, v \geq 0$.

As a result, it must have \underline{asymptotic flatness}, where if a line is drawn to (x, y), where y is fixed, such that $dist(0,0; 0, y) = v_0, while dist(0, 0; x, 0) = u. Then, \lim_{u \to \infty} f(u, v_0)/u = 1$. This also applies in the opposite direction, where x is fixed.

It must be \underline{continuous} in its variables, such that with a minute movement of a point, the distance changes minutely as well.

\underline{The set of isometries} (distance preserving one-to-one functions) that fix the origin onto itself (f(0) = 0) is an infinite set.

Based on these requirements, an ansatz (educated guess, verified by later results) is made, such that $f(u, v) = F(G(u) + G(v)), where F: [0, \infty) \to \mathbb{R} and G: [0, \infty) \to \mathbb{R}$. The use of G(u) and G(v) is needed to assure symmetry. The use of addition is mandated by symmetry, using addition rather than another symmetrical operation simply due to ease of calculations.

\underline{Theorem:} $\exists only one suitable pair F, G; G(x) = x^2, F(x) = \sqrt{x}, that fits all requirements. If G(x) = x^n, F(x) = \sqrt[n]{x}$, it would have all required properties except infinite set of isometries.

\underline{Property:} If $dist(p; q) = 0$, then $p = q$, where p and q are asome vector $\in \mathbb{R}$

\subsubsection{Euclidean Distance}
The distance function in one space between two points $a$ and $b$ is simply $|a
- b|$. However, we can also write it in the following way: $\sqrt{(a - b)^2}$

In $\mathbb{R}^2$, the distance function is:
$$dist(x, y++;+a,b) := \sqrt{(x - a)^2 + (y - b)^2}$$

And in $\mathbb{R}^3$, the distance function is:
$$dist(x, y, z+;+ a,b,c) := \sqrt{(x - a)^2 + (y - b)^2 + (c - z)^2}$$

The generalized form of Euclidean Distance in $N$ space is:
$$dist(\vec{p}, \vec{q}) = \sqrt{\sum_{j = 1}^N (p_j - q_j)^2}$$

This is known as the \underline{Euclidean Distance}. We use this specific
definition of distance because this is preserved under an infinite set of rigid
or isometric motions, such as rotation, reflection, translation, etc.

\subsubsection{Geometric Distance}
\underline{Please add here}

\textbf{Basic Transformations}:
\begin{itemize}
\item $T_h : x \mapsto x+h$
\item $R: x \mapsto -x$
\end{itemize}

\textbf{Properties}:
\begin{enumerate}
\item $dist(\vec{p}, \vec{q}) = dist(\vec{q}, \vec{p})$
\item $dist(\vec{p}, \vec{q}) \geq 0$
\item $dist(\vec{p}, \vec{q}) = 0 \leftrightarrow \vec{p} = \vec{q}$
\end{enumerate}

\subsubsection{Basic Distance Bounds Lemma}
$\forall \vec{p}, \vec{q} \in
\mathbb{R}^d$, and $\forall j \in \{1,2,3,\dots,d\}$:
$$|p_j - q_j| \leq dist(\vec{p}, \vec{q}) \leq \sqrt{d} \max_{1 \leq k \leq
d} |p_k - q_k|$$

\textbf{Proof}
Note that $(p_j - q_j)^2 \leq \sum_{k = 1}^d (p_k - q_k)^2$ is trivial, because
you can only add positive number when you add squares. Now let's take the square
root, and we get
$$\sqrt{(p_j - q_j)^2} = |p_j - q_j| \leq \sqrt{\sum_{k = 1}^d (p_k - q_k)^2} =
dist(\vec{p}, \vec{q})$$

To prove the other inequality, it is trivial as well. We can just factor out the
length of the vector $d$ and multiply that with the maximum value of the
distance vector. Then we get:

$$dist(\vec{p}, \vec{q}) = \sqrt{\sum_{k = 1}^d (p_k - q_k)^2} \leq \sqrt{d \max_{1 \leq k \leq d} (p_k -
q_k)^2} = \sqrt{d} \max_{1 \leq k \leq
d} |p_k - q_k|$$

\textbf{Cor}: Componentwise Nature of Convergence

Let $(\vec{p}_n)_{n = 1}^\infty$ be a sequence in $\mathbb{R}^d$, and let
$\vec{p} \in \mathbb{R}^d$. Then $\vec{p}_n \to \vec{p}$ if and only if
$p_{n|j} \to p_j$ ($\vec{p} = (p_1, p_2, p_3, \dots, p_d)$ and $\vec{p}_n =
(p_{n|1}, p_{n|2}, \dots, p_{n|d})$). Otherwise known as convergence of points
can be reduced to conversion of dimensions.

This follows directly from the inequality, because if the total distance goes to
0, then $|p_j - q_j|$ goes to 0. Therefore if the points converge, the
corresponding coordinates must converge.

To prove the converse, we prove using the other side of the distance bounds. If
all $d$ coordinates are going to 0, then if we take the maximum, that would be
going to 0. (the maximum of a sequence is less than the sum of the sequence, but
if every term of the sum is going to 0, then the sum is going to 0, then the
maximum is going to 0). Therefore the distance must also be going to 0. Thus the two points converges.

\subsubsection{Triangle Inequality}
$$dist(\vec{p}, \vec{q}) + dist(\vec{q}, \vec{r}) \geq dist(\vec{p}, \vec{r})$$

This can be generalized by mathematical induction to $dist(\vec{p_0}, \vec{q_n})
\leq \sum_{j = 1}^n dist(\vec{p}_{j - 1}, \vec{p}_{j})$ (Otherwise known that the
shortest distance between two points is the straight line, or the \textbf{Generalized
Triangle Inequality} or the ``Broken Line Inequality'')

\subsubsection{Other Distance}
Of course, there are other distance formulas, like the \underline{Minkowski Distance}
$$((x - a)^p + (y - b)^p)^{\frac{1}{p}} ++++ (p > 1)$$
This is another distance formula, but under this, only reflection preserves
distance.

\subsection{Metric Topology in $\mathbb{R^n}$}
\subsubsection{Continuity}
Let $f: D\to\mathbb{R}$, $D\subseteq\mathbb{R}^2$, $D\neq\emptyset$. Let $(a, b)\in D$. We say that $f$ is \underline{continuous} at $(a, b)$ if:
$$f(a, b) = \lim_{\substack{(x, y)\to(a,b)\\(x, y)\in D}} f(x, y)$$
Or in other terms:
$$\forall \epsilon > 0, \exists+ \delta>0, \forall (x, y)\in D: dist(x,
y+;+a,b) < \delta \to |f(x,y) - f(a,b)| < \epsilon$$

$f:D \to \mathbb{R}, D \subseteq \mathbb{R^2}, D \neq \emptyset is continuous if f is continuous at (a, b) for all (a, b) \in D.$

\subsubsection{Directional Limits}
Let $D \subseteq \mathbb{R} and a \in D \cup \partial D$ (the boundry, both already included in D, and not), $\lim_{x \to a^+} f(x) = L$ means $\forall \epsilon > 0, \exists \delta(\epsilon) > 0: \forall x \in D \cap (a, \infty), |x-a| < \delta(\epsilon) \Rightarrow |f(x) - L| < \epsilon$. The limit only exists if both directions equal the same value.

\subsubsection{Limits in Higher Dimensions}
The same theory can be applied to higher dimensions, such that if two arbitrary approaches are not the same, it doesn't exist, but if several approaches yield the same result, the definition of a limit is used. Polar coordinate substitutions can be used to give format to directions of approach.

Due to difficulty defining approaching through lines, it is said that $(x, y) \to (a, b) iff dist(x, y: a, b) \to 0$.

Let $f: D \to \mathbb{R}, D \subseteq \mathbb{R^2}, D \neq \emptyset, and (a, b) \in D \cup \delta D.$ Then, $L = \lim_{(x, y) \in D \to (a,b)} f(x) iff \forall \epsilon > 0, \exists \delta > 0, \forall (x, y) \in D: dist(x, y: a, b) < \delta \to |f(x, y) - L| < \epsilon$. As a corollary, when (a, b) is on the boundry, the approach can only be from the domain.

\subsection{Properties of Domain}
For the extreme value theorem to apply to a domain, the set must be compact, such that it must be bounded and closed over limits. On the $\mathbb{R}$ dimension, this applies to all closed intervals, as well as the empty set, though functions except the empty set cannot accept it as a domain.

\subsubsection{Bounded}
$If D \subseteq \mathbb(R^2)$ is bounded if $\exists M > 0: D \subseteq [-M, M] x [-M, M]$. Thus, a sequence is considered bounded if the set of all values within the sequence is bounded.

\subsubsection{Closed}
The term closed is used to apply to sets which are closed under limits. On $mathbb{R}$, if $x_n \in [a, b] for \forall n \in mathbb{N}, and x_n \to x \in \mathbb{R}, then x \in [a, b]$.

$D \subseteq \mathbb{R^2} is closed if for any points (x_n, y_n) \in D (for all n \in \mathbb{N}, if (x_n, y_n) \to (x, y) \in \mathbb{R^2}, then (x, y) \in D. (x_n, y_n) \to (a, b) as n \to \infty$ means $d_n = \sqrt[2]{(x_n - a)^2 + (y_n - b)^2} \to 0 as n \to \infty.$

This applies the definition of limits to sequences, such that $(x_n, y_n) \to (a, b) if dist(x_n, y_n: a, b) \to 0 as n \to \infty$.

Thus, $D \subseteq \mathbb{R^2}$ is closed if for any sequence $((x_n, y_n))^\infty_{n=1}$ in D that converges, the limit poiint (a, b) of the sequence also lies in D.

\subsubsection{Open Set Theorem}
$D \subseteq \mathbb{R^2}$ is open if $\forall (a, b) \in D, \exists r > 0:$ the disk of radius r, $B_r(a, b) \subseteq D$, such that D = Interior of D

It follows that for any open set, the complement set within the space is a closed set.

\underline{Proof}:
Assume $D$ is open, we prove $D^c$ is closed. Choose any convergent sequence
$((x_n, y_n))_{n = 1}^\infty$, converging to $(a, b)$, where $(x_n, y_n) \in
D^c$ for all $n \geq 1$.

We prove this by contradiction. Assume that $(a, b) \in D$. Since $D$ is open,
$\exists+ r > 0:B_r(a, b)\subseteq D$. $\exists+ N, \forall n \geq
\mathbb{N}, (x_n, y_n) \in B_r(a, b)$ since $(x_n, y_n)\to(a,b)$. But we
assumed that $(x_n, y_n) \in D^c$, and $(x_n, y_n) \in D$. But $D \cap D^c =
\emptyset$. Therefore $D^c$ is closed under taking limits.

For the other direction, we can pick some point within D, then assume there is no ball, such that any ball contains some ball not in D, even as radius $\to 0$, creating a sequence of points converging on the point, P, a contradiction.

\subsubsection{Extreme-Value Theorem}
Let $f:D \to \mathbb{R}$ be continuous, where D $\subseteq \mathbb{R^d}$ is compact. Then $\exists P, Q \in D$, which do not need to be unique, such that $\forall X \in D: f(P) \leq f(x) \leq f(Q)$.

\underline{Proof}:
Assume that f is not bounded above, such that $\forall n \geq 1, f(P_n) > n$, where $P_n \in D$. For some subsequence $P_{n_k} \in D$ converging to P by the Balzano-Weirstrauss, by closure of D, $P \in D$. This is a contradiction since $f(P_{n_k}) \to \infty$ and $\to f(P)$, such that it must be bounded from above.

Thus, $\exists M = sup_{x \in D}f(x)$. We can find $P_n \in D$, with $f(P_n) \to M$. For some convergent subsequence $P_{n_k} \in D, P_{n_k} \to Q$.

\section{Inequalities}
\subsection{Level of Operations}
Powers/root $\to$ Multiplation/division $\to$ addition/subtraction $\to$
succession/pretrition
\subsection{AM-GM}

$$\mu = [x_1, x_2, \dots, x_n] \text{ and } x_1, x_2, x_3, \dots, x_n \geq 0$$
``Multiset'' $\mu = \{(x, n), (y, m), \dots\}$

We define the arithmetic mean of a multiset as:
$$A(\mu) = \frac{x_1 + x_2 + \dots + x_n}{n}$$

And the geometric mean as:
$$G(\mu) = \sqrt[\uproot{2}n]{x_1x_2x_3\dots x_n}$$

\subsubsection{AM-GM Inequality}
$A(\mu) \geq G(\mu)$, with equality iff all elements
of $\mu$ are the same.

\subsubsection{Proof}
This is done by mathematical induction. Base case is $n = 2$, then $\mu = [x,
y]$. Then $A(\mu) = \frac{x + y}{2}, G(\mu) = \sqrt{xy}$

We know by the trivial inequality that $(\sqrt{x} + \sqrt{y})^2 \geq 0$, with
equality case happening iff $x = y$. Then we get:
\begin{align*}
    x - 2 \sqrt{xy} + y &\geq 0\\
        \frac{x + y}{2} &\geq \sqrt{xy}\\
                 A(\mu) &\geq G(\mu)
\end{align*}

Now we induce on $n$, we seek to prove that case $n$ implies case $2n$.

$\mu = [x_1, x_2, \dots, x_n, y_1, y_2, \dots, y_n]$

Then we know that
\begin{align*}
    A(\mu) &= \frac{A(\mu_x) + A(\mu_y)}{2}\\
           &\geq \frac{G(\mu_x) + G(\mu_y)}{2}\\
           &\geq \sqrt{G(\mu_x)G(\mu_y)}\\
           &= G(\mu)
\end{align*}

Note that in all inequalities used, the equality case is always when all $x_n$
and $y_n$ are the same element, therefore the equality case holds in all cases
where the length of the list is $2^n$.

Now we prove that case $n$ implies $n-1$

$\mu = [x_1, x_2, x_3, \dots, x_{n - 1}]$

Note that we can construct $\mu' = [x_1, x_2, x_3, \dots, x_{n - 1}, A(\mu)]$

Note that $A(\mu') = A(\mu)$, and since the AM-GM inequality is true for $\mu'$
by the assumption, we know

\begin{align*}
    A(\mu') = A(\mu) &\geq \sqrt[\uproot{2} n]{x_1x_2x_3\dots x_{n - 1}A(\mu)}\\
                     &\geq \sqrt[\uproot{2} n]{G(\mu)^{n-1}A(\mu)}\\
                     &\geq \sqrt[\uproot{2} n]{G(\mu)^{n-1}G(\mu)}\\
                     &\geq G(\mu)
\end{align*}

\subsection{Young's Inequality}
\subsubsection{H\"{o}lder Conjugate}
$q$ is said to be the H\"{o}lder Conjugate of $p$:
$$q := p^* := \frac{p}{p - 1}$$

Note that $q > 1$ and $\frac{1}{p} + \frac{1}{q} = 1$

\subsubsection{Young's Inequality}
$a, b \geq 0$; $p > 1$; $q = p^*$, then Young's Inequality states that:
$$ab \leq \frac{a^p}{p} + \frac{b^q}{q}$$
With equality case iff $a^p = b^q$

\subsubsection{Proof}
We first proof Young's Inequality assuming that $p, q \in \mathbb{Q}$.
We can rewrite $p = \frac{n + m}{n}$ and $q = \frac{n + m}{m}$ for some $n, m
\in \mathbb{N}$. Now Young's Inequality turns into:

$$ab \leq \frac{na^{\frac{n + m}{n}}}{n + m} + \frac{mb^{\frac{m + n}{m}}}{n + m}$$

If we let $x = a^{\frac{1}{n}}$ and $y = b^{\frac{1}{m}}$. Then the inequality
turns into:

$$ab = x^ny^m = \leq \frac{nx^{n + m} + my^{n + m}}{n + m}$$

And that is true by weighted AM-GM, with equality iff $x = y$, which equals to
$a^{\frac{1}{n}} = b^{\frac{1}{m}}$, which equals to 

We can prove the inequality for irrational by taking limits, because
$\forall n \geq n_0: f(n) \leq g(n)$, and the limits of both $f(x)$ and $g(x)$ as
$n\to\infty$ exists and are finite, then we know that $\lim_{n\to\infty}f(n)
\leq \lim_{n\to\infty} g(n)$. When $p$ and $q$ are irrational, we construct
$\{p_n\}$ and $\{q_n\}$ as two sequences of rationals that approaches $p$ and
$q$, the left hand side of Young's Inequality is unaffected by the limit, and by
what we've just said about limits, we know that:
$$ab \leq \lim_{n\to\infty} \frac{a^{p_n}}{p_n} + \frac{b^{q_n}}{q_n} =
\frac{a^p}{p} + \frac{b^q}{q}$$

\subsection{H\"{o}lder's Inequality}:
\subsubsection{$p$-norm}
In $\mathbb{R}^2$: let $||(a, b)||_p = (|a^p| + |b^p|)^{1/p}$ for any $p \in
\mathbb{R} > 1$, this is known as the $p$-norm of a vector. Note that $||(a,
b)||_2 = ||(a, b)|| = \sqrt{a^2 + b^2}$

\subsubsection{H\"{o}lder's Inequality}
$\forall (a, b), (c, d) \in \mathbb{R^2}, p > 1, q = p^* = \frac{p}{p-1}$:
$$ 0 \leq |ac| + |bd| \leq ||(a, b)||_p||(c, d)||_q$$

Equality happens iff $(\frac{a}{s})^p = (\frac{c}{t})^q$ and $(\frac{b}{s})^p =
(\frac{d}{t})^q$ where $s = ||(a, b)||_p$ and $t = ||(c, d)||_q$, such that $||(a, b)||_p||(c, d)||_q = (|a|^p + |b|^p)^{1/p}(|c|^q + |d|^q)^{1/q}$

\subsection{Cauchy-Schwarz Inequality}
This is a special case of H\"{o}lder's Inequality, where $p = q = 2$. (This is very important, 2 is the \textit{only} value that is its own conjugate, this is why Euclidean distance is so special)

If we plug in 2 for $p$ and $q$ and use the Triangle Inequality:
$|ac + bd| \leq |ac| + |bd| \leq \sqrt{a^2 + b^2} \sqrt{c^2 + d^2}$, or $|v \dot u| \leq ||u||||v||$ with equality iff $ab \geq 0$.

Then, by Young's inequality, $|\frac{ac}{st}| = |\frac{a}{s}||\frac{c}{t}| \leq \frac{|a|^p}{p|s|^p} + \frac{|c|^q}{q|t^q}$, and the same is true for bd.

It follows that $\frac{1}{st}(|ac| + |bd|) \leq \frac{|a|^p + |b|^p}{p|s|^p} + \frac{|c|^q + |d|^q}{q|t|^q} = \frac{1}{p} + \frac{1}{q} = 1$, or $|ac| + |bd| \leq st$.

\subsection{Triangle Inequality}
$$dist(\vec{p}, \vec{q}) + dist(\vec{q}, \vec{r}) \geq dist(\vec{p}, \vec{r})$$

This can be generalized by mathematical induction to $dist(\vec{p_0}, \vec{q_n})
\leq \sum_{j = 1}^n dist(\vec{p}_{j - 1}, \vec{p}_{j})$ (Otherwise known that the
shortest distance between two points is the straight line, or the \textbf{Generalized
Triangle Inequality} or the ``Broken Line Inequality'')

This can be thought of algebraically, such that $|a + b| \leq |a| + |b|$ with equality iff $ab \geq 0$

\subsection{Minkowski's Inequality}
In $\mathbb{R}^2$: let $||(a, b)||_p = (|a^p| + |b^p|)^{1/p}$ for any $p \in \mathbb{R} > 1$, this is known as the $p$-norm of a vector. Note that $||(a,b)||_2 = ||(a, b)|| = \sqrt{a^2 + b^2}$

Mikowski's states that $||u + v||_p \leq ||u||_p + ||v||_p$, with equality if v = tu or u = tv for some $t \geq 0$.

This can be thought of as the triangle inequality for the p-norm, rather than the ordinary norm.

\subsubsection{P-Norm}
In $\mathbb{R^2}$, the p-norm of a vector, $||(a, b)||_p = (|a|^p + |b|^p)^{frac{1}{p}}$ for any p > 1, where p $\in \mathbb{Q}$.
The rational power of some number, m,  exists if there is some sequence, qn, where $n \to \infty, qn \to$ the rational number, only true if for any sequence which does this, the limit is equal.
This is proven by for any two sequences, qn and rn, $m^{qn}/m^{rn} = m^{qn-rn}$, such that as $n \to \infty$, it equals 1.

\subsubsection{Proof}
If u = (a, b) and v = (c, d), then $(|a+c|^p + |b+d|^p)^{\frac{1}{p}p} = |a+c|^p + |c+d|^p = |a+c||a+c|^{p-1} + |b+d||b+d|^{p-1} \leq (|a| + |c|)|a+c|^{p-1} + (|b| + |d|)|b+d|^{p-1} = |a||a+c|^{p-1} + |b||b+d|^{p-1} + |c||a+c|^{p-1} + |d||b+d|^{p-1}$.

Then, by the Holder inequality, that is $\leq (|a|^p + |b|^p)^{1/p}(|a+c|^{(p-1)q} + |b+d|^{(p-1)q})^{\frac{1}{q}} + ... = ||u||_p||v||_p||u+v||_p^{\frac{p}{q}}$

This can be divided to get the inequality.

\underline{Add proof of the equality definition}

\end{document}

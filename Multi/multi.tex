\documentclass[11 pt, twoside]{article}
\usepackage{textcomp}
\usepackage[margin=1in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{color}
%\usepackage{indentfirst}
\usepackage[parfill]{parskip}
\usepackage{setspace}
\usepackage{tikz}
\usepackage{amsmath}
%\usepackage{hyperref}
\usepackage{amsfonts}
\usepackage{amssymb}

\begin{document}

\title{Multivariable Calculus}
\author{Avery Karlin}
\date{Fall 2015}

\maketitle
\newpage
\tableofcontents
\vspace{11pt}
\noindent
\underline{Teacher}: Stern
\newpage

\input{unit1.tex} %R^1 and EVT in R^2
\input{unit2.tex} %Inequalities and EVT in R^n
\input{unit3.tex} %Differentiability

\section{Linear Algebra}
\subsection{Linear Mappings/Functions}

Consider a function $\vec{l}: \mathbb{R}^d \to \mathbb{R}^e$. $\vec{l}$ is
called a \textbf{linear mapping} if the image of any k-flat in $\mathbb{R}^d$
($0 \leq k \leq d$) is a \~{k}-flat in $\mathbb{R}^e$, where $\~{k} \leq k$.
Basically these definitions ``preserve flatness.''

\subsection{Sufficient Conditions for Linear Mapping}
For $\vec{l}$ to be a linear function, it must have the following properties:

\begin{enumerate}
    \item Additivity: $\vec{l}(\vec{x} + \vec{y}) = \vec{l}(\vec{x}) +
        \vec{l}(\vec{y})$
    \item Homogeneity: $\vec{l}(c\vec{x}) = c\vec{l}(\vec{x})$
\end{enumerate}
\vspace{0.2cm}

\textbf{Proof}: Take k-flat in $\mathbb{R}^d$ $F = \{t_1\vec{a}_1 +
t_2\vec{a}_2 + \dots + t_k\vec{a}_k \+|\+ t_1, t_2, \dots, t_k \in
\mathbb{R}\}$. Here, $\vec{a}_1, \vec{a}_2, \dots, \vec{a}_k \in
\mathbb{R}^d$ such that all of them are independent of each other, i.e. no
$\vec{a}_i$ is a linear combination of $\{\vec{a}_j \+|\+ j \neq i\}$. Because
otherwise $\vec{a}_i$ can be broken up and the dimension of $F$ will be reduced.
This can also be phrased as the set of $\vec{a}_i$ must satisfy the following condition:
$F = \vec{0} \implies t_1 = t_2 = \dots = t_k = 0$. Because if there exists a
non-trivial solution, we can subtract all the terms with their coefficient being
0, and divide through the non-zero coefficient, then we would express
$\vec{a}_i$ as a linear combination of others. Therefore, if all the vectors are
independent, the equation $F = \vec{0}$ only has the trivial solution.

So now consider the linear mapping $\vec{l}$. We can distribute because the
mapping is additive, and we can factor out the coefficients because it is
homogeneous.

\begin{align*}
    \vec{l}(F) &= \{\vec{l}(t_1\vec{a}_1 + \dots + t_k\vec{a}_k) \+|\+ t_1, \dots, t_k \in \mathbb{R}\}\\
               &= \{t_1 \vec{l}(\vec{a}_1) + \dots + t_k \vec{l}(\vec{a}_k)\}
\end{align*}

If we denote $\vec{l}(\vec{a}_i) := b_i$, we can rewrite $\vec{l}(F) = \{t_1
b_1 + \dots + t_k b_k\}$. This is a \~{k}-flat for some \~{k} $\leq$ k, because
we can always pick a subset of $\vec{b}_i$ such that they are all independent of
each other (unless they are all $\vec{0}$, but in that case $\vec{l}(F)$ is just
the 0-flat), and then we reduce, and the final result would be a \~{k}-flat.
\vspace{0.2cm}

\textbf{Example}: Homogeneity but not additive and therefore non-linear.

Note that if we are dealing with one dimension, all functions that have an
unlimit range are linear mappings. However, in $\mathbb{R}^2$, homogeneity is
not enough.

Consider the function:

\[
    \vec{f}(x, y) =
        \begin{cases}
            (\frac{x^3 + y^3}{x^2 + y^2}, \frac{xy^2}{x^2+y^2}) & \mbox{if } (x,
            y) \neq (0, 0)\\
            (0, 0) & \mbox{if } (x, y) = (0,0)
        \end{cases}
\]

$\vec{f}$ is homogeneous as it is a composition of homogeneous functions, but it
does not preserve flatness for obvious reasons, and that is because this
function is not additive.

\subsection{Matrices}
\subsubsection{Definition}
Matrices are list of vectors, with each column being a single vector. For
example, $((1,2,0),(-1, 3, 4))$ can be rewritten as

$$\left|\begin{array}{cc}
    1 & -1\\
    2 & 3\\
    0 & 4\\
\end{array} \right|$$

This is known as a matrix, and an element of a matrix can be denoted with two
subscripts with the lower case of the matrix' name, with the first subscript denoting the row number, and the second denoting the
column number. If the above matrix is $A$, then $a_{11} = 1$ and $a_{31} = 0$.

\subsubsection{Operations}

There is also a transpose operation, if $B = A^T$ (if $B$ is $A$ transposed),
then $b_{ij} = b_{ji}$

If we have two collections of items $A = [\vec{a}_1, \vec{a}_2, \dots, \vec{a}_n]$
and $B = [\vec{b}_1, \vec{b_2}, \dots, \vec{a}_p]$, we can generate all possible
dot products by a matrix product. $C = AB$ means:

\begin{align*}
    c_{ij} &= (i\mbox{-th row of } A)^T \cdot (j\mbox{-th column of } B)\\
           &= \sum_n a_{in} b_{nj}
\end{align*}

As a whole, the $C$ column would look like this (here we denote $\vec{\alpha}_i$
as the $i^{th}$ row of $A$ and suppose $A$ has $m$ rows). Then:

\[
    C = \left|\begin{array}{ccc}
        \vec{\alpha}_1 \cdot \vec{b}_1 & \dots & \vec{\alpha}_1 \cdot
        \vec{b}_j\\
        \vdots & \ddots & \vdots\\
        \vec{\alpha}_m \cdot \vec{b}_1 & \dots & \vec{\alpha}_m \cdot
        \vec{b}_j
    \end{array}\right|
\]

\textbf{Size Requirement for Matrix Multiplication}

For $AB$ to be rigorously defined, the two matrices must be of the form: $A
\subseteq \mathbb{R}^{d\times k}$ and $B \subseteq \mathbb{R}^{k \times
e}$.


\subsection{Unique Expression of Linear Functions}
\subsubsection{Theorem}
For any linear map $\vec{l}: \mathbb{R}^d \to \mathbb{R}^e$.
There is an unique matrix $A \in \mathbb{R}^{e \times d}$ ($d$ columns and
$e$ rows) such that $\vec{l}(\vec{x}) = A\vec{x}$ where the right hand size is a
matrix product, where $\vec{x}$ is regarded as a $d \times 1$ column.

\section{Differentiability of Vector Valued Functions}
\subsection{Definition of Differentiability}
Take the function $\vec{f}: \mathbb{R}^d \to \mathbb{R}^e$, $\vec{f} =
(f_1, f_2, \dots, f_e)$, $\vec{f}(\vec{x}) \in \mathbb{R}^e$. If we
want to find the change in the output at a point $\vec{p} \in D^\circ$, it can
be written as $\Delta \vec{f}(\vec{p}, \vec{h}) =
\vec{f}(\vec{p} + \vec{h}) - \vec{f}(\vec{p}) \in \mathbb{R}^e$. We say that
$\vec{f}$ is differentiable if there exists a linear function $\vec{l}(\vec{h})$:

$$\boxed{\lim_{\vec{h}\to \vec{0}} \frac{|\+\Delta \vec{f}(\vec{p}, \vec{h}) -
\vec{l}(\vec{h})\+|}{||\vec{h}||}}$$

\subsection{Definition of Differentiability}
Given a vectored valued function $\vec{\gamma}(t)$:

\begin{equation*}
    \vec{\gamma}(t) = \left[
    \begin{array}{c}
        \gamma_1 (t)\\
        \gamma_2 (t)\\
        \vdots\\
        \gamma_d (t)
    \end{array} \right]
\end{equation*}

We define the ``speed'' vector of $\vec{\gamma}$ as its derivative, which is
defined as:-0

$$\frac{d\vec{\gamma}}{dt}(t) := \lim_{h\to0} \frac{\vec{\gamma}(t + h) - \vec{\gamma}(t)}{h}$$

But because of the componentwise nature of limits, we can distribute the limit
into each component of $\vec{\gamma}$, so we can rewrite the speed vector as:

\begin{equation*}
    \frac{d\vec{\gamma}}{dt}(t) = \left[
    \begin{array}{c}
        \gamma_1' (t)\\
        \gamma_2' (t)\\
        \vdots\\
        \gamma_d' (t)
    \end{array} \right]
\end{equation*}


\end{document}

\documentclass[11 pt, twoside]{article}
\usepackage{textcomp}
\usepackage[margin=1in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{color}
%\usepackage{indentfirst}
\usepackage[parfill]{parskip}
\usepackage{setspace}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{hyperref}
\hypersetup {
	colorlinks,
	citecolor=black,
	filecolor=black,
	linkcolor=black,
	urlcolor=black
}
\usepackage{amsfonts}
\usepackage{amssymb}
%\usepackage{color}
%\usepackage{verbatim}

\begin{document}

\title{Multivariable Calculus}
\author{Avery Karlin}
\date{Fall 2015}

\maketitle
\newpage
\tableofcontents
\vspace{11pt}
\noindent
\underline{Teacher}: Stern
\newpage

\input{unit1.tex} %R^1 and EVT in R^2
\input{unit2.tex} %Inequalities and EVT in R^n
\input{unit3.tex} %Differentiability

\section{Linear Algebra}
\subsection{Linear Mappings/Functions}

Consider a function $\vec{l}: \mathbb{R}^d \to \mathbb{R}^e$. $\vec{l}$ is
called a \textbf{linear mapping} if the image of any k-flat in $\mathbb{R}^d$
($0 \leq k \leq d$) is a \~{k}-flat in $\mathbb{R}^e$, where $\~{k} \leq k$.
Basically these definitions ``preserve flatness.''

Thus the rigorous definition of a linear function is if $\vec{l}: \mathbb{R^d} \to \mathbb{R^e}$ is linear, then $\exists A \in \mathbb{R^{e \times d}}$ such that $\vec{l}(\vec{x}) = A\vec{x}$, where $\vec{x}$ is viewed as a column matrix (d \times 1).

\subsection{Sufficient Conditions for Linear Mapping}
For $\vec{l}$ to be a linear function, it must have the following properties:

\begin{enumerate}
    \item Additivity: $\vec{l}(\vec{x} + \vec{y}) = \vec{l}(\vec{x}) +
        \vec{l}(\vec{y})$
    \item Homogeneity: $\vec{l}(c\vec{x}) = c\vec{l}(\vec{x})$
\end{enumerate}
\vspace{0.2cm}

\textbf{Proof}: Take k-flat in $\mathbb{R}^d$ $F = \{t_1\vec{a}_1 +
t_2\vec{a}_2 + \dots + t_k\vec{a}_k \+|\+ t_1, t_2, \dots, t_k \in
\mathbb{R}\}$. Here, $\vec{a}_1, \vec{a}_2, \dots, \vec{a}_k \in
\mathbb{R}^d$ such that all of them are independent of each other, i.e. no
$\vec{a}_i$ is a linear combination of $\{\vec{a}_j \+|\+ j \neq i\}$. Because
otherwise $\vec{a}_i$ can be broken up and the dimension of $F$ will be reduced.
This can also be phrased as the set of $\vec{a}_i$ must satisfy the following condition:
$F = \vec{0} \implies t_1 = t_2 = \dots = t_k = 0$. Because if there exists a
non-trivial solution, we can subtract all the terms with their coefficient being
0, and divide through the non-zero coefficient, then we would express
$\vec{a}_i$ as a linear combination of others. Therefore, if all the vectors are
independent, the equation $F = \vec{0}$ only has the trivial solution.

So now consider the linear mapping $\vec{l}$. We can distribute because the
mapping is additive, and we can factor out the coefficients because it is
homogeneous.

\begin{align*}
    \vec{l}(F) &= \{\vec{l}(t_1\vec{a}_1 + \dots + t_k\vec{a}_k) \+|\+ t_1, \dots, t_k \in \mathbb{R}\}\\
               &= \{t_1 \vec{l}(\vec{a}_1) + \dots + t_k \vec{l}(\vec{a}_k)\}
\end{align*}

If we denote $\vec{l}(\vec{a}_i) := b_i$, we can rewrite $\vec{l}(F) = \{t_1
b_1 + \dots + t_k b_k\}$. This is a \~{k}-flat for some \~{k} $\leq$ k, because
we can always pick a subset of $\vec{b}_i$ such that they are all independent of
each other (unless they are all $\vec{0}$, but in that case $\vec{l}(F)$ is just
the 0-flat), and then we reduce, and the final result would be a \~{k}-flat.
\vspace{0.2cm}

\textbf{Example}: Homogeneity but not additive and therefore non-linear.

Note that if we are dealing with one dimension, all functions that have an
unlimit range are linear mappings. However, in $\mathbb{R}^2$, homogeneity is
not enough.

Consider the function:

\[
    \vec{f}(x, y) =
        \begin{cases}
            (\frac{x^3 + y^3}{x^2 + y^2}, \frac{xy^2}{x^2+y^2}) & \mbox{if } (x,
            y) \neq (0, 0)\\
            (0, 0) & \mbox{if } (x, y) = (0,0)
        \end{cases}
\]

$\vec{f}$ is homogeneous as it is a composition of homogeneous functions, but it
does not preserve flatness for obvious reasons, and that is because this
function is not additive.

\subsection{Matrices}
\subsubsection{Definition}
Matrices are list of vectors, with each column being a single vector. For
example, $((1,2,0),(-1, 3, 4))$ can be rewritten as

$$\left|\begin{array}{cc}
    1 & -1\\
    2 & 3\\
    0 & 4\\
\end{array} \right|$$

This is known as a matrix, and an element of a matrix can be denoted with two
subscripts with the lower case of the matrix' name, with the first subscript denoting the row number, and the second denoting the
column number. If the above matrix is $A$, then $a_{11} = 1$ and $a_{31} = 0$.

\subsubsection{Determinant}

Geometrically, if you treat the matrix $A \in \varmathbb{R}^{d \times n}$ as an
$d$-dimensional strucutre composed of $n$ $d$-dimensional vectors, then the
determinant is the ``volume'' of said structure.

Algebraically

\begin{align*}
    \det A &= \sum_{\sigma \in S_n} (sgn \+ \sigma) a_{1 \sigma_1} a_{2 \sigma_2}
        \dots a_{n\sigma_n}\\
	    \sigma &= (\sigma_1, \sigma_2, \dots, \sigma_n)
	    \end{align*}

	    where 

	    $$sgn \+ \sigma = \frac{\Pi_{i < j} (x_{\sigma_i} - x_{\sigma_j})}{\Pi_{i
	    < j} (x_i - x_j)}$$

	    where $x_1, x_2, \dots, x_n$ are distant values. This function, in a more
	    explicit/less percise form is:

	    $$sgn = (-1)^N$$

	    where $N$ is the number of ordered pairs $(i, j)$ where $i < j$ but $\sigma_j <
	    \sigma_i$.

	    And $S_n$ is the set of permutations of $\sigma$

	    \textbf{Properties of the Determinant Function}:
	    \begin{enumerate}
	        \item $\det I = 1$ (unit property)
		    \item $\det[\vec{a}_1, \dots, c\vec{a}_j, \dots, \vec{a}_n] = c \det A$
		        \item $\det A = \det A^T$
			\end{enumerate}


\subsubsection{Operations}

\textbf{The Transpose operation}: $B = A^T$ ($B$ is $A$ transposed),
then $b_{ij} = a_{ji}$

\textbf{The dot product}: If we have $A = [\vec{a}_1 \vec{a}_2 \dots \vec{a}_n]
\in \mathbb{R}^{d \times n}$ and $B = [\vec{b}_1 \vec{b}_2 \dots \vec{b}_m]
\in \mathbb{R}^{d \times m}$, then we say that the dot product of the two is:

\[
    A \cdot B = \left|\begin{array}{cccc}
        \vec{a}_1 \cdot \vec{b}_1 & \vec{a}_1 \cdot \vec{b}_2 & \dots &
        \vec{a}_1 \cdot \vec{b}_m \\
        \vec{a}_2 \cdot \vec{b}_1 & \vec{a}_2 \cdot \vec{b}_2 & \dots &
        \vec{a}_2 \cdot \vec{b}_m \\
        \vdots & \vdots & \ddots & \vdots\\
        \vec{a}_n \cdot \vec{b}_1 & \vec{a}_n \cdot \vec{b}_2 & \dots &
        \vec{a}_n \cdot \vec{b}_m
                      \end{array}\right|
\]

This operation is distributive, which means that $A \cdot (B + C) = A \cdot B +
A \cdot C$. However, this product is not associative, i.e. $A \cdot (B \cdot C)
\neq (A \cdot B) \cdot C$.

To solve this problem, we have the matrix multiplication operator.

\textbf{Matrix Multiplation}: For $A \in \mathbb{R}^{n\times d}$, $B \in
\mathbb{R}^{d\times m}$, we define the matrix multiplication product to be:

$$\boxed{AB := A^T \cdot B} \in \mathbb{R}^{n\times m}$$

If we call $C = AB$, then we can say that

$$c_{ij} = \sum_{k = 1}^d a_{ik} b_{kj} \text{  } (1 \leq i \leq n, 1 \leq j
\leq m)$$

As a whole, the $C$ column would look like this (here we denote $\vec{\alpha}_i$
as the $i^{th}$ row of $A$ and suppose $A$ has $p$ rows). Then:

\[
    C = \left|\begin{array}{ccc}
        \vec{\alpha}_1 \cdot \vec{b}_1 & \dots & \vec{\alpha}_1 \cdot
        \vec{b}_m\\
        \vdots & \ddots & \vdots\\
        \vec{\alpha}_p \cdot \vec{b}_1 & \dots & \vec{\alpha}_m \cdot
        \vec{b}_m
    \end{array}\right|
\]

Note this operation is both distribut and associative, quick proof:

$$A (BC) \overset{?}{=} (AB) C$$

$$[A(BC)]_{ij} = \sum_{k} a_{ik} [BC]_{kj} = \sum_k a_{ik} (\sum_l b_{kl} c_{lj})
= \sum_{(k, l)} a_{ik} b_{kl} c_{lj}$$

$$[(AB)C]_{ij} = \sum_l [AB]_{il} c_{li} = \sum_l(\sum_k a_{ik} b_{kl})c_{lj} =
\sum_{(l, k)} a_{ik} b_{kl} c_{lj}$$

Therefore the values of the two matrices are the same. We also have to prove
that they are the same size. If $A \in \mathbb{R}^{n \times d}$, $B \in
\mathbb{R}^{d \times e}$ and $C \in \mathbb{R}^{e \times m}$. $BC \in
\mathbb{R}^{d \times m}$ and $A(BC) \in \mathbb{R}^{n \times m}$. $AB \in
\mathbb{R}^{n \times e}$ and $(AB)C \in \mathbb{R}^{n \times m}$.
Therefore matrix multiplication is associative.

\subsubsection{Examples}:

\[
    \left[\begin{array}{cc}
        0 & 0 \\
        0 & 1
    \end{array}\right]
    \left[\begin{array}{cc}
        1 & 0 \\
        0 & 0
    \end{array}\right]
    =
    \left[\begin{array}{cc}
        0 & 0 \\
        0 & 0
    \end{array}\right]
    = O_{2 \times 2}
    = O
\]

Note that in the world of matrices, the product of two non-zero matrices can
result in the zero matrix.

\[
    \left[\begin{array}{cc}
        1 & 0 \\
        1 & 0
    \end{array}\right]
    \left[\begin{array}{cc}
        1 & 0 \\
        0 & 0
    \end{array}\right]
    =
    \left[\begin{array}{cc}
        1 & 0 \\
        1 & 0
    \end{array}\right]
\]

\[
    \left[\begin{array}{cc}
        1 & 0 \\
        0 & 0
    \end{array}\right]
    \left[\begin{array}{cc}
        1 & 0 \\
        1 & 0
    \end{array}\right]
    =
    \left[\begin{array}{cc}
        1 & 0 \\
        0 & 0
    \end{array}\right]
\]

Note that the communicative property does not hold for matrix multiplication.

\subsubsection{Norm}

Given $A \in \varmathbb{R}^{e \times d} = [\vec{a}_1 \+ \vec{a}_2 \+ \dots \+
\vec{a}_d]$, we define the norm of $A$, $||A||$ as:

\[
    ||A|| := \sqrt{\sum_{j = 1}^d ||\+\vec{a}_j\+||^2} = \sqrt{\sum_{j=1}^d
        \sum_{i=1}^e a_{ij}^2}
	\]

	In other words, the norm of a matrix is the sqareroot of the sum of the squares
	of every element in the matrix.

	\textbf{Properties of the Norm}:
	\begin{enumerate}
	    \item $||A|| > 0, ||A|| = 0 \leftrightarrow A = O$
	        \item $||cA|| = |C|||A||$
		    \item $||A+B|| \leq ||A|| + ||B||$ (Triangle Inequality)
		        \item $||AB|| \leq ||A||||B||$ (Generalized Cauchy-Schwarz Inequality)
			\end{enumerate}

			To prove these, we first establish a correspondence between any matrix $A \in
			\varmathbb{R}^{e \times d}$ and a vector in $\varmathbb{R}^{ed}$, we define the
			mapping $\Psi$ from $\varmathbb{R}^{e \times d}$ to $\varmathbb{R}^{ed}$:

			\[
			    A \overset{\Psi}{\longleftrightarrow} (a_{11}, \dots, a_{1d}, a_{21}, \dots,
			        a_{2d}, \dots, a_{e1}, \dots, a_{ed})
				\]

				Note that under this operation, all vector space properties of the matrix are
				preserved. Note that $||A|| = ||\Psi(A)||$, $\Psi(cA) = cA$, and $\Psi(A + B) =
				A + B$.

				However, note that the product is not exactly preserved with this transformation
				to vector, since the size of the matrix plays an integral part in matrix
				multiplication. To prove property 4 of the norm, we need to do a bit of work.

				Let $C = AB$, then wwe know that
				\begin{align*}
				    ||AB||^2 = \sum_i \sum_j \left(\sum_k a_{ik}b_{kj}\right)^2 &= \sum_i \sum_j
				        (\vec{\alpha}_i \cdot \vec{b}_j)^2 \\
					    &\leq \sum_i \sum_j ||\+\vec{\alpha}_i\+||^2 ||\+\vec{b}_j\+||^2\\
					        &= (\sum_i ||\+\vec{\alpha}_i\+||^2)(\sum_j ||\+\vec{b}_j\+||^2)\\
						    &= ||A^T||^2 ||B||^2\\
						        &= ||A||^2 ||B||^2
							\end{align*}

							Therefore, $||AB|| \leq ||A|| ||B||$.

							\subsubsection{Inverses}

							Suppose there exists $A \in \varmathbb{R}^{n \times n}$ and $B \in
							\varmathbb{R}^{n \times n}$, we say that $B$ is a \textbf{two sided inverse} of
							$A$ if $AB = BA = I (= I_n)$, where $I$ is the identity matrix, which functions
							like the number one in real number multiplication.

							\textbf{Thm}: If $A$ has a two-sided inverse, then it has exactly one, namely
							$A^{-1}$.

							Suppose that $B$ and $C$ are both two-sided inverses for $A$, i.e. $AB = BA = I$
							and $AC = CA = I$. We know we can represent $B = BI = B(AC) = BA(C) = IC = C$.
							Therefore $B = C$.

							\textbf{Thm}: $A$ is invertible iff $\det A \neq 0$.

							Let $B = A^{-1}$, $AB = I$. Now we take the determinant of both sides, we get:

							$$\det (AB) = \det I$$

							Since the determinant is distributive, we get:

							$$(\det A)(\det B) = 1$$

							Now it's clear that $\det A \neq 0$

							CONVERSE = TODO

							\subsubsection{Matrix Valued Functions}

							It is entirely possible for functions to give out matrices as its output.
							Suppose $A : D \subseteq \varmathbb{R}^d \to \varmathbb{R}^{e \times k}$.
							Similar to how vector valued functions have component functions, matrix valued
							functions have entry functions. $A$ would look something like this:

							\[
							    A(\vec{x}) = \left[\begin{array}{ccc}
							                a_{11}(\vec{x}) & \dots & a_{1k}(\vec{x})\\
									            \vdots & \vdots & \vdots \\
										                a_{e1}(\vec{x}) & & a_{ek}(\vec{x})
												    \end{array}\right]
												    \]

												    Continuity for such functions is an entrywise property: $A$ is continuous at
												    $\vec{p}$ iff $a_{ij}$ is continuous at $\vec{p}$ $\forall i, j$
\subsection{Unique Expression of Linear Functions}
\subsubsection{Theorem}
For any linear map $\vec{l}: \mathbb{R}^d \to \mathbb{R}^e$.
There is an unique matrix $A \in \mathbb{R}^{e \times d}$ ($d$ columns and
$e$ rows) such that $\vec{l}(\vec{x}) = A\vec{x}$ where the right hand size is a
matrix product, where $\vec{x}$ is regarded as a $d \times 1$ column.

\subsubsection{Proof}
$\vec{l}(\vec{X}) = (l_1(\vec{x}), l_2(\vec{x}),...,l_e(\vec{x}))$, such that each $l_j$ is a linear real-valued function. 

\underline{Claim:} A real valued linear function, $\Gamma: \mathbb{R^d} \to \mathbb{R}$ has the form $\Gamma(\vec{x}) = \vec{a} \cdot \vec{x}$ for some $\vec{a} \in \mathbb{R^d}$.

$\vec{x} = x_1\vec{e}_1 + x_2\vec{e}_2 + ... + x_d\vec{e}_d = (x_1, x_2, ..., x_d)$, and $\Gamma(\vec{x}) = \Gamma(x_1\vec{e}_1 + ... + x_d\vec{e}_d) = \Gamma(\vec{e}_1)x_1 + \Gamma(\vec{e}_2)x_2 + ... + \Gamma(\vec{e}_d)x_d$. If each term, $\Gamma(\vec{e}_n)x_n = a_n$, then $\Gamma(\vec{x}) = \vec{a} \cdot \vec{x}$.

\underline{Claim:} If \vec{a} $\cdot \vec{x}  = \vec{b} \cdot \vec{x}$ for all $\vec{x} \in \mathbb{R^d}$, where $\vec{a}$, $\vec{b} \in \mathbb{R^d}$, then $\vec{a} = \vec{b}$.

For some $\vec{x} = \vec{e}_1 = (1, 0, 0, ..., 0)$, then $a_1 = \vec{a} \cdot \vec{e_1} = \vec{b} \cdot \vec{e_1} = b_1$, and so forth.

Thus, $\vec{l}(\vec{x}) = (\vec{\alpha}_1 \cdot \vec{x}, \vec{\alpha}_2 \cdot \vec{x}, ..., \vec{\alpha}_d \cdot \vec{x})$, able to be written as a column matrix of the vector components, which by the definition of the dot product, can be written as a column matrix of $\alpha^T$, multiplied by $\vec{x}$ = A$\vec{x}$ = B$\vec{x}$.

\section{Differentiability of Vector Valued Functions}
\subsection{Definition of Differentiability}
Let $\vec{f} : D \subseteq \mathbb{R}^d \to \mathbb{R}^e$. Let $\vec{p}
\in D^\circ$. Then $\vec{f}$ is differentiable at $\vec{p}$ if $\exists \+A \in
\mathbb{R}^{e \times d}$ such that

$$\boxed{\forall \epsilon > 0, \exists \+ \delta > 0: 0 < ||\vec{h}|| < \delta
\implies ||\vec{f}(\vec{p} + \vec{h}) - \vec{f}(\vec{p}) - A
\vec{h}|| < \epsilon ||\vec{h}||}$$

If $A$ exists, then we call it the derivative of $\vec{f}$ at point $\vec{p}$, say that $\vec{f}$ is differentiable at $\vec{p}$. We write the derivative as $D\vec{f}(\vec{p}) \in \mathbb{R}^{e \times d}$, and $D\vec{f}(\vec{p})_{ij}$ (Jacobian derivative) $= \partial_{x_j} f_i (\vec{p})$.

\subsection{The Jacobian Derivative}

\underline{Claim:} The above definition decomposes componentwise.

This is done by the basic norm bounds $(|v_j| \leq ||\vec{v}|| \leq \sqrt{d}max_{1 \leq k \leq d} |v_k|)$.

It can then be shown that $|f_i(\vec{p} + \vec{h}) - f_i(\vec{p}) - \vec{\alpha}_i \cdot \vec{h}| < \epsilon||\vec{h}||$, where $\vec{\alpha}_i^T$ is the i-th row of A. Thus, $\vec{\alpha}_i = \vec{\nabla}f_i(\vec{p})$, such that $\vec{\alpha}_i$ is unique.

Then D$\vec{f}$ is the column matrix of the transposed gradient of each component function, or the column matrix of the Jacobian derivative of each component function, or the matrix of the partial derivatives, such that $[D\vec{f}]_ij$ (the ij-th entry) $= \partial_{x_j}f_i = \frac{\partial f_i}{\partial x_j}$. The final interpretation is the row matrix of $\partial_{x_i}\vec{f}$.

\section{The Gradient Operator}

\subsection{Basic Rules}

Since all rules of single-variable derivative operators apply to partial derivative operators, due to functioning similarly, just holding all but one variable constant:

\begin{itemize}
\item $\vec{\nabla}(f + g) = \vec{\nabla}f + \vec{\nabla}g$
\item $\vec{\nabla}(cf) = c\vec{\nabla}f$
\item $\vec{\nabla}(fg) = f\vec{\nabla}g + g\vec{\nabla}f$
\item $\vec{\nabla}(\frac{f}{g}) = \frac{g\vec{\nabla}f - f\vec{\nabla}g}{g^2}$
\end{itemize}

\subsection{Chain Rule}

If $\vec{f}: D \subseteq \mathbb{R^d} \to \mathbb{R^e}$, $\vec{g}: E \subseteq \mathbb{R^e} \to \mathbb{R^k}$, $\vec{f}(D) \cap E \neq \emptyset$, and $\vec{p} \in D^o$ such that $\vec{f}(\vec{p}) \in E^o$. Then, if $\vec{f}$ is differentiable at $\vec{p}$, and $\vec{g}$ is differentiable at $\vec{q} = \vec{f}(\vec{p})$, then $\vec{g} \circ \vec{f}$ is also differentiable at $\vec{p}$, and $D(\vec{g} \circ \vec{f})(\vec{p}) = D\vec{g}(\vec{f}(\vec{p}))D\vec{f}(\vec{p})$.

Thus, it is the same rule as in single-variable calculus, though matrix multiplication is used rather than regular multiplication.

\subsection{Implicit Function Theorem}
Let $F: D \subseteq \mathbb{R}^{d+1} \to \mathbb{R}$ be $C^1$ on $D^o$. Suppose $F(\vec{A}) = 0$, where $\vec{A} = (\vec{a}, \alpha), \vec{a} \in \mathbb{R}^d, \alpha \in \mathbb{R}$. Also suppose $\partial_yF(\vec{A}) \neq 0$. Here, we write $F(\vec{x}, y) = F(x_1, x_2, ..., x_d, y) = F(\vec{X})$. Then $\exists$ an open set $U \subseteq \mathbb{R}^d$ and an open interval $I \subseteq \mathbb{R}$, such that $\vec{a} \in U$ and $\alpha \in I$ and $U x I \subseteq D^o$, and $\exists$ a function $f: U \to I$, such that $\{\vec{X} \in U x I|F(\vec{X}) = 0\} = G_f := \{(\vec{x}, f(\vec{x}))|\vec{x} \in U\}$. Also f is also $C^1$, and $$\vec{\nabla}f(\vec{x}) = -\frac{1}{\partial_yF(\vec{x}, f(\vec{x}))}\vec{\nabla}_\vec{x}F(\vec{x}, f(\vec{x}))$$, where $\vec{\nabla}_\vec{x}F(\vec{x}) = (\partial_{x_1}F, \partial_{x_2}F, ..., \partial_{x_d}F)$, called the partial gradient.

\subsubsection{Meaning}
If F is $C^1$ on its domain $D \subseteq \mathbb{R}^{d+1}, \vec{A} \in D^o, F(\vec{A}) = 0,$ and $\partial_yF(\vec{A}) \neq 0, F(\vec{x}, y) = 0$ can be solved inpliticly for y as a function of $vec{x}$ locally in some neighborhood of a solution point, $\vec{A} = (\vec{a}, \alpha)$.

\subsubsection{Proof}
The implicit function theorem is done in three parts, first by proving the existance of f, then proving the continuity of f, and finally that f is $C^1$, allowing the equation to be proved.

For the first part, a cube (or any convex set) is drawn with an axis from the center of the top and bottom faces, where the midpoint of the axes is said to be $\vec{A}$. $\partial_yF(\vec{A}) \neq 0$ and it can be assumed to be either positive or negative, not changing the proof. Let $g_\vec{a}(y) = F(\vec{a}, y)$. When $y = \alpha, g'_\vec{a}(y) > 0$, meaning $\partial_yF(\vec{x}, y)$ is continuous since F is $C^1$. By continuity, we can assume that the partial derivative is positive for all points within the cube, such that there exists a cube where that is true by the continuity. Thus, the derivative is defined on the axis, such that $\vec{a}$ is fixed. By the theorem assumptions, $F(\vec{A}) = 0, g_\vec{a}(\alpha) = 0$, such that $\exists$ some point $p > \alpha$ and $-p < \alpha$, such that the function value is greater and less than 0 respectively.

By the continuity of F, $g_\vec{a}(y)$ is continuous, such that a square can be drawn around  $g_\vec{a}(p)$, such that F > 0 for all points within by continuity. We can do the same for p, gaining edge lengths of $r^+$ and $r^-$ respectively, such that the minimum edge length forms the open boundary of U, and I := (-p, p). For each $\vec{a}$ by the Intermediate Value Theorem and the derivative, there must be exactly one point such that $F(\vec{a}, \alpha) = 0$, such that the function of those $\alpha$ is f, proving the existance of the implicit function.

For the second part, since for all $\vec{x}, F(\vec{x}, f(\vec{x})) = 0$ for all $\vec{x} \in U$, we can add $\vec{h}$ to $\vec{x}$, such that it is still in U, such that $F(\vec{x}+\vec{h}, f(\vec{x} + \vec{h})) - F(\vec{x}, f(\vec{x})) = 0$, or $\vec{\nabla}F(\vec{X}^*) \cdot \vec{H} = 0$, by the mean value theorem where $\vec{X}^* \in [(\vec{x}, f(\vec{x})), (\vec{x} + \vec{h}, f(\vec{x} + \vec{h}))]$. Further, we can seperate the gradient, such that $\vec{\nabla}_\vec{x}F(\vec{X}^*) \cdot \vec{h} + \partial_yF(\vec{X}^*)\delta f$ = 0. Since if f is continuious, $\delta f = f(\vec{x} + \vec{h}) - f(\vec{x}) \to 0$ as $\vec{h} \to 0$ is true, we use that to prove the continuity of f.

We take the absolute value of the two sides of the equation, such that $|-\partial_yF(\vec{X}^*)\delta f| = |\vec{\nabla}_\vec{x}F(\vec{x}^*) \cdot \vec{h}| \leq ||\vec{\nabla}_\vec{x}F(\vec{X}^*)||||\vec{h}|| \leq ||\vec{\nabla}F(\vec{X}^*)||||\vec{h}|| \leq M||\vec{h}||$ by Cauchy-Schwartz Inequality. Since F is $C^1$, then $\vec{\nabla}F$ is contnuous on U x I, and $||\vec{\nabla}F||$ is continuous on U x I, let M := $max_{UxI}||\vec{\nabla}F||$. We then use the fact that the box is convex, such that any two points within the convex region, the line segment joining them is contained within the region. Thus, $|\partial_yF(\vec{X}^*)||\delta f| \leq M||\vec{h}||$, isolating $\delta f$. We can also say that $\partial_y F(\vec{X}^*) \geq \frac{1}{2}\partial_yF(\vec{A})$, by continuity saying we can create some point $\vec{X}^*$, such that it is true, such that we can create a cube subset of the original cube, such that it is true for all points, $\vec{X}$. Since $\frac{2M||\vec{H}||}{\partial_yF(\vec{A})} \to 0$ as $\vec{h} \to \vec{0}$, we can state the same thing of $\delta f$, since it must be greater than 0, proving continuity.

For the third part, let $\vec{h} = h\vec{e}_j = (0, ..., 1, ..., 0)$, where the 1 is in the $j^{th}$ place. By the previous equation, $-h(\vec{\nabla}_\vec{x}F(\vec{X}^*) \cdot \vec{e}_j) = \partial_yF(\vec{X}^*)\delta f$, after which we divide by h on both sides. As $h \to 0, \frac{\delta f}{h} = -\frac{\vec{\nabla}_\vec{x}F(\vec{X}^*) \cdot \vec{e}_j}{\partial_y F(\vec{X}^*)} \to -\frac{\vec{\nabla}_\vec{x}F(\vec{x}, f(\vec{x})) \cdot \vec{e}_j}{\partial_yF(\vec{x}, f(\vec{x}))} = -\frac{\partial_{x_j}F(\vec{x}, f(\vec{x}))}{\partial_yF(\vec{x}, f(\vec{x})}.$ Since this is equal to $\partial_{x_j}f(\vec{x})$, both can be shown easily to be differentiable.

\subsubsection{Theorem Summary}
$F(\vec{x}, y) = 0$ can be written as $y = f(x)$ locally in a neighborhood (for all $(\vec{x}, y) \in U x I$) of some solution point, $(\vec{a}, \alpha) \in D_F^o \subseteq \mathbb{R}^{d+1}$, when $\partial_yF(\vec{a}, \alpha) \neq 0$ and F is $C^1$ on $D^o$.

In addition, it follows that f is also $C^1$, and $\vec{\nabla}f = \frac{-1}{\partial_yF o (Id x f)}\vec{\nabla}_\vec{x}F o (Id x f)$, where $(Id x f)(\vec{x}) = (\vec{x}, f(\vec{x}))$, where Id is the identity function which makes that valid.

\subsubsection{Vector-Valued Theorem}
For some $\vec{F}(\vec{x}, \vec{y}) = \vec{0}$, it can be written for all $(\vec{x}, \vec{y}) \in U x V$ as $\vec{y} = \vec{f}(\vec{x})$, when $\vec{x} \in \mathbb{R}^d$ and 

\end{document}
